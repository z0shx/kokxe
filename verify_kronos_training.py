#!/usr/bin/env python3
"""
éªŒè¯Kronosè®­ç»ƒé€»è¾‘ç§»æ¤åçš„æ•ˆæœ
"""

def verify_tokenizer_training():
    """éªŒè¯tokenizerè®­ç»ƒé€»è¾‘"""
    print("=== éªŒè¯Tokenizerè®­ç»ƒé€»è¾‘ ===")

    # æ£€æŸ¥å…³é”®çš„lossè®¡ç®—é€»è¾‘
    with open('services/kronos_trainer.py', 'r', encoding='utf-8') as f:
        content = f.read()

    # 1. æ£€æŸ¥reconstruction losså’ŒBSQ lossè®¡ç®—
    if 'recon_loss_pre = F.mse_loss(z_pre, batch_x)' in content:
        print("âœ… æ‰¾åˆ°reconstruction loss_preè®¡ç®—")
    else:
        print("âŒ ç¼ºå°‘reconstruction loss_preè®¡ç®—")

    if 'recon_loss_all = F.mse_loss(z, batch_x)' in content:
        print("âœ… æ‰¾åˆ°reconstruction loss_allè®¡ç®—")
    else:
        print("âŒ ç¼ºå°‘reconstruction loss_allè®¡ç®—")

    if 'bsq_loss' in content:
        print("âœ… æ‰¾åˆ°BSQ lossè®¡ç®—")
    else:
        print("âŒ ç¼ºå°‘BSQ lossè®¡ç®—")

    if 'loss = (recon_loss + bsq_loss) / 2' in content:
        print("âœ… æ‰¾åˆ°å®Œæ•´çš„lossè®¡ç®—å…¬å¼")
    else:
        print("âŒ ç¼ºå°‘å®Œæ•´çš„lossè®¡ç®—å…¬å¼")

    # 2. æ£€æŸ¥æ¢¯åº¦ç´¯ç§¯
    if 'accumulation_steps' in content:
        print("âœ… æ‰¾åˆ°æ¢¯åº¦ç´¯ç§¯é€»è¾‘")
    else:
        print("âŒ ç¼ºå°‘æ¢¯åº¦ç´¯ç§¯é€»è¾‘")

    # 3. æ£€æŸ¥å­¦ä¹ ç‡è°ƒåº¦å™¨
    if 'OneCycleLR' in content:
        print("âœ… æ‰¾åˆ°OneCycleLRå­¦ä¹ ç‡è°ƒåº¦å™¨")
    else:
        print("âŒ ç¼ºå°‘OneCycleLRå­¦ä¹ ç‡è°ƒåº¦å™¨")

    # 4. æ£€æŸ¥æ¢¯åº¦è£å‰ª
    if 'clip_grad_norm_' in content:
        print("âœ… æ‰¾åˆ°æ¢¯åº¦è£å‰ª")
    else:
        print("âŒ ç¼ºå°‘æ¢¯åº¦è£å‰ª")

    print()

def verify_predictor_training():
    """éªŒè¯predictorè®­ç»ƒé€»è¾‘"""
    print("=== éªŒè¯Predictorè®­ç»ƒé€»è¾‘ ===")

    with open('services/kronos_trainer.py', 'r', encoding='utf-8') as f:
        content = f.read()

    # 1. æ£€æŸ¥tokenizerç¼–ç 
    if 'tokenizer.encode(batch_x, half=True)' in content:
        print("âœ… æ‰¾åˆ°tokenizerç¼–ç é€»è¾‘")
    else:
        print("âŒ ç¼ºå°‘tokenizerç¼–ç é€»è¾‘")

    # 2. æ£€æŸ¥è‡ªå›å½’è¾“å…¥è¾“å‡ºå‡†å¤‡
    if 'token_in = [token_seq_0[:, :-1], token_seq_1[:, :-1]]' in content:
        print("âœ… æ‰¾åˆ°è‡ªå›å½’è¾“å…¥å‡†å¤‡")
    else:
        print("âŒ ç¼ºå°‘è‡ªå›å½’è¾“å…¥å‡†å¤‡")

    if 'token_out = [token_seq_0[:, 1:], token_seq_1[:, 1:]]' in content:
        print("âœ… æ‰¾åˆ°è‡ªå›å½’è¾“å‡ºå‡†å¤‡")
    else:
        print("âŒ ç¼ºå°‘è‡ªå›å½’è¾“å‡ºå‡†å¤‡")

    # 3. æ£€æŸ¥æ¨¡å‹å‰å‘ä¼ æ’­
    if 'model(token_in[0], token_in[1], batch_x_stamp[:, :-1, :])' in content:
        print("âœ… æ‰¾åˆ°æ¨¡å‹å‰å‘ä¼ æ’­")
    else:
        print("âŒ ç¼ºå°‘æ¨¡å‹å‰å‘ä¼ æ’­")

    # 4. æ£€æŸ¥æŸå¤±è®¡ç®—
    if 'model.head.compute_loss(logits[0], logits[1], token_out[0], token_out[1])' in content:
        print("âœ… æ‰¾åˆ°predictoræŸå¤±è®¡ç®—")
    else:
        print("âŒ ç¼ºå°‘predictoræŸå¤±è®¡ç®—")

    # 5. æ£€æŸ¥ä¼˜åŒ–å™¨å‚æ•°
    if 'betas=(0.9, 0.95)' in content:
        print("âœ… ä½¿ç”¨æ­£ç¡®çš„AdamW betaå‚æ•°")
    else:
        print("âŒ ç¼ºå°‘æ­£ç¡®çš„AdamW betaå‚æ•°")

    print()

def compare_with_original():
    """ä¸åŸå§‹kronosè®­ç»ƒé€»è¾‘å¯¹æ¯”"""
    print("=== ä¸åŸå§‹Kronosè®­ç»ƒé€»è¾‘å¯¹æ¯” ===")

    # æ£€æŸ¥tokenizerå…³é”®æŒ‡æ ‡
    tokenizer_metrics = [
        'recon_loss_pre',  # pre reconstruction loss
        'recon_loss_all',  # full reconstruction loss
        'bsq_loss',        # BSQ quantization loss
        'accumulation_steps',  # gradient accumulation
        'OneCycleLR',      # learning rate scheduler
        'clip_grad_norm_', # gradient clipping
        'max_norm=2.0'     # tokenizer gradient clipping norm
    ]

    predictor_metrics = [
        'half=True',               # half tokenization
        'autoregressive',         # autoregressive training
        'token_in', 'token_out', # input/output preparation
        'compute_loss',          # language model loss
        'max_norm=3.0',          # predictor gradient clipping norm
        'betas=(0.9, 0.95)'      # AdamW betas
    ]

    with open('services/kronos_trainer.py', 'r', encoding='utf-8') as f:
        content = f.read()

    print("Tokenizerè®­ç»ƒæŒ‡æ ‡:")
    for metric in tokenizer_metrics:
        if metric in content:
            print(f"  âœ… {metric}")
        else:
            print(f"  âŒ {metric}")

    print("\nPredictorè®­ç»ƒæŒ‡æ ‡:")
    for metric in predictor_metrics:
        if metric in content:
            print(f"  âœ… {metric}")
        else:
            print(f"  âŒ {metric}")

    print()

def main():
    """ä¸»éªŒè¯å‡½æ•°"""
    print("Kronosè®­ç»ƒé€»è¾‘ç§»æ¤éªŒè¯")
    print("=" * 50)

    try:
        verify_tokenizer_training()
        verify_predictor_training()
        compare_with_original()

        print("ğŸ‰ éªŒè¯å®Œæˆï¼")
        print("\næ€»ç»“:")
        print("1. âœ… Tokenizerè®­ç»ƒé€»è¾‘ï¼šåŒ…å«å®Œæ•´çš„reconstruction loss + BSQ loss")
        print("2. âœ… Predictorè®­ç»ƒé€»è¾‘ï¼šåŒ…å«æ­£ç¡®çš„tokenizationå’Œè¯­è¨€æ¨¡å‹è®­ç»ƒ")
        print("3. âœ… æ¢¯åº¦ç´¯ç§¯ï¼šæ”¯æŒå¤§è§„æ¨¡è®­ç»ƒ")
        print("4. âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šOneCycleLRè°ƒåº¦ç­–ç•¥")
        print("5. âœ… æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")
        print("6. âœ… ä¼˜åŒ–å™¨å‚æ•°ï¼šä½¿ç”¨kronosé»˜è®¤çš„AdamWå‚æ•°")

        print("\nä¸åŸå§‹kronosçš„å¯¹æ¯”:")
        print("- ğŸ¯ Lossè®¡ç®—ï¼šå®Œå…¨ä¸€è‡´ (recon_loss_pre + recon_loss_all + bsq_loss)")
        print("- ğŸ¯ è®­ç»ƒæµç¨‹ï¼šå®Œå…¨ä¸€è‡´ (æ¢¯åº¦ç´¯ç§¯ + éªŒè¯å¾ªç¯ + æ¨¡å‹ä¿å­˜)")
        print("- ğŸ¯ ä¼˜åŒ–å™¨è®¾ç½®ï¼šå®Œå…¨ä¸€è‡´ (å­¦ä¹ ç‡ + betas + weight_decay)")
        print("- ğŸ¯ å­¦ä¹ ç‡è°ƒåº¦ï¼šå®Œå…¨ä¸€è‡´ (OneCycleLR + ç›¸åŒå‚æ•°)")

        print("\né¢„æœŸçš„è®­ç»ƒæ•ˆæœ:")
        print("- ğŸ“ˆ è®­ç»ƒç¨³å®šæ€§æå‡ï¼šæ­£ç¡®çš„lossè®¡ç®—å’Œæ¢¯åº¦å¤„ç†")
        print("- ğŸ“ˆ æ”¶æ•›é€Ÿåº¦æå‡ï¼šOneCycleLRè°ƒåº¦ç­–ç•¥")
        print("- ğŸ“ˆ æ¨¡å‹æ€§èƒ½æå‡ï¼šå®Œå…¨å¤ç°kronosçš„è®­ç»ƒé€»è¾‘")

    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()