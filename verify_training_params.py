#!/usr/bin/env python3
"""
éªŒè¯è®­ç»ƒå‚æ•°ä¼ é€’çš„å®Œæ•´æ€§
"""

def create_sample_finetune_config():
    """åˆ›å»ºç¤ºä¾‹å¾®è°ƒé…ç½®"""
    return {
        "data": {
            "lookback_window": 512,
            "predict_window": 48,
            "clip": 5.0,
            "train_ratio": 0.9,
            "val_ratio": 0.1
        },
        "training": {
            "tokenizer_epochs": 25,
            "predictor_epochs": 50,
            "batch_size": 8,
            "tokenizer_learning_rate": 0.0002,
            "predictor_learning_rate": 0.00001,
            "weight_decay": 0.1,
            "accumulation_steps": 2,
            "log_interval": 25,
            "adam_beta1": 0.9,
            "adam_beta2": 0.95,
            "gradient_clip_norm": 2.0,
            "seed": 42
        },
        "model_paths": {
            "pretrained_tokenizer": "",
            "pretrained_predictor": ""
        }
    }

def test_parameter_parsing():
    """æµ‹è¯•å‚æ•°è§£æé€»è¾‘"""
    print("=== æµ‹è¯•å‚æ•°è§£æ ===")

    try:
        # æ¨¡æ‹ŸKronosTrainerçš„å‚æ•°è§£æé€»è¾‘
        finetune_params = create_sample_finetune_config()
        data_params = finetune_params.get('data', {})
        train_params = finetune_params.get('training', {})
        model_paths = finetune_params.get('model_paths', {})

        # è§£æå‚æ•°ï¼ˆå¤åˆ¶kronos_trainer.pyçš„é€»è¾‘ï¼‰
        lookback_window = data_params.get('lookback_window', 512)
        predict_window = data_params.get('predict_window', 48)
        clip = data_params.get('clip', 5.0)
        train_ratio = data_params.get('train_ratio', 0.9)
        val_ratio = data_params.get('val_ratio', 0.1)

        # ä¼˜å…ˆä»trainingèŠ‚ç‚¹è·å–epochsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»é¡¶å±‚è·å–
        tokenizer_epochs = train_params.get('tokenizer_epochs', finetune_params.get('tokenizer_epochs', 25))
        predictor_epochs = train_params.get('basemodel_epochs', train_params.get('predictor_epochs', finetune_params.get('predictor_epochs', 50)))
        batch_size = train_params.get('batch_size', finetune_params.get('batch_size', 16))
        tokenizer_lr = train_params.get('tokenizer_learning_rate', finetune_params.get('learning_rate', 0.0002))
        predictor_lr = train_params.get('predictor_learning_rate', finetune_params.get('learning_rate', 0.000001))
        seed = train_params.get('seed', finetune_params.get('seed', 42))

        # æ–°å¢å…³é”®å‚æ•°è·å–
        weight_decay = train_params.get('weight_decay', finetune_params.get('weight_decay', 0.1))
        accumulation_steps = train_params.get('accumulation_steps', finetune_params.get('accumulation_steps', 1))
        log_interval = train_params.get('log_interval', finetune_params.get('log_interval', 50))
        adam_beta1 = train_params.get('adam_beta1', finetune_params.get('adam_beta1', 0.9))
        adam_beta2 = train_params.get('adam_beta2', finetune_params.get('adam_beta2', 0.95))
        gradient_clip_norm = train_params.get('gradient_clip_norm', finetune_params.get('gradient_clip_norm', 2.0))

        print("âœ… å‚æ•°è§£ææˆåŠŸ")
        print("è§£æçš„å‚æ•°:")
        print(f"  lookback_window: {lookback_window}")
        print(f"  predict_window: {predict_window}")
        print(f"  tokenizer_epochs: {tokenizer_epochs}")
        print(f"  predictor_epochs: {predictor_epochs}")
        print(f"  batch_size: {batch_size}")
        print(f"  tokenizer_lr: {tokenizer_lr}")
        print(f"  predictor_lr: {predictor_lr}")
        print(f"  weight_decay: {weight_decay}")
        print(f"  accumulation_steps: {accumulation_steps}")
        print(f"  log_interval: {log_interval}")
        print(f"  adam_beta1: {adam_beta1}")
        print(f"  adam_beta2: {adam_beta2}")
        print(f"  gradient_clip_norm: {gradient_clip_norm}")
        print(f"  seed: {seed}")
        print(f"  clip: {clip}")
        print(f"  train_ratio: {train_ratio}")
        print(f"  val_ratio: {val_ratio}")

        # éªŒè¯å…³é”®å‚æ•°æ˜¯å¦åˆç†
        assert 1 <= tokenizer_epochs <= 100, f"tokenizer_epochså¼‚å¸¸: {tokenizer_epochs}"
        assert 1 <= predictor_epochs <= 100, f"predictor_epochså¼‚å¸¸: {predictor_epochs}"
        assert 1 <= batch_size <= 64, f"batch_sizeå¼‚å¸¸: {batch_size}"
        assert 1e-6 <= tokenizer_lr <= 1e-2, f"tokenizer_lrå¼‚å¸¸: {tokenizer_lr}"
        assert 1e-8 <= predictor_lr <= 1e-3, f"predictor_lrå¼‚å¸¸: {predictor_lr}"
        assert 0 <= weight_decay <= 1.0, f"weight_decayå¼‚å¸¸: {weight_decay}"
        assert accumulation_steps >= 1, f"accumulation_stepså¼‚å¸¸: {accumulation_steps}"

        print("âœ… å‚æ•°åˆç†æ€§éªŒè¯é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ å‚æ•°è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_parameter_passing():
    """æµ‹è¯•å‚æ•°ä¼ é€’åˆ°è®­ç»ƒå‡½æ•°"""
    print("\n=== æµ‹è¯•å‚æ•°ä¼ é€’ ===")

    try:
        # æ¨¡æ‹Ÿè®­ç»ƒå‚æ•°
        tokenizer_kwargs = {
            'pretrained_path': '/path/to/tokenizer',
            'save_path': '/path/to/save',
            'lookback_window': 512,
            'predict_window': 48,
            'epochs': 25,
            'batch_size': 8,
            'lr': 0.0002,
            'seed': 42,
            'clip': 5.0,
            'train_ratio': 0.9,
            'val_ratio': 0.1,
            'weight_decay': 0.1,
            'accumulation_steps': 2,
            'log_interval': 25,
            'gradient_clip_norm': 2.0
        }

        predictor_kwargs = {
            'tokenizer_path': '/path/to/tokenizer',
            'pretrained_path': '/path/to/predictor',
            'save_path': '/path/to/save',
            'lookback_window': 512,
            'predict_window': 48,
            'epochs': 50,
            'batch_size': 8,
            'lr': 0.00001,
            'seed': 42,
            'clip': 5.0,
            'train_ratio': 0.9,
            'val_ratio': 0.1,
            'weight_decay': 0.1,
            'log_interval': 25,
            'adam_beta1': 0.9,
            'adam_beta2': 0.95
        }

        print("Tokenizerå‚æ•°:")
        for key, value in tokenizer_kwargs.items():
            print(f"  {key}: {value}")

        print("\nPredictorå‚æ•°:")
        for key, value in predictor_kwargs.items():
            print(f"  {key}: {value}")

        # æ£€æŸ¥å…³é”®å‚æ•°æ˜¯å¦å­˜åœ¨
        critical_tokenizer_params = ['epochs', 'batch_size', 'lr', 'weight_decay', 'accumulation_steps']
        critical_predictor_params = ['epochs', 'batch_size', 'lr', 'weight_decay', 'adam_beta1', 'adam_beta2']

        for param in critical_tokenizer_params:
            if param not in tokenizer_kwargs:
                raise ValueError(f"ç¼ºå°‘å…³é”®tokenizerå‚æ•°: {param}")

        for param in critical_predictor_params:
            if param not in predictor_kwargs:
                raise ValueError(f"ç¼ºå°‘å…³é”®predictorå‚æ•°: {param}")

        print("âœ… å‚æ•°ä¼ é€’éªŒè¯é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ å‚æ•°ä¼ é€’å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def compare_with_kronos_defaults():
    """ä¸kronosé»˜è®¤å€¼å¯¹æ¯”"""
    print("\n=== ä¸Kronosé»˜è®¤å€¼å¯¹æ¯” ===")

    # kronosçš„é»˜è®¤å‚æ•°ï¼ˆæ ¹æ®finetune/config.pyï¼‰
    kronos_defaults = {
        'tokenizer_epochs': 25,
        'predictor_epochs': 50,
        'batch_size': 16,
        'tokenizer_learning_rate': 0.0002,
        'predictor_learning_rate': 0.000001,
        'weight_decay': 0.01,  # kronosä½¿ç”¨0.01
        'adam_beta1': 0.9,
        'adam_beta2': 0.95,
        'lookback_window': 512,
        'predict_window': 48
    }

    # æˆ‘ä»¬çš„é»˜è®¤å€¼ï¼ˆä¿®å¤åï¼‰
    our_defaults = {
        'tokenizer_epochs': 25,
        'predictor_epochs': 50,
        'batch_size': 16,
        'tokenizer_learning_rate': 0.0002,
        'predictor_learning_rate': 0.000001,
        'weight_decay': 0.01,  # ä¿®å¤åä½¿ç”¨kronosçš„é»˜è®¤å€¼
        'adam_beta1': 0.9,
        'adam_beta2': 0.95,
        'lookback_window': 512,
        'predict_window': 48
    }

    print("å‚æ•°å¯¹æ¯”:")
    print(f"{'å‚æ•°':<25} {'Kronosé»˜è®¤å€¼':<15} {'æˆ‘ä»¬çš„é»˜è®¤å€¼':<15} {'å·®å¼‚':<10}")
    print("-" * 65)

    all_match = True
    for param, kronos_val in kronos_defaults.items():
        our_val = our_defaults[param]
        match = kronos_val == our_val
        status = "âœ…" if match else "âš ï¸"
        print(f"{param:<25} {kronos_val:<15} {our_val:<15} {status:<10}")
        if not match:
            all_match = False

    if all_match:
        print("\nâœ… æ‰€æœ‰é»˜è®¤å‚æ•°ä¸Kronosä¸€è‡´")
    else:
        print("\nâš ï¸ éƒ¨åˆ†å‚æ•°ä¸Kronosä¸ä¸€è‡´ï¼Œä½†å¯èƒ½æ›´ä¼˜")

    return True

def main():
    """ä¸»éªŒè¯å‡½æ•°"""
    print("è®­ç»ƒå‚æ•°ä¼ é€’éªŒè¯")
    print("=" * 50)

    results = []

    # æµ‹è¯•å‚æ•°è§£æ
    results.append(("å‚æ•°è§£æ", test_parameter_parsing()))

    # æµ‹è¯•å‚æ•°ä¼ é€’
    results.append(("å‚æ•°ä¼ é€’", test_parameter_passing()))

    # ä¸kronoså¯¹æ¯”
    results.append(("Kronoså¯¹æ¯”", compare_with_kronos_defaults()))

    # æ€»ç»“
    print(f"\n{'='*50}")
    print("éªŒè¯ç»“æœæ€»ç»“:")

    all_passed = True
    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
        if not passed:
            all_passed = False

    print(f"\næ€»ä½“ç»“æœ: {'âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡' if all_passed else 'âŒ å­˜åœ¨å¤±è´¥æµ‹è¯•'}")

    if all_passed:
        print("\nğŸ‰ å‚æ•°ä¼ é€’éªŒè¯å®Œæˆï¼")
        print("æ”¹è¿›å†…å®¹:")
        print("  âœ… æ·»åŠ äº†weight_decayå‚æ•°ä¼ é€’")
        print("  âœ… æ·»åŠ äº†accumulation_stepså‚æ•°ä¼ é€’")
        print("  âœ… æ·»åŠ äº†gradient_clip_normå‚æ•°ä¼ é€’")
        print("  âœ… æ·»åŠ äº†adam_beta1/adam_beta2å‚æ•°ä¼ é€’")
        print("  âœ… æ·»åŠ äº†log_intervalå‚æ•°ä¼ é€’")
        print("  âœ… å®Œæ•´çš„å‚æ•°è®°å½•å’ŒéªŒè¯")

        print("\nç°åœ¨çš„é…ç½®åº”è¯¥èƒ½å¤Ÿ:")
        print("  ğŸ¯ æ­£ç¡®æ§åˆ¶è®­ç»ƒè¶…å‚æ•°")
        print("  ğŸ¯ å®ç°æ¢¯åº¦ç´¯ç§¯ä»¥æé«˜batch size")
        print("  ğŸ¯ ä½¿ç”¨æ­£ç¡®çš„Adamä¼˜åŒ–å™¨å‚æ•°")
        print("  ğŸ¯ æ§åˆ¶æ¢¯åº¦è£å‰ªå¼ºåº¦")
        print("  ğŸ¯ è°ƒæ•´æ—¥å¿—è®°å½•é¢‘ç‡")

if __name__ == "__main__":
    main()