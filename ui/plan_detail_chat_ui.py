"""
è®¡åˆ’è¯¦æƒ…é¡µ AI Agent å¯¹è¯ç•Œé¢æ¨¡å—
"""
import asyncio
import gradio as gr
from utils.logger import setup_logger
from ui.custom_chatbot import create_custom_chatbot, process_streaming_messages

logger = setup_logger(__name__, "plan_detail_chat_ui.log")


class PlanDetailChatUI:
    """è®¡åˆ’è¯¦æƒ…é¡µ AI Agent å¯¹è¯ UI"""

    def __init__(self, plan_detail_ui):
        self.plan_detail_ui = plan_detail_ui

    async def _collect_stream_messages(self, plan_id: int, user_message: str, conversation_type: str):
        """æ”¶é›†æµå¼æ¶ˆæ¯çš„è¾…åŠ©æ–¹æ³•"""
        from services.langchain_agent import agent_service

        messages = []
        message_batches = []
        try:
            async for message_batch in agent_service.stream_conversation(
                plan_id=plan_id,
                user_message=user_message,
                conversation_type=conversation_type
            ):
                message_batches.append(message_batch)
                for message in message_batch:
                    messages.append(message)
        except Exception as e:
            logger.error(f"æ”¶é›†æµå¼æ¶ˆæ¯å¤±è´¥: {e}")
            messages = [{"role": "assistant", "content": f"âŒ å¯¹è¯å¤±è´¥: {str(e)}"}]

        # ä½¿ç”¨å¢å¼ºçš„æ¶ˆæ¯å¤„ç†
        return process_streaming_messages(message_batches)

    def _get_latest_prediction_csv_data(self, plan_id: int) -> str:
        """è·å–æœ€æ–°é¢„æµ‹æ•°æ®çš„CSVæ ¼å¼æ–‡æœ¬"""
        try:
            from database.models import TrainingRecord, PredictionData
            from database.db import get_db
            from sqlalchemy import desc, and_

            # è·å–æœ€æ–°çš„æœ‰é¢„æµ‹æ•°æ®çš„å·²å®Œæˆè®­ç»ƒè®°å½•
            with get_db() as db:
                completed_trainings = db.query(TrainingRecord).filter(
                    TrainingRecord.plan_id == plan_id,
                    TrainingRecord.status == 'completed'
                ).order_by(desc(TrainingRecord.created_at)).all()

                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰é¢„æµ‹æ•°æ®çš„è®­ç»ƒè®°å½•
                latest_training_with_pred = None
                for training in completed_trainings:
                    pred_count = db.query(PredictionData).filter(
                        PredictionData.training_record_id == training.id
                    ).count()
                    if pred_count > 0:
                        latest_training_with_pred = training
                        break

                if not latest_training_with_pred:
                    return None

                latest_training = latest_training_with_pred

                # è·å–è¯¥è®­ç»ƒè®°å½•çš„æœ€æ–°æ¨ç†æ‰¹æ¬¡çš„é¢„æµ‹æ•°æ®
                latest_batch = db.query(PredictionData.inference_batch_id).filter(
                    PredictionData.training_record_id == latest_training.id
                ).order_by(desc(PredictionData.created_at)).first()

                if not latest_batch:
                    return None

                # è·å–æœ€æ–°æ‰¹æ¬¡çš„é¢„æµ‹æ•°æ®ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
                predictions_query = db.query(PredictionData).filter(
                    and_(
                        PredictionData.training_record_id == latest_training.id,
                        PredictionData.inference_batch_id == latest_batch.inference_batch_id
                    )
                ).order_by(PredictionData.timestamp.asc()).all()

                if not predictions_query:
                    return None

                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                predictions = []
                for pred in predictions_query:
                    predictions.append({
                        'timestamp': pred.timestamp,
                        'open': pred.open,
                        'high': pred.high,
                        'low': pred.low,
                        'close': pred.close,
                        'volume': pred.volume or 0,
                        'amount': pred.amount or 0,
                        'upward_probability': pred.upward_probability or 0,
                        'volatility_amplification_probability': pred.volatility_amplification_probability or 0,
                        'close_min': pred.close_min,
                        'close_max': pred.close_max
                    })

            # æ„å»ºCSVæ ¼å¼çš„é¢„æµ‹æ•°æ®
            csv_lines = []

            # CSVå¤´éƒ¨
            csv_lines.append("timestamp,open,high,low,close,volume,amount,upward_probability,volatility_amplification_probability")

            # æ•°æ®è¡Œ
            for pred in predictions:
                timestamp_str = pred['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
                upward_prob = pred.get('upward_probability', 0) * 100
                vol_prob = pred.get('volatility_amplification_probability', 0) * 100

                csv_lines.append(
                    f"{timestamp_str},{pred['open']:.2f},{pred['high']:.2f},"
                    f"{pred['low']:.2f},{pred['close']:.2f},"
                    f"{pred.get('volume', 0):.2f},{pred.get('amount', 0):.2f},"
                    f"{upward_prob:.2f}%,{vol_prob:.2f}%"
                )

            return "\n".join(csv_lines)

        except Exception as e:
            logger.error(f"è·å–é¢„æµ‹æ•°æ®å¤±è´¥: {e}")
            return None

    def build_ui(self):
        """æ„å»º AI Agent å¯¹è¯ç•Œé¢"""
        components = {}

        # AI Agent å¯¹è¯
        gr.Markdown("**AI Agent å¯¹è¯**")
        agent_chatbot = create_custom_chatbot(height=500)

        # AI Agent å¯¹è¯äº¤äº’ç•Œé¢
        with gr.Row():
            with gr.Column(scale=4):
                agent_user_input = gr.Textbox(
                    label="è¾“å…¥æ¶ˆæ¯",
                    placeholder="è¯·è¾“å…¥æ‚¨çš„æ¶ˆæ¯æˆ–æŒ‡ä»¤...",
                    lines=2,
                    max_lines=5
                )
            with gr.Column(scale=1):
                with gr.Row():
                    agent_send_btn = gr.Button("ğŸ“¤ å‘é€", variant="primary", size="sm")
                    agent_execute_inference_btn = gr.Button("ğŸ§  æ‰§è¡Œæ¨ç†", variant="secondary", size="sm")
                with gr.Row():
                    agent_clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯", variant="secondary", size="sm")

        # å¯¹è¯çŠ¶æ€æ˜¾ç¤º
        agent_status = gr.Markdown("", visible=False)

        # ä¿å­˜ç»„ä»¶å¼•ç”¨
        components.update({
            'agent_chatbot': agent_chatbot,
            'agent_user_input': agent_user_input,
            'agent_send_btn': agent_send_btn,
            'agent_execute_inference_btn': agent_execute_inference_btn,
            'agent_clear_btn': agent_clear_btn,
            'agent_status': agent_status
        })

        # å®šä¹‰ç®€åŒ–çš„åŒæ­¥äº‹ä»¶å¤„ç†å‡½æ•°
        def agent_send_message_wrapper(pid, user_message, history):
            """å‘é€æ¶ˆæ¯ç»™AI Agentï¼ˆçœŸæ­£çš„æµå¼ç‰ˆæœ¬ï¼‰"""
            from utils.common import validate_plan_exists

            is_valid, plan_id, error_msg = validate_plan_exists(pid)

            if not is_valid:
                yield history + [{"role": "assistant", "content": f"âŒ {error_msg}"}], gr.update(value=""), gr.update(visible=True, value=f"âŒ {error_msg}")
                return

            if not user_message or not user_message.strip():
                yield history + [{"role": "assistant", "content": f"âŒ è¯·è¾“å…¥æ¶ˆæ¯å†…å®¹"}], gr.update(value=""), gr.update(visible=True, value=f"âŒ è¯·è¾“å…¥æ¶ˆæ¯å†…å®¹")
                return

            try:
                # è°ƒç”¨çœŸå®çš„ Agent æœåŠ¡è¿›è¡Œæµå¼å¯¹è¯
                from services.langchain_agent import agent_service

                # åˆ›å»ºæµå¼è¾“å‡ºç”Ÿæˆå™¨
                def create_stream_generator():
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)

                    async def async_stream():
                        current_history = history.copy()
                        async for message_batch in agent_service.stream_conversation(
                            plan_id=plan_id,
                            user_message=user_message.strip(),
                            conversation_type="user_chat"
                        ):
                            # ä½¿ç”¨å¢å¼ºçš„æ¶ˆæ¯å¤„ç†
                            processed = process_streaming_messages([message_batch])
                            for message in processed:
                                current_history.append(message)
                                yield current_history, gr.update(value=""), gr.update(visible=False, value="")

                    try:
                        return loop.run_until_complete(async_stream().__anext__())
                    except StopAsyncIteration:
                        return None
                    finally:
                        loop.close()

                # å®ç°çœŸæ­£çš„æµå¼è¾“å‡º - ç®€åŒ–ç‰ˆæœ¬
                def stream_chat_sync():
                    """åŒæ­¥åŒ…è£…å™¨ï¼Œç”¨äºå¤„ç†å¼‚æ­¥æµå¼è¾“å‡º"""
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)

                    async def async_stream():
                        from services.langchain_agent import agent_service
                        current_history = history.copy()

                        async for message_batch in agent_service.stream_conversation(
                            plan_id=plan_id,
                            user_message=user_message.strip(),
                            conversation_type="user_chat"
                        ):
                            # ä½¿ç”¨å¢å¼ºçš„æ¶ˆæ¯å¤„ç†
                            processed = process_streaming_messages([message_batch])
                            for message in processed:
                                current_history.append(message)
                                # å®æ—¶yieldå½“å‰çŠ¶æ€
                                yield current_history, gr.update(value=""), gr.update(visible=False, value="")

                    try:
                        # åˆ›å»ºå¼‚æ­¥ç”Ÿæˆå™¨
                        async_gen = async_stream()

                        # ä½¿ç”¨åŒæ­¥å¾ªç¯å¤„ç†å¼‚æ­¥ç”Ÿæˆå™¨
                        while True:
                            try:
                                result = loop.run_until_complete(async_gen.__anext__())
                                yield result
                            except StopAsyncIteration:
                                break
                    finally:
                        loop.close()

                # æ‰§è¡Œæµå¼è¾“å‡º
                for result in stream_chat_sync():
                    yield result

            except Exception as e:
                logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {e}")
                error_message = [{"role": "assistant", "content": f"âŒ å‘é€å¤±è´¥: {str(e)}"}]
                yield history + error_message, gr.update(value=""), gr.update(visible=False, value="")

        def agent_execute_inference_wrapper(pid, history):
            """æ‰§è¡ŒAI Agentæ¨ç†ï¼ˆé‡ç½®ä¸Šä¸‹æ–‡ - æµå¼ç‰ˆæœ¬ï¼‰"""
            from utils.common import validate_plan_exists
            from services.historical_data_service import historical_data_service

            is_valid, plan_id, error_msg = validate_plan_exists(pid)

            if not is_valid:
                yield history + [{"role": "assistant", "content": f"âŒ {error_msg}"}], gr.update(visible=True, value=f"âŒ {error_msg}")
                return

            try:
                # æ¸…é™¤ç°æœ‰å¯¹è¯å†å²ï¼Œé‡ç½®ä¸Šä¸‹æ–‡
                empty_history = []

                # è·å–æœ€æ–°25å°æ—¶å†…çš„å®é™…äº¤æ˜“æ•°æ®
                historical_data = historical_data_service.get_optimal_historical_data(plan_id)
                if not historical_data:
                    yield empty_history + [{"role": "assistant", "content": "âŒ æœªæ‰¾åˆ°å¯ç”¨çš„å†å²Kçº¿æ•°æ®"}], gr.update(visible=False, value="")
                    return

                # è·å–æœ€æ–°é¢„æµ‹äº¤æ˜“æ•°æ®ï¼ˆæœ€æ–°æ‰¹æ¬¡å·ï¼‰
                prediction_data = self._get_latest_prediction_csv_data(plan_id)

                if not prediction_data:
                    error_message = [{"role": "assistant", "content": "âŒ æœªæ‰¾åˆ°å¯ç”¨çš„é¢„æµ‹æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œæ¨¡å‹æ¨ç†"}]
                    yield empty_history + error_message, gr.update(visible=False, value="")
                    return

                # æ„å»ºæ¨ç†è¯·æ±‚ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œç”±ç³»ç»Ÿæç¤ºè¯æä¾›å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
                inference_request = f"""ã€æœ€æ–°25å°æ—¶å®é™…äº¤æ˜“æ•°æ®ã€‘
{historical_data}

ã€æœ€æ–°é¢„æµ‹äº¤æ˜“æ•°æ®ï¼ˆæœ€æ–°æ‰¹æ¬¡ï¼‰ã€‘
{prediction_data}

è¯·åŸºäºä»¥ä¸Šæ•°æ®è¿›è¡ŒAIäº¤æ˜“å†³ç­–åˆ†æã€‚"""

                # å®ç°çœŸæ­£çš„æµå¼æ¨ç†è¾“å‡º
                def stream_inference_sync():
                    """åŒæ­¥åŒ…è£…å™¨ï¼Œç”¨äºå¤„ç†å¼‚æ­¥æµå¼è¾“å‡º"""
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)

                    async def async_stream():
                        from services.langchain_agent import agent_service
                        current_history = empty_history.copy()  # é‡ç½®å†å²

                        async for message_batch in agent_service.stream_conversation(
                            plan_id=plan_id,
                            user_message=inference_request,
                            conversation_type="inference_session"
                        ):
                            # ä½¿ç”¨å¢å¼ºçš„æ¶ˆæ¯å¤„ç†
                            processed = process_streaming_messages([message_batch])
                            for message in processed:
                                current_history.append(message)
                                # å®æ—¶yieldå½“å‰çŠ¶æ€ - åªè¿”å›2ä¸ªå€¼
                                yield current_history, gr.update(visible=False, value="")

                    try:
                        # åˆ›å»ºå¼‚æ­¥ç”Ÿæˆå™¨
                        async_gen = async_stream()

                        # ä½¿ç”¨åŒæ­¥å¾ªç¯å¤„ç†å¼‚æ­¥ç”Ÿæˆå™¨
                        while True:
                            try:
                                result = loop.run_until_complete(async_gen.__anext__())
                                yield result
                            except StopAsyncIteration:
                                break
                    finally:
                        loop.close()

                # æ‰§è¡Œæµå¼è¾“å‡º
                for result in stream_inference_sync():
                    yield result

            except Exception as e:
                logger.error(f"æ‰§è¡Œæ¨ç†å¤±è´¥: {e}")
                error_message = [{"role": "assistant", "content": f"âŒ æ‰§è¡Œæ¨ç†å¤±è´¥: {str(e)}"}]
                yield history + error_message, gr.update(visible=False, value="")

        # ä¿å­˜äº‹ä»¶å¤„ç†å‡½æ•°
        components.update({
            'agent_send_message_wrapper': agent_send_message_wrapper,
            'agent_execute_inference_wrapper': agent_execute_inference_wrapper,
            'agent_clear_conversation_wrapper': self.agent_clear_conversation_wrapper
        })

        return components

    async def _collect_all_messages_async(self, plan_id: int, user_message: str):
        """æ”¶é›†æ‰€æœ‰æµå¼æ¶ˆæ¯"""
        from services.langchain_agent import agent_service

        messages = []
        message_batches = []

        async for message_batch in agent_service.stream_conversation(
            plan_id=plan_id,
            user_message=user_message,
            conversation_type="auto_inference"
        ):
            message_batches.append(message_batch)
            for message in message_batch:
                messages.append(message)

        # ä½¿ç”¨å¢å¼ºçš„æ¶ˆæ¯å¤„ç†
        return process_streaming_messages(message_batches)

    def agent_clear_conversation_wrapper(self, pid, history):
        """æ¸…é™¤AI Agentå¯¹è¯"""
        from utils.common import validate_plan_exists

        is_valid, plan_id, error_msg = validate_plan_exists(pid)

        if not is_valid:
            return history, gr.update(value=""), gr.update(visible=False, value=f"âŒ {error_msg}")

        try:
            result = self.plan_detail_ui.clear_agent_records(plan_id)
            # æ¸…ç©ºèŠå¤©å†å²
            empty_history = []
            status_message = f"âœ… {result}"
            return empty_history, gr.update(value=""), gr.update(visible=True, value=status_message)

        except Exception as e:
            logger.error(f"æ¸…é™¤å¯¹è¯å¤±è´¥: {e}")
            return history, gr.update(value=""), gr.update(visible=False, value=f"âŒ æ¸…é™¤å¤±è´¥: {str(e)}")

    def bind_events(self, components, plan_id_input):
        """ç»‘å®šäº‹ä»¶å¤„ç†å™¨"""
        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨ - ä½¿ç”¨ç»„ä»¶å¼•ç”¨è€Œä¸æ˜¯å­—å…¸é”®
        components['agent_send_btn'].click(
            fn=components['agent_send_message_wrapper'],
            inputs=[plan_id_input, components['agent_user_input'], components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_user_input'], components['agent_status']],
            show_progress=True
        )

        components['agent_execute_inference_btn'].click(
            fn=components['agent_execute_inference_wrapper'],
            inputs=[plan_id_input, components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_status']],
            show_progress=True
        )

        components['agent_clear_btn'].click(
            fn=components['agent_clear_conversation_wrapper'],
            inputs=[plan_id_input, components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_user_input'], components['agent_status']]
        )

        # æ”¯æŒå›è½¦å‘é€æ¶ˆæ¯
        components['agent_user_input'].submit(
            fn=components['agent_send_message_wrapper'],
            inputs=[plan_id_input, components['agent_user_input'], components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_user_input'], components['agent_status']],
            show_progress=True
        )