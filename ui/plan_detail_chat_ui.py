"""
è®¡åˆ’è¯¦æƒ…é¡µ AI Agent å¯¹è¯ç•Œé¢æ¨¡å—
"""
import asyncio
import gradio as gr
from utils.logger import setup_logger
from ui.custom_chatbot import create_custom_chatbot, process_streaming_messages
from ui.enhanced_chatbot import enhanced_chatbot

logger = setup_logger(__name__, "plan_detail_chat_ui.log")


class PlanDetailChatUI:
    """è®¡åˆ’è¯¦æƒ…é¡µ AI Agent å¯¹è¯ UI"""

    def __init__(self, plan_detail_ui):
        self.plan_detail_ui = plan_detail_ui
        self.current_session_id = None

    def _async_to_sync_stream(self, async_func, initial_history=None, **kwargs):
        """
        é‡å†™çš„å¼‚æ­¥æµè½¬åŒæ­¥å¤„ç†æ–¹æ³•
        æ”¯æŒçœŸæ­£çš„å®æ—¶æµå¼è¾“å‡º
        """
        import asyncio
        import sys
        import threading
        import queue
        from ui.streaming_handler import StreamingHandler

        # ç»Ÿä¸€ä½¿ç”¨çº¿ç¨‹å¤„ç†ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
        result_queue = queue.Queue()
        error_queue = queue.Queue()

        def run_async_in_thread():
            try:
                # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)

                async def stream_processor():
                    handler = StreamingHandler()
                    current_history = initial_history.copy() if initial_history else []
                    session_id = "session_" + str(hash(str(kwargs)))

                    async for message_batch in handler.process_agent_stream_realtime(
                        async_func(**kwargs), session_id
                    ):
                        # ç«‹å³å¤„ç†æ¯ä¸ªæ¶ˆæ¯æ‰¹æ¬¡
                        for message in message_batch:
                            current_history.append(message)
                            result_queue.put(("message", current_history.copy(), gr.update(value=""), gr.update(visible=False, value="")))

                    result_queue.put(("done", None, None, None))

                new_loop.run_until_complete(stream_processor())
                new_loop.close()

            except Exception as e:
                error_queue.put(e)

        # å¯åŠ¨å¼‚æ­¥å¤„ç†çº¿ç¨‹
        thread = threading.Thread(target=run_async_in_thread, daemon=True)
        thread.start()

        # å®æ—¶è·å–ç»“æœå¹¶yield
        while True:
            try:
                # æ£€æŸ¥é”™è¯¯
                if not error_queue.empty():
                    raise error_queue.get()

                # è·å–æ¶ˆæ¯
                status, history, user_input_update, status_update = result_queue.get(timeout=0.1)
                if status == "message":
                    yield history, user_input_update, status_update
                elif status == "done":
                    break

            except queue.Empty:
                continue

            except Exception as e:
                # é™çº§å¤„ç†ï¼šä½¿ç”¨ç®€å•çš„åŒæ­¥åŒ…è£…
                current_history = initial_history.copy() if initial_history else []
                error_message = [{"role": "assistant", "content": f"âŒ æµå¼å¤„ç†é”™è¯¯: {str(e)}"}]
                yield current_history + error_message, gr.update(value=""), gr.update(visible=True, value=f"âŒ æµå¼å¤„ç†é”™è¯¯: {str(e)}")
                break

    def _validate_plan_and_message(self, pid, user_message, history):
        """éªŒè¯è®¡åˆ’å­˜åœ¨æ€§å’Œæ¶ˆæ¯å†…å®¹"""
        from utils.common import validate_plan_exists

        is_valid, plan_id, error_msg = validate_plan_exists(pid)
        if not is_valid:
            return False, None, None, history, f"âŒ {error_msg}"

        if not user_message or not user_message.strip():
            return False, None, None, history, "âŒ è¯·è¾“å…¥æ¶ˆæ¯å†…å®¹"

        return True, plan_id, user_message.strip(), history, None

    async def _collect_stream_messages(self, plan_id: int, user_message: str, conversation_type: str):
        """æ”¶é›†æµå¼æ¶ˆæ¯çš„è¾…åŠ©æ–¹æ³•"""
        from services.langchain_agent import agent_service

        messages = []
        message_batches = []
        try:
            async for message_batch in agent_service.stream_conversation(
                plan_id=plan_id,
                user_message=user_message,
                conversation_type=conversation_type
            ):
                message_batches.append(message_batch)
                for message in message_batch:
                    messages.append(message)
        except Exception as e:
            logger.error(f"æ”¶é›†æµå¼æ¶ˆæ¯å¤±è´¥: {e}")
            messages = [{"role": "assistant", "content": f"âŒ å¯¹è¯å¤±è´¥: {str(e)}"}]

        # ä½¿ç”¨å¢å¼ºçš„æ¶ˆæ¯å¤„ç†
        return process_streaming_messages(message_batches)

    def _get_latest_prediction_data(self, plan_id: int):
        """è·å–æœ€æ–°é¢„æµ‹æ•°æ®ï¼ˆåŸå§‹æ•°æ®ï¼‰"""
        try:
            from database.models import TrainingRecord, PredictionData
            from database.db import get_db
            from sqlalchemy import desc, and_

            with get_db() as db:
                # è·å–æœ€æ–°æœ‰é¢„æµ‹æ•°æ®çš„è®­ç»ƒè®°å½•
                latest_training = db.query(TrainingRecord).filter(
                    TrainingRecord.plan_id == plan_id,
                    TrainingRecord.status == 'completed'
                ).join(PredictionData, TrainingRecord.id == PredictionData.training_record_id).order_by(desc(TrainingRecord.created_at)).first()

                if not latest_training:
                    return None

                # è·å–æœ€æ–°æ‰¹æ¬¡çš„é¢„æµ‹æ•°æ®
                latest_batch = db.query(PredictionData.inference_batch_id).filter(
                    PredictionData.training_record_id == latest_training.id
                ).order_by(desc(PredictionData.created_at)).first()

                if not latest_batch:
                    return None

                return db.query(PredictionData).filter(
                    and_(
                        PredictionData.training_record_id == latest_training.id,
                        PredictionData.inference_batch_id == latest_batch.inference_batch_id
                    )
                ).order_by(PredictionData.timestamp.asc()).all()

        except Exception as e:
            logger.error(f"è·å–é¢„æµ‹æ•°æ®å¤±è´¥: {e}")
            return None

    def _format_prediction_as_csv(self, predictions) -> str:
        """æ ¼å¼åŒ–é¢„æµ‹æ•°æ®ä¸ºCSV"""
        if not predictions:
            return None

        # CSVå¤´éƒ¨
        csv_lines = ["timestamp,open,high,low,close,volume,amount,upward_probability,volatility_amplification_probability"]

        # æ•°æ®è¡Œ
        for pred in predictions:
            timestamp_str = pred.timestamp.strftime('%Y-%m-%d %H:%M:%S')
            upward_prob = (pred.upward_probability or 0) * 100
            vol_prob = (pred.volatility_amplification_probability or 0) * 100

            csv_lines.append(
                f"{timestamp_str},{pred.open:.2f},{pred.high:.2f},"
                f"{pred.low:.2f},{pred.close:.2f},"
                f"{pred.volume or 0:.2f},{pred.amount or 0:.2f},"
                f"{upward_prob:.2f}%,{vol_prob:.2f}%"
            )

        return "\n".join(csv_lines)

    def _get_latest_prediction_csv_data(self, plan_id: int) -> str:
        """è·å–æœ€æ–°é¢„æµ‹æ•°æ®çš„CSVæ ¼å¼æ–‡æœ¬"""
        predictions = self._get_latest_prediction_data(plan_id)
        return self._format_prediction_as_csv(predictions)

    def build_ui(self):
        """æ„å»º AI Agent å¯¹è¯ç•Œé¢"""
        components = {}

        # AI Agent å¯¹è¯
        gr.Markdown("**AI Agent å¯¹è¯**")
        agent_chatbot = create_custom_chatbot(height=500)

        # AI Agent å¯¹è¯äº¤äº’ç•Œé¢
        with gr.Row():
            with gr.Column(scale=4):
                agent_user_input = gr.Textbox(
                    label="è¾“å…¥æ¶ˆæ¯",
                    placeholder="è¯·è¾“å…¥æ‚¨çš„æ¶ˆæ¯æˆ–æŒ‡ä»¤...",
                    lines=2,
                    max_lines=5
                )
            with gr.Column(scale=1):
                # åˆ›å»ºæŒ‰é’®ç»„ä»¶
                agent_send_btn = gr.Button("ğŸ“¤ å‘é€", variant="primary", size="sm")
                agent_cancel_btn = gr.Button("âŒ å–æ¶ˆæ¨ç†", variant="stop", size="sm", interactive=False, visible=False)
                agent_execute_inference_btn = gr.Button("ğŸ§  æ‰§è¡Œæ¨ç†", variant="secondary", size="sm")
            with gr.Row():
                agent_clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯", variant="secondary", size="sm")

        # å¯¹è¯çŠ¶æ€æ˜¾ç¤º
        agent_status = gr.Markdown("", visible=False)

        # ä¿å­˜ç»„ä»¶å¼•ç”¨
        components.update({
            'agent_chatbot': agent_chatbot,
            'agent_user_input': agent_user_input,
            'agent_send_btn': agent_send_btn,
            'agent_cancel_btn': agent_cancel_btn,
            'agent_execute_inference_btn': agent_execute_inference_btn,
            'agent_clear_btn': agent_clear_btn,
            'agent_status': agent_status
        })

        # å®šä¹‰ç®€åŒ–çš„åŒæ­¥äº‹ä»¶å¤„ç†å‡½æ•°
        def update_button_state_on_input(pid, user_input, history):
            """è¾“å…¥å˜åŒ–æ—¶æ›´æ–°æŒ‰é’®çŠ¶æ€"""
            has_input = bool(user_input and user_input.strip())
            has_context = bool(history and len(history) > 0)
            return enhanced_chatbot.update_button_states(False, has_context=has_context, has_input=has_input)
        def agent_send_message_wrapper(pid, user_message, history):
            """å‘é€æ¶ˆæ¯ç»™AI Agentï¼ˆæ”¯æŒå–æ¶ˆçš„æµå¼ç‰ˆæœ¬ï¼‰"""
            # éªŒè¯è¾“å…¥
            is_valid, plan_id, clean_message, current_history, error_msg = self._validate_plan_and_message(pid, user_message, history)
            if not is_valid:
                yield history + [{"role": "assistant", "content": error_msg}], gr.update(value=""), gr.update(visible=True, value=error_msg), enhanced_chatbot.update_button_states(False, has_input=False, has_context=len(history) > 0)
                return

            try:
                # ç”Ÿæˆä¼šè¯ID
                self.current_session_id = enhanced_chatbot.generate_session_id(str(plan_id), clean_message)

                # ä½¿ç”¨å¸¦å–æ¶ˆåŠŸèƒ½çš„å¼‚æ­¥æµè½¬åŒæ­¥å¤„ç†
                from services.langchain_agent import agent_service
                for result in enhanced_chatbot.async_to_sync_stream_with_cancel(
                    agent_service.stream_conversation,
                    session_id=self.current_session_id,
                    initial_history=current_history,
                    plan_id=plan_id,
                    user_message=clean_message,
                    conversation_type="user_chat"
                ):
                    yield result

            except Exception as e:
                logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {e}")
                error_message = [{"role": "assistant", "content": f"âŒ å‘é€å¤±è´¥: {str(e)}"}]
                yield current_history + error_message, gr.update(value=""), gr.update(visible=True, value=str(e)), enhanced_chatbot.update_button_states(False, has_input=False, has_context=len(current_history) > 0)

        def agent_cancel_inference_wrapper(pid, history):
            """å–æ¶ˆæ¨ç†"""
            if self.current_session_id:
                success = enhanced_chatbot.cancel_task(self.current_session_id)
                if success:
                    logger.info(f"æ¨ç†å·²å–æ¶ˆ: {self.current_session_id}")
                    # ä¿ç•™å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡
                    return history, gr.update(visible=True, value="æ¨ç†å·²å–æ¶ˆï¼Œä¿ç•™å½“å‰ä¸Šä¸‹æ–‡"), enhanced_chatbot.update_button_states(False, has_input=False, has_context=len(history) > 0)
                else:
                    logger.warning(f"å–æ¶ˆæ¨ç†å¤±è´¥: {self.current_session_id}")
                    return history, gr.update(visible=True, value="å–æ¶ˆæ¨ç†å¤±è´¥"), enhanced_chatbot.update_button_states(False, has_input=False, has_context=len(history) > 0)
            else:
                return history, gr.update(visible=True, value="æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„æ¨ç†"), enhanced_chatbot.update_button_states(False, has_input=False, has_context=len(history) > 0)

        def agent_execute_inference_wrapper(pid, history):
            """æ‰§è¡ŒAI Agentæ¨ç†ï¼ˆé‡ç½®ä¸Šä¸‹æ–‡ - æµå¼ç‰ˆæœ¬ï¼‰"""
            # éªŒè¯è®¡åˆ’å­˜åœ¨æ€§ï¼ˆé‡ç”¨éªŒè¯æ–¹æ³•ï¼Œä½†ä¼ å…¥ç©ºæ¶ˆæ¯ä»¥è·³è¿‡æ¶ˆæ¯éªŒè¯ï¼‰
            _, plan_id, _, _, error_msg = self._validate_plan_and_message(pid, "dummy", [])
            if error_msg and "è®¡åˆ’ä¸å­˜åœ¨" in error_msg:
                yield history + [{"role": "assistant", "content": error_msg}], gr.update(visible=True, value=error_msg)
                return

            try:
                # æ¸…é™¤ç°æœ‰å¯¹è¯å†å²ï¼Œé‡ç½®ä¸Šä¸‹æ–‡
                empty_history = []

                # è·å–æœ€æ–°25å°æ—¶å†…çš„å®é™…äº¤æ˜“æ•°æ®
                from services.historical_data_service import historical_data_service
                historical_data = historical_data_service.get_optimal_historical_data(plan_id)
                if not historical_data:
                    yield empty_history + [{"role": "assistant", "content": "âŒ æœªæ‰¾åˆ°å¯ç”¨çš„å†å²Kçº¿æ•°æ®"}], gr.update(visible=False, value="")
                    return

                # è·å–æœ€æ–°é¢„æµ‹äº¤æ˜“æ•°æ®
                prediction_data = self._get_latest_prediction_csv_data(plan_id)
                if not prediction_data:
                    yield empty_history + [{"role": "assistant", "content": "âŒ æœªæ‰¾åˆ°å¯ç”¨çš„é¢„æµ‹æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œæ¨¡å‹æ¨ç†"}], gr.update(visible=False, value="")
                    return

                # æ„å»ºæ¨ç†è¯·æ±‚
                inference_request = f"""ã€æœ€æ–°25å°æ—¶å®é™…äº¤æ˜“æ•°æ®ã€‘
{historical_data}

ã€æœ€æ–°é¢„æµ‹äº¤æ˜“æ•°æ®ï¼ˆæœ€æ–°æ‰¹æ¬¡ï¼‰ã€‘
{prediction_data}

è¯·åŸºäºä»¥ä¸Šæ•°æ®è¿›è¡Œäº¤æ˜“å†³ç­–ã€‚"""

                # ä½¿ç”¨é€šç”¨å¼‚æ­¥æµè½¬åŒæ­¥å¤„ç†ï¼ˆæ¨ç†ç‰ˆæœ¬ï¼Œé‡ç½®å†å²ï¼‰
                from services.langchain_agent import agent_service
                for history, user_input_update, status_update in self._async_to_sync_stream(
                    agent_service.stream_conversation,
                    initial_history=empty_history,  # é‡ç½®å†å²
                    plan_id=plan_id,
                    user_message=inference_request,
                    conversation_type="inference_session"
                ):
                    # æ¨ç†å‡½æ•°åªéœ€è¦è¿”å› chatbot å’Œ statusï¼Œå¿½ç•¥ user_input_update
                    yield history, status_update

            except Exception as e:
                logger.error(f"æ‰§è¡Œæ¨ç†å¤±è´¥: {e}")
                error_message = [{"role": "assistant", "content": f"âŒ æ‰§è¡Œæ¨ç†å¤±è´¥: {str(e)}"}]
                yield history + error_message, gr.update(visible=False, value="")

        # ä¿å­˜äº‹ä»¶å¤„ç†å‡½æ•°
        components.update({
            'agent_send_message_wrapper': agent_send_message_wrapper,
            'agent_cancel_inference_wrapper': agent_cancel_inference_wrapper,
            'agent_execute_inference_wrapper': agent_execute_inference_wrapper,
            'agent_clear_conversation_wrapper': self.agent_clear_conversation_wrapper,
            'update_button_state_on_input': update_button_state_on_input
        })

        
        return components

    async def _collect_all_messages_async(self, plan_id: int, user_message: str):
        """æ”¶é›†æ‰€æœ‰æµå¼æ¶ˆæ¯"""
        from services.langchain_agent import agent_service

        messages = []
        message_batches = []

        async for message_batch in agent_service.stream_conversation(
            plan_id=plan_id,
            user_message=user_message,
            conversation_type="auto_inference"
        ):
            message_batches.append(message_batch)
            for message in message_batch:
                messages.append(message)

        # ä½¿ç”¨å¢å¼ºçš„æ¶ˆæ¯å¤„ç†
        return process_streaming_messages(message_batches)

    def agent_clear_conversation_wrapper(self, pid, history):
        """æ¸…é™¤AI Agentå¯¹è¯"""
        from utils.common import validate_plan_exists

        is_valid, plan_id, error_msg = validate_plan_exists(pid)

        if not is_valid:
            return history, gr.update(value=""), gr.update(visible=False, value=f"âŒ {error_msg}")

        try:
            result = self.plan_detail_ui.clear_agent_records(plan_id)
            # æ¸…ç©ºèŠå¤©å†å²
            empty_history = []
            status_message = f"âœ… {result}"
            return empty_history, gr.update(value=""), gr.update(visible=True, value=status_message)

        except Exception as e:
            logger.error(f"æ¸…é™¤å¯¹è¯å¤±è´¥: {e}")
            return history, gr.update(value=""), gr.update(visible=False, value=f"âŒ æ¸…é™¤å¤±è´¥: {str(e)}")

    def bind_events(self, components, plan_id_input):
        """ç»‘å®šäº‹ä»¶å¤„ç†å™¨"""
        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨ - ä½¿ç”¨ç»„ä»¶å¼•ç”¨è€Œä¸æ˜¯å­—å…¸é”®
        components['agent_send_btn'].click(
            fn=components['agent_send_message_wrapper'],
            inputs=[plan_id_input, components['agent_user_input'], components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_user_input'], components['agent_status'], components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']],
            show_progress=True
        )

        components['agent_cancel_btn'].click(
            fn=components['agent_cancel_inference_wrapper'],
            inputs=[plan_id_input, components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_status'], components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']]
        )

        components['agent_execute_inference_btn'].click(
            fn=components['agent_execute_inference_wrapper'],
            inputs=[plan_id_input, components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_status'], components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']],
            show_progress=True
        )

        components['agent_clear_btn'].click(
            fn=components['agent_clear_conversation_wrapper'],
            inputs=[plan_id_input, components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_user_input'], components['agent_status'], components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']]
        )

        # æ”¯æŒå›è½¦å‘é€æ¶ˆæ¯
        components['agent_user_input'].submit(
            fn=components['agent_send_message_wrapper'],
            inputs=[plan_id_input, components['agent_user_input'], components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_user_input'], components['agent_status'], components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']],
            show_progress=True
        )

        # æ·»åŠ è¾“å…¥å˜åŒ–ç›‘å¬ï¼ŒåŠ¨æ€æ›´æ–°æŒ‰é’®çŠ¶æ€
        def update_button_state_on_input(pid, user_input, history):
            """è¾“å…¥å˜åŒ–æ—¶æ›´æ–°æŒ‰é’®çŠ¶æ€"""
            has_input = bool(user_input and user_input.strip())
            has_context = bool(history and len(history) > 0)
            return enhanced_chatbot.update_button_states(False, has_context=has_context, has_input=has_input)

        components['agent_user_input'].change(
            fn=update_button_state_on_input,
            inputs=[plan_id_input, components['agent_user_input'], components['agent_chatbot']],
            outputs=[components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']]
        )