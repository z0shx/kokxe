"""
è®¡åˆ’è¯¦æƒ…é¡µ AI Agent å¯¹è¯ç•Œé¢æ¨¡å—
"""
import asyncio
import gradio as gr
from utils.logger import setup_logger
from ui.custom_chatbot import create_custom_chatbot, process_streaming_messages

logger = setup_logger(__name__, "plan_detail_chat_ui.log")


class ChatButtonStateManager:
    """ç®€åŒ–çš„èŠå¤©æŒ‰é’®çŠ¶æ€ç®¡ç†å™¨"""

    # ç±»å˜é‡ç”¨äºè·Ÿè¸ªæ´»åŠ¨ä»»åŠ¡
    _active_tasks = {}

    @staticmethod
    def update_button_states(is_running: bool, has_context: bool = False, has_input: bool = False):
        """æ›´æ–°æŒ‰é’®çŠ¶æ€"""
        if is_running:
            return (
                gr.update(interactive=False, visible=False),  # å‘é€æŒ‰é’®
                gr.update(interactive=True, visible=True),   # å–æ¶ˆæŒ‰é’®
                gr.update(interactive=False, visible=False)   # æ‰§è¡Œæ¨ç†æŒ‰é’®
            )
        else:
            send_interactive = has_input or has_context
            return (
                gr.update(interactive=send_interactive, visible=True),  # å‘é€æŒ‰é’®
                gr.update(interactive=False, visible=False),            # å–æ¶ˆæŒ‰é’®
                gr.update(interactive=True, visible=True)               # æ‰§è¡Œæ¨ç†æŒ‰é’® - å§‹ç»ˆå¯ç”¨
            )

    @staticmethod
    def generate_session_id(plan_id: str, input_text: str = "") -> str:
        """ç”Ÿæˆä¼šè¯ID"""
        import hashlib
        import time
        content = f"{plan_id}_{input_text}_{time.time()}"
        return hashlib.md5(content.encode()).hexdigest()[:16]

    @classmethod
    def register_task(cls, session_id: str, task_info: dict):
        """æ³¨å†Œä»»åŠ¡"""
        cls._active_tasks[session_id] = task_info

    @classmethod
    def cancel_task(cls, session_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        if session_id in cls._active_tasks:
            task_info = cls._active_tasks[session_id]
            try:
                # ç®€åŒ–çš„å–æ¶ˆé€»è¾‘
                task_info['cancelled'] = True
                del cls._active_tasks[session_id]
                return True
            except Exception:
                return False
        return False


class PlanDetailChatUI:
    """è®¡åˆ’è¯¦æƒ…é¡µ AI Agent å¯¹è¯ UI"""

    def __init__(self, plan_detail_ui):
        self.plan_detail_ui = plan_detail_ui
        self.current_session_id = None
        self.active_tasks = {}  # ç®€åŒ–çš„ä»»åŠ¡ç®¡ç†

    def _async_to_sync_stream(self, async_func, initial_history=None, **kwargs):
        """ç®€åŒ–çš„å¼‚æ­¥æµè½¬åŒæ­¥å¤„ç†æ–¹æ³•"""
        import asyncio

        try:
            # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            current_history = initial_history.copy() if initial_history else []

            async def stream_processor():
                try:
                    async for message_batch in async_func(**kwargs):
                        if isinstance(message_batch, list):
                            for message in message_batch:
                                current_history.append(message)
                                yield current_history.copy(), gr.update(value=""), gr.update(visible=False, value="")
                        else:
                            current_history.append(message_batch)
                            yield current_history.copy(), gr.update(value=""), gr.update(visible=False, value="")

                except Exception as e:
                    error_message = [{"role": "assistant", "content": f"âŒ æµå¼å¤„ç†é”™è¯¯: {str(e)}"}]
                    current_history.extend(error_message)
                    yield current_history.copy(), gr.update(value=""), gr.update(visible=True, value=f"âŒ æµå¼å¤„ç†é”™è¯¯: {str(e)}")

            # è¿è¡Œå¼‚æ­¥ç”Ÿæˆå™¨
            async_gen = stream_processor()
            try:
                while True:
                    result = loop.run_until_complete(async_gen.__anext__())
                    yield result
            except StopAsyncIteration:
                pass

        except Exception as e:
            # é”™è¯¯å¤„ç†
            current_history = initial_history.copy() if initial_history else []
            error_message = [{"role": "assistant", "content": f"âŒ å¼‚æ­¥å¤„ç†é”™è¯¯: {str(e)}"}]
            yield current_history + error_message, gr.update(value=""), gr.update(visible=True, value=f"âŒ å¼‚æ­¥å¤„ç†é”™è¯¯: {str(e)}")

        finally:
            try:
                loop.close()
            except:
                pass

    def _validate_plan_and_message(self, pid, user_message, history):
        """éªŒè¯è®¡åˆ’å­˜åœ¨æ€§å’Œæ¶ˆæ¯å†…å®¹"""
        from utils.common import validate_plan_exists

        is_valid, plan_id, error_msg = validate_plan_exists(pid)
        if not is_valid:
            return False, None, None, history, f"âŒ {error_msg}"

        if not user_message or not user_message.strip():
            return False, None, None, history, "âŒ è¯·è¾“å…¥æ¶ˆæ¯å†…å®¹"

        return True, plan_id, user_message.strip(), history, None

    async def _collect_stream_messages(self, plan_id: int, user_message: str, conversation_type: str):
        """æ”¶é›†æµå¼æ¶ˆæ¯çš„è¾…åŠ©æ–¹æ³•"""
        from services.langchain_agent import agent_service

        messages = []
        message_batches = []
        try:
            async for message_batch in agent_service.stream_conversation(
                plan_id=plan_id,
                user_message=user_message,
                conversation_type=conversation_type
            ):
                message_batches.append(message_batch)
                for message in message_batch:
                    messages.append(message)
        except Exception as e:
            logger.error(f"æ”¶é›†æµå¼æ¶ˆæ¯å¤±è´¥: {e}")
            messages = [{"role": "assistant", "content": f"âŒ å¯¹è¯å¤±è´¥: {str(e)}"}]

        # ä½¿ç”¨å¢å¼ºçš„æ¶ˆæ¯å¤„ç†
        return process_streaming_messages(message_batches)

    def _get_latest_prediction_data(self, plan_id: int):
        """è·å–æœ€æ–°é¢„æµ‹æ•°æ®ï¼ˆåŸå§‹æ•°æ®ï¼‰"""
        try:
            from database.models import TrainingRecord, PredictionData
            from database.db import get_db
            from sqlalchemy import desc, and_

            with get_db() as db:
                # è·å–æœ€æ–°æœ‰é¢„æµ‹æ•°æ®çš„è®­ç»ƒè®°å½•
                latest_training = db.query(TrainingRecord).filter(
                    TrainingRecord.plan_id == plan_id,
                    TrainingRecord.status == 'completed'
                ).join(PredictionData, TrainingRecord.id == PredictionData.training_record_id).order_by(desc(TrainingRecord.created_at)).first()

                if not latest_training:
                    return None

                # è·å–æœ€æ–°æ‰¹æ¬¡çš„é¢„æµ‹æ•°æ®
                latest_batch = db.query(PredictionData.inference_batch_id).filter(
                    PredictionData.training_record_id == latest_training.id
                ).order_by(desc(PredictionData.created_at)).first()

                if not latest_batch:
                    return None

                return db.query(PredictionData).filter(
                    and_(
                        PredictionData.training_record_id == latest_training.id,
                        PredictionData.inference_batch_id == latest_batch.inference_batch_id
                    )
                ).order_by(PredictionData.timestamp.asc()).all()

        except Exception as e:
            logger.error(f"è·å–é¢„æµ‹æ•°æ®å¤±è´¥: {e}")
            return None

    def _format_prediction_as_csv(self, predictions) -> str:
        """æ ¼å¼åŒ–é¢„æµ‹æ•°æ®ä¸ºCSV"""
        if not predictions:
            return None

        # CSVå¤´éƒ¨
        csv_lines = ["timestamp,open,high,low,close,volume,amount,upward_probability,volatility_amplification_probability"]

        # æ•°æ®è¡Œ
        for pred in predictions:
            timestamp_str = pred.timestamp.strftime('%Y-%m-%d %H:%M:%S')
            upward_prob = (pred.upward_probability or 0) * 100
            vol_prob = (pred.volatility_amplification_probability or 0) * 100

            csv_lines.append(
                f"{timestamp_str},{pred.open:.2f},{pred.high:.2f},"
                f"{pred.low:.2f},{pred.close:.2f},"
                f"{pred.volume or 0:.2f},{pred.amount or 0:.2f},"
                f"{upward_prob:.2f}%,{vol_prob:.2f}%"
            )

        return "\n".join(csv_lines)

    def _format_prediction_with_monte_carlo_stats(self, predictions) -> str:
        """
        æ ¼å¼åŒ–é¢„æµ‹æ•°æ®ä¸ºCSVæ ¼å¼ï¼Œä¿æŒç²¾ç¡®æ—¶é—´æˆ³å¹¶æ˜¾ç¤ºæ‰€æœ‰è’™ç‰¹å¡ç½—è·¯å¾„ä¿¡æ¯

        Args:
            predictions: é¢„æµ‹æ•°æ®åˆ—è¡¨

        Returns:
            str: åŒ…å«ç²¾ç¡®æ—¶é—´æˆ³çš„è¯¦ç»†é¢„æµ‹æ•°æ®æ–‡æœ¬
        """
        if not predictions:
            return "æ— é¢„æµ‹æ•°æ®å¯ç”¨"

        # ç”Ÿæˆè¾“å‡ºæ–‡æœ¬
        output_lines = []
        output_lines.append("ã€è’™ç‰¹å¡ç½—è·¯å¾„é¢„æµ‹æ•°æ®ï¼ˆä¿æŒç²¾ç¡®æ—¶é—´ï¼‰ã€‘")
        output_lines.append(f"æ€»é¢„æµ‹æ•°æ®ç‚¹: {len(predictions)}")
        output_lines.append("")

        # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œä¿æŒæ—¶é—´é¡ºåº
        predictions_sorted = sorted(predictions, key=lambda x: x.timestamp)

        # æ·»åŠ æ‰€æœ‰é¢„æµ‹æ•°æ®ï¼ˆä¿æŒç²¾ç¡®æ—¶é—´æˆ³ï¼‰
        output_lines.append("timestamp,path_id,open,high,low,close,volume,amount,upward_probability,volatility_amplification_probability")

        # ä¸ºæ¯ä¸ªé¢„æµ‹æ•°æ®æ·»åŠ è·¯å¾„ID
        path_groups = {}
        for i, pred in enumerate(predictions_sorted):
            timestamp_str = pred.timestamp.strftime('%Y-%m-%d %H:%M:%S')

            # æŒ‰æ—¶é—´æˆ³åˆ†ç»„ï¼Œä¸ºæ¯ä¸ªæ—¶é—´ç‚¹çš„è·¯å¾„åˆ†é…ID
            if timestamp_str not in path_groups:
                path_groups[timestamp_str] = 0
            else:
                path_groups[timestamp_str] += 1

            path_id = f"path_{path_groups[timestamp_str]}"
            upward_prob = (pred.upward_probability or 0) * 100
            vol_prob = (pred.volatility_amplification_probability or 0) * 100

            output_lines.append(
                f"{timestamp_str},{path_id},{pred.open:.2f},{pred.high:.2f},"
                f"{pred.low:.2f},{pred.close:.2f},"
                f"{pred.volume or 0:.2f},{pred.amount or 0:.2f},"
                f"{upward_prob:.2f}%,{vol_prob:.2f}%"
            )

        # æ·»åŠ æ¯ä¸ªæ—¶é—´ç‚¹çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå¿«é€Ÿæ¦‚è§ˆï¼‰
        output_lines.append("")
        output_lines.append("ã€æ—¶é—´ç‚¹ç»Ÿè®¡æ¦‚è§ˆã€‘")
        output_lines.append("timestamp,path_count,open_min,open_max,high_min,high_max,low_min,low_max,close_min,close_max")

        # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„ç»Ÿè®¡ä¿¡æ¯
        time_stats = {}
        for pred in predictions_sorted:
            timestamp_str = pred.timestamp.strftime('%Y-%m-%d %H:%M:%S')
            if timestamp_str not in time_stats:
                time_stats[timestamp_str] = {
                    'count': 0,
                    'opens': [],
                    'highs': [],
                    'lows': [],
                    'closes': []
                }

            time_stats[timestamp_str]['count'] += 1
            time_stats[timestamp_str]['opens'].append(pred.open)
            time_stats[timestamp_str]['highs'].append(pred.high)
            time_stats[timestamp_str]['lows'].append(pred.low)
            time_stats[timestamp_str]['closes'].append(pred.close)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ˜¾ç¤ºå‰15ä¸ªæ—¶é—´ç‚¹ï¼‰
        for i, (timestamp_str, stats) in enumerate(sorted(time_stats.items())[:15]):
            output_lines.append(
                f"{timestamp_str},{stats['count']},"
                f"{min(stats['opens']):.2f},{max(stats['opens']):.2f},"
                f"{min(stats['highs']):.2f},{max(stats['highs']):.2f},"
                f"{min(stats['lows']):.2f},{max(stats['lows']):.2f},"
                f"{min(stats['closes']):.2f},{max(stats['closes']):.2f}"
            )

        if len(time_stats) > 15:
            output_lines.append(f"... ï¼ˆè¿˜æœ‰ {len(time_stats) - 15} ä¸ªæ—¶é—´ç‚¹çš„ç»Ÿè®¡ï¼‰")

        return "\n".join(output_lines)

    def _get_latest_prediction_csv_data(self, plan_id: int) -> str:
        """è·å–æœ€æ–°é¢„æµ‹æ•°æ®çš„CSVæ ¼å¼æ–‡æœ¬ï¼ˆåŒ…å«è’™ç‰¹å¡ç½—è·¯å¾„ç»Ÿè®¡ï¼‰"""
        predictions = self._get_latest_prediction_data(plan_id)
        return self._format_prediction_with_monte_carlo_stats(predictions)

    def build_ui(self):
        """æ„å»º AI Agent å¯¹è¯ç•Œé¢"""
        components = {}

        # AI Agent å¯¹è¯
        gr.Markdown("**AI Agent å¯¹è¯**")
        agent_chatbot = create_custom_chatbot(height=500)

        # AI Agent å¯¹è¯äº¤äº’ç•Œé¢
        with gr.Row():
            with gr.Column(scale=4):
                agent_user_input = gr.Textbox(
                    label="è¾“å…¥æ¶ˆæ¯",
                    placeholder="è¯·è¾“å…¥æ‚¨çš„æ¶ˆæ¯æˆ–æŒ‡ä»¤...",
                    lines=2,
                    max_lines=5
                )
            with gr.Column(scale=1):
                # åˆ›å»ºæŒ‰é’®ç»„ä»¶
                agent_send_btn = gr.Button("ğŸ“¤ å‘é€", variant="primary", size="sm")
                agent_cancel_btn = gr.Button("âŒ å–æ¶ˆæ¨ç†", variant="stop", size="sm", interactive=False, visible=False)
                agent_execute_inference_btn = gr.Button("ğŸ§  æ‰§è¡Œæ¨ç†", variant="secondary", size="sm")
            with gr.Row():
                agent_clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯", variant="secondary", size="sm")

        # å¯¹è¯çŠ¶æ€æ˜¾ç¤º
        agent_status = gr.Markdown("", visible=False)

        # ä¿å­˜ç»„ä»¶å¼•ç”¨ï¼ˆç§»é™¤é‡å¤çš„å¯¹è¯æ¢å¤ä¸‹æ‹‰é€‰æ‹©å™¨ç»„ä»¶ï¼‰
        components.update({
            'agent_chatbot': agent_chatbot,
            'agent_user_input': agent_user_input,
            'agent_send_btn': agent_send_btn,
            'agent_cancel_btn': agent_cancel_btn,
            'agent_execute_inference_btn': agent_execute_inference_btn,
            'agent_clear_btn': agent_clear_btn,
            'agent_status': agent_status
        })

        # å®šä¹‰ç®€åŒ–çš„åŒæ­¥äº‹ä»¶å¤„ç†å‡½æ•°
        def update_button_state_on_input(pid, user_input, history):
            """è¾“å…¥å˜åŒ–æ—¶æ›´æ–°æŒ‰é’®çŠ¶æ€"""
            has_input = bool(user_input and user_input.strip())
            has_context = bool(history and len(history) > 0)
            return ChatButtonStateManager.update_button_states(False, has_context=has_context, has_input=has_input)
        def agent_send_message_wrapper(pid, user_message, history):
            """å‘é€æ¶ˆæ¯ç»™AI Agentï¼ˆæ”¯æŒå–æ¶ˆçš„æµå¼ç‰ˆæœ¬ï¼‰"""
            # éªŒè¯è¾“å…¥
            is_valid, plan_id, clean_message, current_history, error_msg = self._validate_plan_and_message(pid, user_message, history)
            if not is_valid:
                button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(history) > 0)
                yield history + [{"role": "assistant", "content": error_msg}], gr.update(value=""), gr.update(visible=True, value=error_msg), button_states[0], button_states[1], button_states[2]
                return

            try:
                # ç”Ÿæˆä¼šè¯ID
                self.current_session_id = ChatButtonStateManager.generate_session_id(str(plan_id), clean_message)

                # ä½¿ç”¨å¸¦å–æ¶ˆåŠŸèƒ½çš„å¼‚æ­¥æµè½¬åŒæ­¥å¤„ç†
                from services.langchain_agent import agent_service
                final_result = None
                for result in ChatButtonStateManager.async_to_sync_stream(
                    agent_service.stream_conversation,
                    session_id=self.current_session_id,
                    initial_history=current_history,
                    plan_id=plan_id,
                    user_message=clean_message,
                    conversation_type="user_chat"
                ):
                    final_result = result
                    yield result

                # å¯¹è¯å®Œæˆåï¼Œè‡ªåŠ¨åˆ·æ–°å¯¹è¯åˆ—è¡¨
                if final_result:
                    try:
                        # è·å–æœ€æ–°çš„å¯¹è¯åˆ—è¡¨
                        choices = self.get_conversation_list_for_selection(plan_id)
                        # æ·»åŠ å¯¹è¯åˆ—è¡¨åˆ·æ–°åˆ°yieldç»“æœä¸­
                        history, _, _, button_updates = final_result
                        yield history, _, _, button_updates, gr.update(choices=choices), gr.update(interactive=True)
                    except Exception as refresh_error:
                        logger.error(f"è‡ªåŠ¨åˆ·æ–°å¯¹è¯åˆ—è¡¨å¤±è´¥: {refresh_error}")
                        # å³ä½¿åˆ·æ–°å¤±è´¥ï¼Œä¹Ÿè¦è¿”å›åŸå§‹ç»“æœ
                        yield final_result

            except Exception as e:
                logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {e}")
                error_message = [{"role": "assistant", "content": f"âŒ å‘é€å¤±è´¥: {str(e)}"}]
                button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(current_history) > 0)
                yield current_history + error_message, gr.update(value=""), gr.update(visible=True, value=str(e)), button_states[0], button_states[1], button_states[2]

        def agent_cancel_inference_wrapper(pid, history):
            """å–æ¶ˆæ¨ç†"""
            if self.current_session_id:
                success = ChatButtonStateManager.cancel_task(self.current_session_id)
                if success:
                    logger.info(f"æ¨ç†å·²å–æ¶ˆ: {self.current_session_id}")
                    # ä¿ç•™å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡
                    button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(history) > 0)
                    return history, gr.update(value=""), gr.update(visible=True, value="æ¨ç†å·²å–æ¶ˆï¼Œä¿ç•™å½“å‰ä¸Šä¸‹æ–‡"), button_states[0], button_states[1], button_states[2]
                else:
                    logger.warning(f"å–æ¶ˆæ¨ç†å¤±è´¥: {self.current_session_id}")
                    button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(history) > 0)
                    return history, gr.update(value=""), gr.update(visible=True, value="å–æ¶ˆæ¨ç†å¤±è´¥"), button_states[0], button_states[1], button_states[2]
            else:
                button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(history) > 0)
                return history, gr.update(value=""), gr.update(visible=True, value="æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„æ¨ç†"), button_states[0], button_states[1], button_states[2]

        def agent_execute_inference_wrapper(pid, history):
            """æ‰§è¡ŒAI Agentæ¨ç†ï¼ˆé‡ç½®ä¸Šä¸‹æ–‡ - æµå¼ç‰ˆæœ¬ï¼‰"""
            # éªŒè¯è®¡åˆ’å­˜åœ¨æ€§ï¼ˆé‡ç”¨éªŒè¯æ–¹æ³•ï¼Œä½†ä¼ å…¥ç©ºæ¶ˆæ¯ä»¥è·³è¿‡æ¶ˆæ¯éªŒè¯ï¼‰
            _, plan_id, _, _, error_msg = self._validate_plan_and_message(pid, "dummy", [])
            if error_msg and "è®¡åˆ’ä¸å­˜åœ¨" in error_msg:
                button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(history) > 0)
                yield history + [{"role": "assistant", "content": error_msg}], gr.update(visible=True, value=error_msg), button_states[0], button_states[1], button_states[2]
                return

            try:
                # æ¸…é™¤ç°æœ‰å¯¹è¯å†å²ï¼Œé‡ç½®ä¸Šä¸‹æ–‡
                empty_history = []

                # è·å–æœ€æ–°25å°æ—¶å†…çš„å®é™…äº¤æ˜“æ•°æ®
                from services.historical_data_service import historical_data_service
                historical_data = historical_data_service.get_optimal_historical_data(plan_id)
                if not historical_data:
                    button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(empty_history) > 0)
                    yield empty_history + [{"role": "assistant", "content": "âŒ æœªæ‰¾åˆ°å¯ç”¨çš„å†å²Kçº¿æ•°æ®"}], gr.update(visible=True, value="æœªæ‰¾åˆ°å¯ç”¨çš„å†å²Kçº¿æ•°æ®"), button_states[0], button_states[1], button_states[2]
                    return

                # è·å–æœ€æ–°é¢„æµ‹äº¤æ˜“æ•°æ®
                prediction_data = self._get_latest_prediction_csv_data(plan_id)
                if not prediction_data:
                    button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(empty_history) > 0)
                    yield empty_history + [{"role": "assistant", "content": "âŒ æœªæ‰¾åˆ°å¯ç”¨çš„é¢„æµ‹æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œæ¨¡å‹æ¨ç†"}], gr.update(visible=True, value="æœªæ‰¾åˆ°å¯ç”¨çš„é¢„æµ‹æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œæ¨¡å‹æ¨ç†"), button_states[0], button_states[1], button_states[2]
                    return

                # æ„å»ºæ¨ç†è¯·æ±‚
                inference_request = f"""ã€æœ€æ–°25å°æ—¶å®é™…äº¤æ˜“æ•°æ®ã€‘
{historical_data}

ã€æœ€æ–°é¢„æµ‹äº¤æ˜“æ•°æ®ï¼ˆæœ€æ–°æ‰¹æ¬¡ï¼‰ã€‘
{prediction_data}

è¯·åŸºäºä»¥ä¸Šæ•°æ®è¿›è¡Œäº¤æ˜“å†³ç­–ã€‚"""

                # ä½¿ç”¨é€šç”¨å¼‚æ­¥æµè½¬åŒæ­¥å¤„ç†ï¼ˆæ¨ç†ç‰ˆæœ¬ï¼Œé‡ç½®å†å²ï¼‰
                from services.langchain_agent import agent_service
                final_result = None
                for history, user_input_update, status_update in self._async_to_sync_stream(
                    agent_service.stream_conversation,
                    initial_history=empty_history,  # é‡ç½®å†å²
                    plan_id=plan_id,
                    user_message=inference_request,
                    conversation_type="inference_session"
                ):
                    # æ¨ç†å‡½æ•°éœ€è¦è¿”å›å®Œæ•´çš„5ä¸ªå€¼ï¼šchatbot, status, send_btn, cancel_btn, inference_btn
                    button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(history) > 0)
                    final_result = (history, status_update, button_states[0], button_states[1], button_states[2])
                    yield final_result

                # æ¨ç†å®Œæˆåï¼Œè‡ªåŠ¨åˆ·æ–°å¯¹è¯åˆ—è¡¨
                if final_result:
                    try:
                        choices = self.get_conversation_list_for_selection(plan_id)
                        # å‘é€åˆ·æ–°ä¿¡å·ï¼Œä½†ç•Œé¢éœ€è¦å¤„ç†é¢å¤–çš„è¾“å‡ºå‚æ•°
                        yield (*final_result, gr.update(choices=choices), gr.update(interactive=True))
                    except Exception as refresh_error:
                        logger.error(f"æ¨ç†å®Œæˆåè‡ªåŠ¨åˆ·æ–°å¯¹è¯åˆ—è¡¨å¤±è´¥: {refresh_error}")
                        # å³ä½¿åˆ·æ–°å¤±è´¥ï¼Œä¹Ÿè¦è¿”å›åŸå§‹ç»“æœ
                        yield final_result

            except Exception as e:
                logger.error(f"æ‰§è¡Œæ¨ç†å¤±è´¥: {e}")
                error_message = [{"role": "assistant", "content": f"âŒ æ‰§è¡Œæ¨ç†å¤±è´¥: {str(e)}"}]
                button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(history) > 0)
                yield history + error_message, gr.update(visible=True, value=f"æ‰§è¡Œæ¨ç†å¤±è´¥: {str(e)}"), button_states[0], button_states[1], button_states[2]

        # ä¿å­˜äº‹ä»¶å¤„ç†å‡½æ•°ï¼ˆç§»é™¤é‡å¤çš„å¯¹è¯æ¢å¤ä¸‹æ‹‰é€‰æ‹©å™¨ç›¸å…³å¤„ç†å‡½æ•°ï¼‰
        components.update({
            'agent_send_message_wrapper': agent_send_message_wrapper,
            'agent_cancel_inference_wrapper': agent_cancel_inference_wrapper,
            'agent_execute_inference_wrapper': agent_execute_inference_wrapper,
            'agent_clear_conversation_wrapper': self.agent_clear_conversation_wrapper,
            'update_button_state_on_input': update_button_state_on_input
        })

        
        return components

    async def _collect_all_messages_async(self, plan_id: int, user_message: str):
        """æ”¶é›†æ‰€æœ‰æµå¼æ¶ˆæ¯"""
        from services.langchain_agent import agent_service

        messages = []
        message_batches = []

        async for message_batch in agent_service.stream_conversation(
            plan_id=plan_id,
            user_message=user_message,
            conversation_type="auto_inference"
        ):
            message_batches.append(message_batch)
            for message in message_batch:
                messages.append(message)

        # ä½¿ç”¨å¢å¼ºçš„æ¶ˆæ¯å¤„ç†
        return process_streaming_messages(message_batches)

    def agent_clear_conversation_wrapper(self, pid, history):
        """æ¸…é™¤AI Agentå¯¹è¯"""
        from utils.common import validate_plan_exists

        is_valid, plan_id, error_msg = validate_plan_exists(pid)

        if not is_valid:
            button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=False)
            return history, gr.update(value=""), gr.update(visible=True, value=f"âŒ {error_msg}"), button_states[0], button_states[1], button_states[2]

        try:
            result = self.plan_detail_ui.clear_agent_records(plan_id)
            # æ¸…ç©ºèŠå¤©å†å²
            empty_history = []
            status_message = f"âœ… {result}"
            button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=False)
            return empty_history, gr.update(value=""), gr.update(visible=True, value=status_message), button_states[0], button_states[1], button_states[2]

        except Exception as e:
            logger.error(f"æ¸…é™¤å¯¹è¯å¤±è´¥: {e}")
            button_states = ChatButtonStateManager.update_button_states(False, has_input=False, has_context=len(history) > 0)
            return history, gr.update(value=""), gr.update(visible=True, value=f"âŒ æ¸…é™¤å¤±è´¥: {str(e)}"), button_states[0], button_states[1], button_states[2]

    def bind_events(self, components, plan_id_input):
        """ç»‘å®šäº‹ä»¶å¤„ç†å™¨"""
        # ç»‘å®šäº‹ä»¶å¤„ç†å™¨ - ä½¿ç”¨ç»„ä»¶å¼•ç”¨è€Œä¸æ˜¯å­—å…¸é”®
        components['agent_send_btn'].click(
            fn=components['agent_send_message_wrapper'],
            inputs=[plan_id_input, components['agent_user_input'], components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_user_input'], components['agent_status'], components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']],
            show_progress=True
        )

        components['agent_cancel_btn'].click(
            fn=components['agent_cancel_inference_wrapper'],
            inputs=[plan_id_input, components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_status'], components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']]
        )

        components['agent_execute_inference_btn'].click(
            fn=components['agent_execute_inference_wrapper'],
            inputs=[plan_id_input, components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_status'], components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']],
            show_progress=True
        )

        components['agent_clear_btn'].click(
            fn=components['agent_clear_conversation_wrapper'],
            inputs=[plan_id_input, components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_user_input'], components['agent_status'], components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']]
        )

        # æ”¯æŒå›è½¦å‘é€æ¶ˆæ¯
        components['agent_user_input'].submit(
            fn=components['agent_send_message_wrapper'],
            inputs=[plan_id_input, components['agent_user_input'], components['agent_chatbot']],
            outputs=[components['agent_chatbot'], components['agent_user_input'], components['agent_status'], components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']],
            show_progress=True
        )

        # æ·»åŠ è¾“å…¥å˜åŒ–ç›‘å¬ï¼ŒåŠ¨æ€æ›´æ–°æŒ‰é’®çŠ¶æ€
        def update_button_state_on_input(pid, user_input, history):
            """è¾“å…¥å˜åŒ–æ—¶æ›´æ–°æŒ‰é’®çŠ¶æ€"""
            has_input = bool(user_input and user_input.strip())
            has_context = bool(history and len(history) > 0)
            return ChatButtonStateManager.update_button_states(False, has_context=has_context, has_input=has_input)

        components['agent_user_input'].change(
            fn=update_button_state_on_input,
            inputs=[plan_id_input, components['agent_user_input'], components['agent_chatbot']],
            outputs=[components['agent_send_btn'], components['agent_cancel_btn'], components['agent_execute_inference_btn']]
        )

        # ç§»é™¤é‡å¤çš„å¯¹è¯æ¢å¤ä¸‹æ‹‰é€‰æ‹©å™¨äº‹ä»¶ç»‘å®š - ç›´æ¥ä½¿ç”¨åˆ—è¡¨ç‚¹å‡»æ¢å¤åŠŸèƒ½

    def setup_auto_refresh_and_initial_load(self, plan_id_input, components):
        """è®¾ç½®è‡ªåŠ¨åˆ·æ–°å’Œåˆå§‹åŠ è½½ï¼ˆç®€åŒ–ç‰ˆæœ¬ - åˆ—è¡¨ç‚¹å‡»æ¢å¤æ— éœ€é¢å¤–åˆå§‹åŒ–ï¼‰"""
        return components

    def restore_conversation_from_records(self, plan_id: int, conversation_id: int = None):
        """
        ä»å¯¹è¯è®°å½•æ¢å¤å®Œæ•´å¯¹è¯åˆ°chatbot

        Args:
            plan_id: è®¡åˆ’ID
            conversation_id: æŒ‡å®šå¯¹è¯IDï¼Œå¦‚æœä¸ºNoneåˆ™æ¢å¤æœ€æ–°å¯¹è¯
        """
        try:
            from database.models import AgentConversation, AgentMessage
            from database.db import get_db
            from ui.custom_chatbot import format_conversation_history
            from sqlalchemy import desc

            with get_db() as db:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šå¯¹è¯IDï¼Œè·å–æœ€æ–°çš„å¯¹è¯
                if conversation_id is None:
                    conversation = db.query(AgentConversation).filter(
                        AgentConversation.plan_id == plan_id
                    ).order_by(desc(AgentConversation.last_message_at)).first()
                else:
                    conversation = db.query(AgentConversation).filter(
                        AgentConversation.id == conversation_id,
                        AgentConversation.plan_id == plan_id
                    ).first()

                if not conversation:
                    return []  # è¿”å›ç©ºå†å²ï¼Œè¡¨ç¤ºæ²¡æœ‰å¯¹è¯è®°å½•

                # è·å–è¯¥å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
                messages = db.query(AgentMessage).filter(
                    AgentMessage.conversation_id == conversation.id
                ).order_by(AgentMessage.created_at).all()

                if messages:
                    # æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºchatbotæ ¼å¼
                    formatted_history = format_conversation_history(messages)
                    logger.info(f"æ¢å¤äº†å¯¹è¯ {conversation.id} çš„ {len(messages)} æ¡æ¶ˆæ¯")
                    return formatted_history
                else:
                    return []

        except Exception as e:
            logger.error(f"æ¢å¤å¯¹è¯è®°å½•å¤±è´¥: {e}")
            return []

    def get_conversation_list_for_selection(self, plan_id: int):
        """
        è·å–å¯¹è¯åˆ—è¡¨ç”¨äºé€‰æ‹©æ¢å¤

        Returns:
            gr.Dropdown choices æ ¼å¼çš„å¯¹è¯åˆ—è¡¨
        """
        try:
            from database.models import AgentConversation, AgentMessage
            from database.db import get_db
            from database.models import now_beijing
            from sqlalchemy import desc, func

            with get_db() as db:
                conversations = db.query(AgentConversation).filter(
                    AgentConversation.plan_id == plan_id
                ).order_by(desc(AgentConversation.last_message_at)).limit(10).all()

                choices = []
                for conv in conversations:
                    # ç»Ÿè®¡æ¶ˆæ¯æ•°é‡
                    message_count = db.query(func.count(AgentMessage.id)).filter(
                        AgentMessage.conversation_id == conv.id
                    ).scalar()

                    # è·å–æœ€æ–°æ¶ˆæ¯é¢„è§ˆ
                    latest_message = db.query(AgentMessage).filter(
                        AgentMessage.conversation_id == conv.id
                    ).order_by(desc(AgentMessage.created_at)).first()

                    preview = ""
                    if latest_message:
                        content = latest_message.content or ""
                        preview = content[:30] + "..." if len(content) > 30 else content

                    # æ ¼å¼åŒ–é€‰æ‹©é¡¹
                    status_emoji = {
                        'active': 'ğŸ’¬',
                        'completed': 'âœ…',
                        'error': 'âŒ',
                        'paused': 'â¸ï¸'
                    }.get(conv.status, 'ğŸ’¬')

                    time_str = conv.last_message_at.strftime("%m-%d %H:%M") if conv.last_message_at else "N/A"
                    label = f"{status_emoji} {time_str} | {message_count}æ¡æ¶ˆæ¯ | {preview}"

                    choices.append((label, conv.id))

                # æ·»åŠ é»˜è®¤é€‰é¡¹
                if not choices:
                    choices = [("æ— å¯¹è¯è®°å½•", None)]

                return choices

        except Exception as e:
            logger.error(f"è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥: {e}")
            return [("æ— å¯¹è¯è®°å½•", None)]

    def restore_selected_conversation(self, plan_id: int, selected_conversation_id):
        """
        æ¢å¤é€‰æ‹©çš„å¯¹è¯

        Args:
            plan_id: è®¡åˆ’ID
            selected_conversation_id: é€‰æ‹©çš„å¯¹è¯ID
        """
        try:
            logger.info(f"å¼€å§‹æ¢å¤å¯¹è¯: plan_id={plan_id}, conversation_id={selected_conversation_id}")

            if not selected_conversation_id:
                logger.warning("é€‰æ‹©çš„å¯¹è¯IDä¸ºç©º")
                return []  # è¿”å›ç©ºå†å²

            restored_history = self.restore_conversation_from_records(plan_id, selected_conversation_id)
            logger.info(f"æˆåŠŸæ¢å¤å¯¹è¯ï¼ŒåŒ…å« {len(restored_history)} æ¡æ¶ˆæ¯")
            return restored_history

        except Exception as e:
            logger.error(f"æ¢å¤å¯¹è¯å¤±è´¥: {e}")
            import traceback
            logger.error(f"æ¢å¤å¯¹è¯å¤±è´¥è¯¦æƒ…: {traceback.format_exc()}")
            # è¿”å›é”™è¯¯æ¶ˆæ¯è€Œä¸æ˜¯ç©ºå†å²
            return [{"role": "assistant", "content": f"âŒ æ¢å¤å¯¹è¯å¤±è´¥: {str(e)}"}]