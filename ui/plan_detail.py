"""
è®¡åˆ’è¯¦æƒ…é¡µUI
åŒ…å«ï¼šä¸Šéƒ¨æ¦‚è§ˆã€å·¦ä¾§è®­ç»ƒåˆ—è¡¨ã€ä¸­é—´Kçº¿å›¾ã€å³ä¾§Agentè®°å½•ã€ä¸‹æ–¹è´¦æˆ·è®¢å•
"""
import gradio as gr
import plotly.graph_objects as go
import pandas as pd
import asyncio
import json
from datetime import datetime, timedelta, timezone
from typing import Optional, List, Dict
from services.plan_service import PlanService
from services.training_service import TrainingService
from services.inference_service import InferenceService
from services.conversation_service import ConversationService
from database.db import get_db
from database.models import TradingPlan, TrainingRecord, PredictionData, AgentDecision, KlineData, AgentConversation, AgentMessage
from sqlalchemy import and_, desc, func
from utils.logger import setup_logger
from ui.constants import DataFrameHeaders, DataTypes, create_empty_dataframe
from ui.ui_utils import UIHelper
from ui.custom_chatbot import create_custom_chatbot, process_streaming_messages, format_conversation_history
from database.models import now_beijing
from utils.timezone_helper import (format_datetime_full_beijing, format_datetime_short_beijing,
                                   format_datetime_beijing, format_time_range_utc8)

logger = setup_logger(__name__, "plan_detail_ui.log")


def safe_dataframe_from_data(data: List[Dict]) -> pd.DataFrame:
    """
    å®‰å…¨åˆ›å»ºDataFrameï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®ç±»å‹éƒ½é€‚åˆGradioæ˜¾ç¤º

    Args:
        data: å­—å…¸åˆ—è¡¨æ•°æ®

    Returns:
        pd.DataFrame: å®‰å…¨çš„DataFrame
    """
    if not data:
        return pd.DataFrame()

    safe_data = []
    for row in data:
        safe_row = {}
        for key, value in row.items():
            if value is None or pd.isna(value):
                safe_row[key] = 'N/A'
            elif isinstance(value, (int, float)):
                # ä¿æŒæ•°å­—ç±»å‹ï¼Œä½†ç¡®ä¿ä¸æ˜¯NaN
                if pd.isna(value):
                    safe_row[key] = 0
                else:
                    safe_row[key] = value
            else:
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                safe_row[key] = str(value)
        safe_data.append(safe_row)

    return pd.DataFrame(safe_data)


class PlanDetailUI:
    """è®¡åˆ’è¯¦æƒ…é¡µUI"""

    def __init__(self):
        self.current_plan_id = None

    def _safe_db_update(self, update_func, plan_id: int, max_retries: int = 3):
        """
        å®‰å…¨çš„æ•°æ®åº“æ›´æ–°æ“ä½œï¼Œå¸¦é‡è¯•æœºåˆ¶

        Args:
            update_func: æ›´æ–°å‡½æ•°ï¼Œæ¥æ”¶dbä¼šè¯ä½œä¸ºå‚æ•°
            plan_id: è®¡åˆ’ID
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for attempt in range(max_retries):
            try:
                from database.db import SessionLocal
                db = SessionLocal()
                try:
                    update_func(db)
                    db.commit()
                    return True
                finally:
                    db.close()
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"æ•°æ®åº“æ›´æ–°å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: plan_id={plan_id}, error={e}")
                    return False
                else:
                    logger.warning(f"æ•°æ®åº“æ›´æ–°å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•({attempt + 1}/{max_retries}): plan_id={plan_id}, error={e}")
                    import time
                    time.sleep(0.1 * (attempt + 1))
        return False

    def load_plan_data(self, plan_id: int) -> Dict:
        """åŠ è½½è®¡åˆ’æ•°æ®"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return {'error': 'è®¡åˆ’ä¸å­˜åœ¨'}

                # è·å–æœ€æ–°è®­ç»ƒè®°å½•
                latest_training = db.query(TrainingRecord).filter(
                    and_(
                        TrainingRecord.plan_id == plan_id,
                        TrainingRecord.status == 'completed',
                        TrainingRecord.is_active == True
                    )
                ).order_by(desc(TrainingRecord.created_at)).first()

                # è·å–æœ€æ–°Agentå†³ç­–
                latest_agent = db.query(AgentDecision).filter(
                    AgentDecision.plan_id == plan_id
                ).order_by(desc(AgentDecision.decision_time)).first()

                return {
                    'plan': plan,
                    'latest_training': latest_training,
                    'latest_agent': latest_agent
                }
        except Exception as e:
            logger.error(f"åŠ è½½è®¡åˆ’æ•°æ®å¤±è´¥: {e}")
            return {'error': str(e)}

    def render_plan_overview(self, plan_id: int) -> tuple:
        """
        æ¸²æŸ“è®¡åˆ’æ¦‚è§ˆï¼ˆä¸Šéƒ¨ï¼‰

        Returns:
            tuple: (overview_text, ws_status_text, ws_start_visible, ws_stop_visible, plan_status_text, plan_start_visible, plan_stop_visible)
        """
        data = self.load_plan_data(plan_id)
        if 'error' in data:
            return (
                f"âŒ é”™è¯¯: {data['error']}",  # overview_text
                "**WebSocketçŠ¶æ€**: âšª æœªè¿æ¥",  # ws_status_text
                True,  # ws_start_visible
                False, # ws_stop_visible
                "**è®¡åˆ’çŠ¶æ€**: âšª å·²åˆ›å»º",  # plan_status_text
                True,  # plan_start_visible
                False  # plan_stop_visible
            )

        plan = data['plan']
        latest_training = data['latest_training']
        latest_agent = data['latest_agent']

        training_version = latest_training.version if latest_training else "æœªè®­ç»ƒ"
        agent_time = format_datetime_full_beijing(latest_agent.decision_time) if latest_agent else "æ— è®°å½•"

        # è‡ªåŠ¨å¾®è°ƒæ—¶é—´è¡¨
        schedule = plan.auto_finetune_schedule or []
        schedule_str = ', '.join(schedule) if schedule else 'æœªé…ç½®'

        # è‡ªåŠ¨é¢„æµ‹é—´éš”æ—¶é—´
        inference_interval_hours = getattr(plan, 'auto_inference_interval_hours', 4)
        if inference_interval_hours and inference_interval_hours > 0:
            inference_schedule_str = f'{inference_interval_hours}å°æ—¶é—´éš”'
        else:
            inference_schedule_str = 'æœªé…ç½®'

        # è®¡åˆ’çŠ¶æ€
        plan_status_emoji = UIHelper.get_status_emoji(plan.status, detailed=True)

        overview = f"""
# ğŸ“Š {plan.plan_name}

---

**äº¤æ˜“å¯¹**: `{plan.inst_id}` | **æ—¶é—´é¢—ç²’åº¦**: `{plan.interval}` | **ç¯å¢ƒ**: {'ğŸ§ª æ¨¡æ‹Ÿç›˜' if plan.is_demo else 'ğŸ’° å®ç›˜'}

**è®¡åˆ’çŠ¶æ€**: {plan_status_emoji}

**æœ€æ–°æ¨¡å‹ç‰ˆæœ¬**: `{training_version}` | **AI Agentæœ€åè¿è¡Œ**: {agent_time}

**è‡ªåŠ¨å¾®è°ƒæ—¶é—´**: {schedule_str}

**è‡ªåŠ¨é¢„æµ‹æ—¶é—´**: {inference_schedule_str}

**åˆ›å»ºæ—¶é—´**: {format_datetime_full_beijing(plan.created_at)}

---
"""

        # è·å–æ§åˆ¶é¢æ¿çŠ¶æ€
        control_status = self.get_control_panel_status(plan_id)

        return (
            overview,  # overview_text
            control_status[0],  # ws_status_text
            control_status[1],  # ws_start_visible
            control_status[2],  # ws_stop_visible
            control_status[3],  # plan_status_text
            control_status[4],  # plan_start_visible
            control_status[5]   # plan_stop_visible
        )

    def get_control_panel_status(self, plan_id: int) -> tuple:
        """
        è·å–æ§åˆ¶é¢æ¿çŠ¶æ€

        Returns:
            tuple: (ws_status_text, ws_start_visible, ws_stop_visible, plan_status_text, plan_start_visible, plan_stop_visible)
        """
        data = self.load_plan_data(plan_id)
        if 'error' in data:
            return (
                "**WebSocketçŠ¶æ€**: âšª æœªè¿æ¥",  # ws_status_text
                True,  # ws_start_visible
                False, # ws_stop_visible
                "**è®¡åˆ’çŠ¶æ€**: âšª å·²åˆ›å»º",  # plan_status_text
                True,  # plan_start_visible
                False  # plan_stop_visible
            )

        plan = data['plan']

        # å¤šé‡æ£€æŸ¥WebSocketè¿æ¥çŠ¶æ€
        try:
            # 1. é¦–å…ˆæ£€æŸ¥WebSocketè®¢é˜…è¡¨çš„çŠ¶æ€ï¼ˆæœ€å¯é çš„æ•°æ®æºï¼‰
            from database.db import get_db
            from database.models import WebSocketSubscription

            ws_connected = False

            with get_db() as db:
                subscription = db.query(WebSocketSubscription).filter(
                    WebSocketSubscription.inst_id == plan.inst_id,
                    WebSocketSubscription.interval == plan.interval,
                    WebSocketSubscription.is_demo == plan.is_demo
                ).first()

                if subscription:
                    # å¦‚æœè®¢é˜…è¡¨ä¸­æœ‰è®°å½•ï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„åœ¨è¿è¡Œ
                    # ä¸»è¦æŒ‡æ ‡ï¼šçŠ¶æ€ä¸ºrunningä¸”æœ‰æ•°æ®æ¥æ”¶
                    if (subscription.status == 'running' and
                        subscription.total_received > 0 and
                        subscription.last_data_time):
                        ws_connected = True
                        logger.debug(f"WebSocketè¿è¡Œä¸­ (æ¥è‡ªè®¢é˜…è¡¨): plan_id={plan_id}, received={subscription.total_received}")

            # 2. å¦‚æœè®¢é˜…è¡¨æ˜¾ç¤ºæœªè¿æ¥ï¼Œå°è¯•è¿æ¥ç®¡ç†å™¨
            if not ws_connected:
                try:
                    from services.ws_connection_manager import ws_connection_manager
                    ws_status = ws_connection_manager.get_connection_status(
                        inst_id=plan.inst_id,
                        interval=plan.interval,
                        is_demo=plan.is_demo
                    )

                    if ws_status['exists'] and ws_status['thread_alive']:
                        ws_connected = ws_status['connected'] and ws_status['running']
                        logger.debug(f"WebSocketçŠ¶æ€ (æ¥è‡ªè¿æ¥ç®¡ç†å™¨): plan_id={plan_id}, connected={ws_connected}")

                except Exception as conn_error:
                    logger.debug(f"è¿æ¥ç®¡ç†å™¨çŠ¶æ€è·å–å¤±è´¥: {conn_error}")

            # 3. æœ€åå›é€€åˆ°æ•°æ®åº“ä¸­çš„çŠ¶æ€
            if not ws_connected and plan.ws_connected:
                ws_connected = plan.ws_connected
                logger.debug(f"WebSocketçŠ¶æ€ (æ¥è‡ªæ•°æ®åº“): plan_id={plan_id}, connected={ws_connected}")

            # 4. å¼‚æ­¥æ›´æ–°æ•°æ®åº“çŠ¶æ€ï¼ˆå¦‚æœå‘ç°ä¸ä¸€è‡´ï¼‰
            if ws_connected != plan.ws_connected:
                def update_plan_ws_status(db):
                    current_plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                    if current_plan and current_plan.ws_connected != ws_connected:
                        db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                            'ws_connected': ws_connected,
                            'last_sync_time': now_beijing()
                        })
                        logger.info(f"è®¡åˆ’WebSocketçŠ¶æ€å·²æ›´æ–°: plan_id={plan_id}, ws_connected={ws_connected}")

                self._safe_db_update(update_plan_ws_status, plan_id)

        except Exception as e:
            logger.error(f"è·å–WebSocketçŠ¶æ€å¤±è´¥: {e}")
            ws_connected = plan.ws_connected

        ws_status_text = "ğŸŸ¢ å·²è¿æ¥" if ws_connected else "âšª æœªè¿æ¥"
        ws_status_display = f"**WebSocketçŠ¶æ€**: {ws_status_text}"

        # è®¡åˆ’çŠ¶æ€
        plan_status_emoji = UIHelper.get_status_emoji(plan.status, detailed=True)
        plan_status_display = f"**è®¡åˆ’çŠ¶æ€**: {plan_status_emoji}"

        # WebSocketçŠ¶æ€æ§åˆ¶
        ws_start_visible = not ws_connected
        ws_stop_visible = ws_connected

        # è®¡åˆ’çŠ¶æ€æ§åˆ¶
        plan_start_visible = plan.status != 'running'
        plan_stop_visible = plan.status == 'running'

        return (
            ws_status_display,  # ws_status_text
            ws_start_visible,   # ws_start_visible
            ws_stop_visible,    # ws_stop_visible
            plan_status_display,  # plan_status_text
            plan_start_visible,  # plan_start_visible
            plan_stop_visible   # plan_stop_visible
        )

    def load_training_records(self, plan_id: int) -> pd.DataFrame:
        """åŠ è½½è®­ç»ƒè®°å½•åˆ—è¡¨ï¼ˆå·¦ä¾§ï¼‰"""
        try:
            records = TrainingService.list_training_records(plan_id)
            if not records:
                return pd.DataFrame()

            df_data = []
            for record in records:
                status_emoji = {
                    'waiting': 'â³',
                    'training': 'ğŸ”„',
                    'completed': 'âœ…',
                    'failed': 'âŒ'
                }.get(record['status'], 'â“')

                df_data.append({
                    'ID': record['id'],
                    'ç‰ˆæœ¬': record['version'],
                    'çŠ¶æ€': f"{status_emoji} {record['status']}",
                    'å¯ç”¨': 'âœ“' if record['is_active'] else 'âœ—',
                    'æ•°æ®é‡': record['data_count'] or 0,
                    'è®­ç»ƒæ—¶é•¿(ç§’)': record['train_duration'] or 0,  # æ”¹ä¸ºçº¯æ•°å­—
                    'åˆ›å»ºæ—¶é—´': format_datetime_short_beijing(record['created_at'])
                })

            return safe_dataframe_from_data(df_data)

        except Exception as e:
            logger.error(f"åŠ è½½è®­ç»ƒè®°å½•å¤±è´¥: {e}")
            return pd.DataFrame()

    def get_current_training_status(self, plan_id: int) -> str:
        """
        è·å–å½“å‰è®¡åˆ’çš„è®­ç»ƒçŠ¶æ€å’Œè¿›åº¦

        Args:
            plan_id: è®¡åˆ’ID

        Returns:
            æ ¼å¼åŒ–çš„è®­ç»ƒçŠ¶æ€å­—ç¬¦ä¸²
        """
        try:
            from services.training_service import TrainingService

            with get_db() as db:
                # æŸ¥æ‰¾å½“å‰è®¡åˆ’æ­£åœ¨è®­ç»ƒçš„è®°å½•
                training_record = db.query(TrainingRecord).filter(
                    and_(
                        TrainingRecord.plan_id == plan_id,
                        TrainingRecord.status.in_(['waiting', 'training'])
                    )
                ).order_by(TrainingRecord.created_at.desc()).first()

                if not training_record:
                    return None

                # è·å–è®­ç»ƒè¿›åº¦
                progress_info = TrainingService.get_training_progress(training_record.id)

                status_emoji = {
                    'waiting': 'â³',
                    'training': 'ğŸ”„'
                }.get(training_record.status, 'â“')

                if progress_info:
                    progress_percent = int(progress_info['progress'] * 100)
                    stage = progress_info['stage']
                    message = progress_info['message']
                    progress_bar = 'â–ˆ' * (progress_percent // 5) + 'â–‘' * (20 - progress_percent // 5)

                    return f"""
**{status_emoji} å½“å‰è®­ç»ƒçŠ¶æ€**

- **è®°å½•ID**: {training_record.id}
- **ç‰ˆæœ¬**: {training_record.version}
- **è¿›åº¦**: {progress_percent}%
- **é˜¶æ®µ**: {stage}
- **çŠ¶æ€**: {message}

`{progress_bar}` ({progress_percent}%)
                    """
                else:
                    return f"""
**{status_emoji} å½“å‰è®­ç»ƒçŠ¶æ€**

- **è®°å½•ID**: {training_record.id}
- **ç‰ˆæœ¬**: {training_record.version}
- **çŠ¶æ€**: {training_record.status}
- **ç­‰å¾…è¿›åº¦æ›´æ–°...**
                    """

        except Exception as e:
            logger.error(f"è·å–è®­ç»ƒçŠ¶æ€å¤±è´¥: {e}")
            return f"âŒ è·å–è®­ç»ƒçŠ¶æ€å¤±è´¥: {str(e)}"

    def get_training_options(self, plan_id: int) -> List[tuple]:
        """
        è·å–è®­ç»ƒ ID é€‰é¡¹åˆ—è¡¨ï¼ˆç”¨äºä¸‹æ‹‰é€‰æ‹©å™¨ï¼‰

        Returns:
            List[tuple]: [(æ˜¾ç¤ºæ–‡æœ¬, training_id), ...]
        """
        try:
            with get_db() as db:
                # è·å–æ‰€æœ‰å·²å®Œæˆä¸”æœ‰é¢„æµ‹æ•°æ®çš„è®­ç»ƒè®°å½•
                training_records = db.query(TrainingRecord).filter(
                    and_(
                        TrainingRecord.plan_id == plan_id,
                        TrainingRecord.status == 'completed'
                    )
                ).order_by(desc(TrainingRecord.created_at)).all()

                options = [("å…¨éƒ¨", None)]  # ç¬¬ä¸€é¡¹æ˜¯"å…¨éƒ¨"

                for record in training_records:
                    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æµ‹æ•°æ®
                    pred_count = db.query(func.count(PredictionData.id)).filter(
                        PredictionData.training_record_id == record.id
                    ).scalar()

                    if pred_count > 0:
                        # è·å–æ¨ç†æ—¶é—´ï¼ˆæœ€æ—©çš„é¢„æµ‹æ•°æ®åˆ›å»ºæ—¶é—´ï¼‰
                        first_pred = db.query(PredictionData).filter(
                            PredictionData.training_record_id == record.id
                        ).order_by(PredictionData.created_at.asc()).first()

                        inference_time_str = format_datetime_short_beijing(first_pred.created_at) if first_pred else ''

                        # æ ¼å¼ï¼šv1 (æ¨ç†: 12-20 10:30)
                        display_text = f"{record.version} (æ¨ç†: {inference_time_str})"
                        options.append((display_text, record.id))

                return options

        except Exception as e:
            logger.error(f"è·å–è®­ç»ƒé€‰é¡¹å¤±è´¥: {e}")
            return [("å…¨éƒ¨", None)]

    def generate_kline_chart(
        self,
        plan_id: int,
        show_predictions: bool = True,
        training_id: Optional[int] = None,
        last_days: int = 30
    ) -> go.Figure:
        """
        ç”ŸæˆKçº¿å›¾ï¼ˆä¸­é—´ï¼Œå«é¢„æµ‹æ•°æ®ï¼‰

        Args:
            plan_id: è®¡åˆ’ID
            show_predictions: æ˜¯å¦æ˜¾ç¤ºé¢„æµ‹
            training_id: è®­ç»ƒè®°å½•IDï¼ŒNoneè¡¨ç¤ºæ˜¾ç¤ºå…¨éƒ¨
            last_days: æ˜¾ç¤ºæœ€è¿‘å¤šå°‘å¤©
        """
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return self._empty_chart("è®¡åˆ’ä¸å­˜åœ¨")

                # æŸ¥è¯¢å†å²Kçº¿æ•°æ®
                end_time = datetime.now()
                start_time = end_time - pd.Timedelta(days=last_days)

                klines = db.query(KlineData).filter(
                    and_(
                        KlineData.inst_id == plan.inst_id,
                        KlineData.interval == plan.interval,
                        KlineData.timestamp >= start_time,
                        KlineData.timestamp <= end_time
                    )
                ).order_by(KlineData.timestamp).all()

                if not klines:
                    return self._empty_chart("æ— å†å²æ•°æ®")

                # åˆ›å»ºKçº¿å›¾
                fig = go.Figure()

                # Kçº¿æ•°æ®æ—¶é—´æˆ³å·²ç»æ˜¯åŒ—äº¬æ—¶é—´ï¼Œç›´æ¥ä½¿ç”¨
                timestamps_beijing = [k.timestamp for k in klines]

                # æ·»åŠ çœŸå®Kçº¿
                fig.add_trace(go.Candlestick(
                    x=timestamps_beijing,
                    open=[k.open for k in klines],
                    high=[k.high for k in klines],
                    low=[k.low for k in klines],
                    close=[k.close for k in klines],
                    name='å®é™…Kçº¿',
                    increasing_line_color='#26a69a',
                    decreasing_line_color='#ef5350'
                ))

                # æ·»åŠ é¢„æµ‹æ•°æ®
                if show_predictions:
                    # å¦‚æœ training_id ä¸º Noneï¼Œæ˜¾ç¤ºæ‰€æœ‰å†å²é¢„æµ‹
                    if training_id is None:
                        # è·å–æ‰€æœ‰å·²å®Œæˆçš„è®­ç»ƒè®°å½•
                        all_training_records = db.query(TrainingRecord).filter(
                            and_(
                                TrainingRecord.plan_id == plan_id,
                                TrainingRecord.status == 'completed'
                            )
                        ).order_by(TrainingRecord.created_at.asc()).all()  # ä»æ—§åˆ°æ–°

                        # ä¸ºæ¯ä¸ªè®­ç»ƒè®°å½•åˆ†é…é¢œè‰²ï¼ˆç›¸åŒè®­ç»ƒç‰ˆæœ¬çš„æ‰€æœ‰æ‰¹æ¬¡ä½¿ç”¨ç›¸åŒé¢œè‰²ï¼‰
                        colors = ['#ff9800', '#2196f3', '#4caf50', '#9c27b0', '#f44336', '#00bcd4']

                        for record_idx, record in enumerate(all_training_records):
                            # ä¸ºå½“å‰è®­ç»ƒç‰ˆæœ¬åˆ†é…é¢œè‰²
                            record_color = colors[record_idx % len(colors)]

                            # è·å–è¯¥è®­ç»ƒè®°å½•çš„æ‰€æœ‰æ¨ç†æ‰¹æ¬¡
                            batches = db.query(
                                PredictionData.inference_batch_id,
                                func.min(PredictionData.created_at).label('inference_time')
                            ).filter(
                                PredictionData.training_record_id == record.id
                            ).group_by(
                                PredictionData.inference_batch_id
                            ).order_by(
                                func.min(PredictionData.created_at).asc()
                            ).all()

                            for batch_idx, batch in enumerate(batches):
                                predictions = db.query(PredictionData).filter(
                                    and_(
                                        PredictionData.training_record_id == record.id,
                                        PredictionData.inference_batch_id == batch.inference_batch_id
                                    )
                                ).order_by(PredictionData.timestamp).all()

                                if predictions:
                                    # åªåœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ—¶æ˜¾ç¤ºå›¾ä¾‹
                                    show_in_legend = (batch_idx == 0)
                                    self._add_prediction_trace(
                                        fig,
                                        predictions,
                                        record.id,
                                        f"{record.version} (æ¨ç†: {format_datetime_short_beijing(batch.inference_time)})",
                                        batch.inference_time,
                                        record_color,  # ç›¸åŒè®­ç»ƒç‰ˆæœ¬ä½¿ç”¨ç›¸åŒé¢œè‰²
                                        show_in_legend=show_in_legend
                                    )
                    else:
                        # æ˜¾ç¤ºæŒ‡å®šè®­ç»ƒè®°å½•çš„æ‰€æœ‰æ‰¹æ¬¡é¢„æµ‹
                        record = db.query(TrainingRecord).filter(
                            TrainingRecord.id == training_id
                        ).first()

                        if record:
                            # ä¸ºè¯¥è®­ç»ƒç‰ˆæœ¬åˆ†é…ä¸€ä¸ªå›ºå®šé¢œè‰²ï¼ˆæ‰€æœ‰æ‰¹æ¬¡ä½¿ç”¨ç›¸åŒé¢œè‰²ï¼‰
                            colors = ['#ff9800', '#2196f3', '#4caf50', '#9c27b0', '#f44336', '#00bcd4']
                            record_color = colors[record.id % len(colors)]

                            # è·å–è¯¥è®­ç»ƒè®°å½•çš„æ‰€æœ‰æ¨ç†æ‰¹æ¬¡
                            batches = db.query(
                                PredictionData.inference_batch_id,
                                func.min(PredictionData.created_at).label('inference_time')
                            ).filter(
                                PredictionData.training_record_id == training_id
                            ).group_by(
                                PredictionData.inference_batch_id
                            ).order_by(
                                func.min(PredictionData.created_at).asc()
                            ).all()

                            for batch_idx, batch in enumerate(batches):
                                predictions = db.query(PredictionData).filter(
                                    and_(
                                        PredictionData.training_record_id == training_id,
                                        PredictionData.inference_batch_id == batch.inference_batch_id
                                    )
                                ).order_by(PredictionData.timestamp).all()

                                if predictions:
                                    # åªåœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ—¶æ˜¾ç¤ºå›¾ä¾‹
                                    show_in_legend = (batch_idx == 0)
                                    self._add_prediction_trace(
                                        fig,
                                        predictions,
                                        record.id,
                                        f"{record.version} (æ¨ç†: {format_datetime_short_beijing(batch.inference_time)})",
                                        batch.inference_time,
                                        record_color,  # ç›¸åŒè®­ç»ƒç‰ˆæœ¬ä½¿ç”¨ç›¸åŒé¢œè‰²
                                        show_in_legend=show_in_legend
                                    )

                fig.update_layout(
                    title=f"{plan.inst_id} {plan.interval} Kçº¿å›¾ (æœ€è¿‘{last_days}å¤©)",
                    xaxis_title="æ—¶é—´ (UTC+8)",
                    yaxis_title="ä»·æ ¼",
                    height=600,
                    template="plotly_white",
                    hovermode='x unified',
                    xaxis_rangeslider_visible=False
                )

                return fig

        except Exception as e:
            logger.error(f"ç”ŸæˆKçº¿å›¾å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._empty_chart(f"ç”Ÿæˆå¤±è´¥: {str(e)}")

    def _add_prediction_trace(
        self,
        fig: go.Figure,
        predictions: List[PredictionData],
        training_id: int,
        version: str,
        inference_time: datetime,
        color: str,
        show_in_legend: bool = True
    ):
        """
        æ·»åŠ é¢„æµ‹è½¨è¿¹åˆ°å›¾è¡¨

        Args:
            fig: Plotly Figureå¯¹è±¡
            predictions: é¢„æµ‹æ•°æ®åˆ—è¡¨
            training_id: è®­ç»ƒè®°å½•ID
            version: ç‰ˆæœ¬å·ï¼ˆå·²åŒ…å«æ¨ç†æ—¶é—´ï¼‰
            inference_time: æ¨ç†æ—¶é—´
            color: çº¿æ¡é¢œè‰²
            show_in_legend: æ˜¯å¦åœ¨å›¾ä¾‹ä¸­æ˜¾ç¤ºï¼ˆç”¨äºæ§åˆ¶åŒä¸€è®­ç»ƒç‰ˆæœ¬åªæ˜¾ç¤ºä¸€æ¬¡ï¼‰
        """
        # é¢„æµ‹æ•°æ®æ—¶é—´æˆ³å·²ç»æ˜¯åŒ—äº¬æ—¶é—´ï¼Œç›´æ¥ä½¿ç”¨
        pred_timestamps_beijing = [p.timestamp for p in predictions]

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸ç¡®å®šæ€§æ•°æ®
        has_uncertainty = any(p.close_min is not None and p.close_max is not None for p in predictions)

        if has_uncertainty:
            # ç»˜åˆ¶ä¸ç¡®å®šæ€§é˜´å½±åŒºåŸŸ
            # 1. ä¸Šè¾¹ç•Œ
            fig.add_trace(go.Scatter(
                x=pred_timestamps_beijing,
                y=[p.close_max if p.close_max is not None else p.close for p in predictions],
                mode='lines',
                line=dict(width=0),
                showlegend=False,
                hoverinfo='skip',
                legendgroup=f'group_{training_id}'
            ))

            # 2. ä¸‹è¾¹ç•Œï¼ˆå¡«å……é˜´å½±ï¼‰
            fig.add_trace(go.Scatter(
                x=pred_timestamps_beijing,
                y=[p.close_min if p.close_min is not None else p.close for p in predictions],
                mode='lines',
                fill='tonexty',
                fillcolor=f'rgba({int(color[1:3], 16)}, {int(color[3:5], 16)}, {int(color[5:7], 16)}, 0.2)',
                line=dict(width=0),
                name=f'{version.split(" ")[0]} ä¸ç¡®å®šæ€§' if show_in_legend else '',
                showlegend=show_in_legend and has_uncertainty,
                hovertemplate='<b>ä¸ç¡®å®šæ€§èŒƒå›´</b><br>æœ€é«˜: %{customdata[0]:.2f}<br>æœ€ä½: %{y:.2f}<extra></extra>',
                customdata=[[p.close_max if p.close_max is not None else p.close] for p in predictions],
                legendgroup=f'group_{training_id}'
            ))

        # æ ¼å¼åŒ–æ¨ç†æ—¶é—´
        inference_time_str = format_datetime_short_beijing(inference_time)

        # 3. å¹³å‡å€¼çº¿æ¡
        fig.add_trace(go.Scatter(
            x=pred_timestamps_beijing,
            y=[p.close for p in predictions],
            mode='lines+markers',
            name=version,
            showlegend=show_in_legend,
            line=dict(color=color, width=2, dash='dash'),
            marker=dict(size=4),
            hovertemplate=f'<b>{version}</b><br>æ—¶é—´: %{{x}}<br>æ”¶ç›˜ä»·: %{{y:.2f}}<extra></extra>',
            legendgroup=f'group_{training_id}'
        ))

    def _empty_chart(self, message: str) -> go.Figure:
        """ç©ºå›¾è¡¨"""
        fig = go.Figure()
        fig.add_annotation(
            text=message,
            xref="paper", yref="paper",
            x=0.5, y=0.5,
            showarrow=False,
            font=dict(size=16)
        )
        fig.update_layout(height=600)
        return fig

    def load_agent_decisions(self, plan_id: int) -> pd.DataFrame:
        """åŠ è½½Agentå¯¹è¯è®°å½•ï¼ˆå³ä¾§ï¼‰"""
        try:
            with get_db() as db:
                # æŸ¥è¯¢è¯¥è®¡åˆ’ç›¸å…³çš„å¯¹è¯ä¼šè¯
                conversations = db.query(AgentConversation).filter(
                    AgentConversation.plan_id == plan_id
                ).order_by(desc(AgentConversation.last_message_at)).limit(20).all()

                if not conversations:
                    return pd.DataFrame()

                df_data = []
                for conv in conversations:
                    # è·å–è¯¥ä¼šè¯çš„æ¶ˆæ¯æ•°é‡
                    message_count = db.query(func.count(AgentMessage.id)).filter(
                        AgentMessage.conversation_id == conv.id
                    ).scalar()

                    # ç»Ÿè®¡å·¥å…·è°ƒç”¨æ¬¡æ•°
                    tool_call_count = db.query(func.count(AgentMessage.id)).filter(
                        and_(
                            AgentMessage.conversation_id == conv.id,
                            AgentMessage.message_type.in_(['tool_call', 'tool_result'])
                        )
                    ).scalar()

                    status_emoji = {
                        'active': 'ğŸ’¬',
                        'completed': 'âœ…',
                        'error': 'âŒ',
                        'paused': 'â¸ï¸'
                    }.get(conv.status, 'ğŸ’¬')

                    # è·å–æœ€æ–°æ¶ˆæ¯çš„å†…å®¹é¢„è§ˆ
                    latest_message = db.query(AgentMessage).filter(
                        AgentMessage.conversation_id == conv.id
                    ).order_by(desc(AgentMessage.created_at)).first()

                    content_preview = ""
                    if latest_message:
                        # æˆªå–å†…å®¹å‰50ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
                        content = latest_message.content or ""
                        if len(content) > 50:
                            content_preview = content[:47] + "..."
                        else:
                            content_preview = content

                    df_data.append({
                        'ID': conv.id,
                        'æ—¶é—´': format_datetime_full_beijing(conv.last_message_at) if conv.last_message_at else 'N/A',
                        'ä¼šè¯ç±»å‹': conv.conversation_type or 'analysis',
                        'çŠ¶æ€': f"{status_emoji} {conv.status}",
                        'æ¶ˆæ¯æ•°': message_count,
                        'å·¥å…·è°ƒç”¨': tool_call_count,
                        'é¢„è§ˆ': content_preview
                    })

                return safe_dataframe_from_data(df_data)

        except Exception as e:
            logger.error(f"åŠ è½½Agentå¯¹è¯è®°å½•å¤±è´¥: {e}")
            return pd.DataFrame()

    def load_inference_records(self, plan_id: int) -> pd.DataFrame:
        """åŠ è½½Kronosæ¨ç†è®°å½•åˆ—è¡¨"""
        try:
            records = InferenceService.list_inference_records(plan_id)
            if not records:
                return pd.DataFrame()

            df_data = []
            for record in records:
                has_pred_emoji = 'âœ…' if record['has_predictions'] else 'âšª'
                inference_time_str = format_datetime_short_beijing(record['inference_time']) if record['inference_time'] else 'N/A'

                df_data.append({
                    'ID': record['training_record_id'],
                    'ç‰ˆæœ¬': record['version'],
                    'æ¨ç†æ—¶é—´': inference_time_str,
                    'é¢„æµ‹æ•°æ®': f"{has_pred_emoji} {record['predictions_count']}æ¡",
                    'æ•°æ®èŒƒå›´': record.get('date_range', 'N/A'),
                    'è®­ç»ƒå®Œæˆ': format_datetime_short_beijing(record['train_end_time']) if record['train_end_time'] else 'N/A'
                })

            return safe_dataframe_from_data(df_data)

        except Exception as e:
            logger.error(f"åŠ è½½æ¨ç†è®°å½•å¤±è´¥: {e}")
            return pd.DataFrame()

    def get_inference_data_timestamp_range(self, plan_id: int, lookback_window: int = None, data_offset: int = None) -> str:
        """åŸºäºæœ€æ–°Kçº¿æ•°æ®å’Œå›çœ‹çª—å£åŠ¨æ€è®¡ç®—æ¨ç†æ•°æ®ç‚¹æ—¶é—´æˆ³èŒƒå›´"""
        try:
            with get_db() as db:
                # è·å–äº¤æ˜“è®¡åˆ’ä¿¡æ¯
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "**ğŸ“Š æ¨ç†æ•°æ®ç‚¹èŒƒå›´**\n\nè®¡åˆ’ä¸å­˜åœ¨"

                # è·å–æœ€æ–°çš„Kçº¿æ•°æ®ï¼ˆæœ€åçš„æ•°æ®ç‚¹ï¼‰
                latest_kline = db.query(KlineData).filter(
                    KlineData.inst_id == plan.inst_id,
                    KlineData.interval == plan.interval
                ).order_by(KlineData.timestamp.desc()).first()

                if not latest_kline:
                    return "**ğŸ“Š æ¨ç†æ•°æ®ç‚¹èŒƒå›´**\n\næš‚æ— Kçº¿æ•°æ®"

                # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–é»˜è®¤å€¼
                if lookback_window is None:
                    # ä»è®¡åˆ’çš„å¾®è°ƒå‚æ•°ä¸­è·å–å›çœ‹çª—å£ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
                    lookback_window = 200  # é»˜è®¤200ä¸ªæ•°æ®ç‚¹
                    if plan.finetune_params and isinstance(plan.finetune_params, dict):
                        lookback_window = plan.finetune_params.get('lookback_window', 200)

                if data_offset is None:
                    data_offset = 0  # é»˜è®¤æ— åç§»

                # è®¡ç®—æ¨ç†æ•°æ®çš„èµ·å§‹æ—¶é—´æˆ³
                # æ¨ç†ä½¿ç”¨çš„æ•°æ®ç‚¹èŒƒå›´ï¼šä»æœ€æ–°æ•°æ®ç‚¹å‘å‰æ¨ lookback_window + data_offset ä¸ªæ•°æ®ç‚¹
                total_data_points = lookback_window + data_offset

                # è·å–ç”¨äºæ¨ç†çš„æ•°æ®èµ·å§‹ç‚¹
                start_kline = db.query(KlineData).filter(
                    KlineData.inst_id == plan.inst_id,
                    KlineData.interval == plan.interval,
                    KlineData.timestamp <= latest_kline.timestamp
                ).order_by(KlineData.timestamp.desc()).offset(total_data_points - 1).first()

                if not start_kline:
                    # å¦‚æœæ•°æ®ç‚¹ä¸å¤Ÿï¼Œè·å–æœ€æ—©çš„æ•°æ®ç‚¹
                    start_kline = db.query(KlineData).filter(
                        KlineData.inst_id == plan.inst_id,
                        KlineData.interval == plan.interval
                    ).order_by(KlineData.timestamp.asc()).first()

                # è®¡ç®—æ•°æ®ç‚¹æ€»æ•°
                total_count = db.query(KlineData).filter(
                    KlineData.inst_id == plan.inst_id,
                    KlineData.interval == plan.interval,
                    KlineData.timestamp >= start_kline.timestamp,
                    KlineData.timestamp <= latest_kline.timestamp
                ).count()

                # æ ¼å¼åŒ–æ—¶é—´èŒƒå›´æ˜¾ç¤ºï¼ˆKçº¿æ•°æ®æ—¶é—´æˆ³å·²ç»æ˜¯åŒ—äº¬æ—¶é—´ï¼Œç›´æ¥æ ¼å¼åŒ–ï¼‰
                start_time = start_kline.timestamp.strftime('%Y-%m-%d %H:%M')
                end_time = latest_kline.timestamp.strftime('%Y-%m-%d %H:%M')

                # è®¡ç®—æ—¶é—´è·¨åº¦
                time_diff = latest_kline.timestamp - start_kline.timestamp
                days = time_diff.days
                hours = time_diff.seconds // 3600

                time_span = ""
                if days > 0:
                    time_span = f"{days}å¤©"
                if hours > 0:
                    time_span += f"{hours}å°æ—¶" if time_span else f"{hours}å°æ—¶"

                range_info = f"""**ğŸ“Š æ¨ç†æ•°æ®ç‚¹èŒƒå›´**

**ğŸ“… æ—¶é—´èŒƒå›´**: {start_time} ~ {end_time}
**ğŸ“ˆ æ•°æ®ç‚¹æ•°é‡**: {total_count}æ¡
**â±ï¸ æ—¶é—´è·¨åº¦**: {time_span or 'ä¸è¶³1å°æ—¶'}
**ğŸ”§ å›çœ‹çª—å£**: {lookback_window}ä¸ªæ•°æ®ç‚¹
**ğŸ“ æ•°æ®åç§»**: {data_offset}ä¸ªæ•°æ®ç‚¹
**ğŸ’¡ æœ€æ–°æ•°æ®**: {latest_kline.timestamp.strftime('%Y-%m-%d %H:%M:%S')}"""

                return range_info

        except Exception as e:
            logger.error(f"è·å–æ¨ç†æ•°æ®ç‚¹æ—¶é—´æˆ³èŒƒå›´å¤±è´¥: {e}")
            return "**ğŸ“Š æ¨ç†æ•°æ®ç‚¹èŒƒå›´**\n\nè·å–å¤±è´¥: " + str(e)

    def get_data_range_info(self, plan_id: int) -> str:
        """è·å–æ•°æ®è¾“å…¥çš„å›çœ‹æ•°æ®æ—¥æœŸæ—¶é—´èŒƒå›´ä¿¡æ¯"""
        try:
            # è·å–æœ€æ–°çš„æ¨ç†è®°å½•ä¸­çš„æ•°æ®èŒƒå›´
            records = InferenceService.list_inference_records(plan_id)
            if not records:
                return "**ğŸ“Š Dataè¾“å…¥ä¿¡æ¯**\n\næš‚æ— æ¨ç†è®°å½•"

            # ä½¿ç”¨æœ€æ–°çš„æ¨ç†è®°å½•çš„æ•°æ®èŒƒå›´
            latest_record = records[0]  # recordsæ˜¯æŒ‰åˆ›å»ºæ—¶é—´é™åºæ’åˆ—çš„

            if latest_record.get('datetime_range') and latest_record.get('datetime_range') != 'N/A':
                data_range_info = f"""**ğŸ“Š Dataè¾“å…¥ä¿¡æ¯**

**æ•°æ®æ—¶é—´èŒƒå›´**: {latest_record.get('datetime_range')}

**è®­ç»ƒç‰ˆæœ¬**: {latest_record.get('version', 'N/A')}
**è®­ç»ƒå®Œæˆæ—¶é—´**: {format_datetime_beijing(latest_record.get('train_end_time'), '%Y-%m-%d %H:%M') if latest_record.get('train_end_time') else 'N/A'}
**é¢„æµ‹æ•°æ®æ¡æ•°**: {latest_record.get('predictions_count', 0)}æ¡"""
            else:
                data_range_info = f"""**ğŸ“Š Dataè¾“å…¥ä¿¡æ¯**

**æ•°æ®æ—¶é—´èŒƒå›´**: æš‚æ— æ•°æ®
**è®­ç»ƒç‰ˆæœ¬**: {latest_record.get('version', 'N/A')}
**è®­ç»ƒå®Œæˆæ—¶é—´**: {format_datetime_beijing(latest_record.get('train_end_time'), '%Y-%m-%d %H:%M') if latest_record.get('train_end_time') else 'N/A'}
**é¢„æµ‹æ•°æ®æ¡æ•°**: {latest_record.get('predictions_count', 0)}æ¡"""

            return data_range_info

        except Exception as e:
            logger.error(f"è·å–æ•°æ®èŒƒå›´ä¿¡æ¯å¤±è´¥: {e}")
            return "**ğŸ“Š Dataè¾“å…¥ä¿¡æ¯**\n\nè·å–æ•°æ®èŒƒå›´ä¿¡æ¯å¤±è´¥"

    def get_agent_decision_detail(self, decision_id: int) -> str:
        """è·å–Agentå†³ç­–è¯¦æƒ…"""
        try:
            with get_db() as db:
                decision = db.query(AgentDecision).filter(
                    AgentDecision.id == decision_id
                ).first()

                if not decision:
                    return "å†³ç­–è®°å½•ä¸å­˜åœ¨"

                # è·å–äº¤æ˜“è®¡åˆ’ä¿¡æ¯ä»¥æ˜¾ç¤ºäº¤æ˜“é™åˆ¶
                plan = db.query(TradingPlan).filter(
                    TradingPlan.id == decision.plan_id
                ).first()

                trading_limits_info = ""
                if plan and plan.trading_limits:
                    limits = plan.trading_limits
                    trading_limits_info = f"""
### ğŸ’° äº¤æ˜“é™åˆ¶é…ç½®

- **å¯ç”¨è´¦æˆ·èµ„é‡‘**: {limits.get('available_usdt_amount', 'N/A')} USDT
- **å¯ç”¨èµ„é‡‘æ¯”ä¾‹**: {limits.get('available_usdt_percentage', 'N/A')}%
- **å¹³æ‘Šæ“ä½œå•é‡**: {limits.get('avg_order_count', 'N/A')} ç¬”
- **æ­¢æŸæ¯”ä¾‹**: {limits.get('stop_loss_percentage', 'N/A')}%
- **æœ€å¤§æŒä»“**: {limits.get('max_position_size', 'N/A')}
- **æœ€å¤§è®¢å•é‡‘é¢**: {limits.get('max_order_amount', 'N/A')} USDT

"""

                detail = f"""
## ğŸ“‹ Agentå†³ç­–è¯¦æƒ… (ID: {decision.id})

**å†³ç­–æ—¶é—´**: {format_datetime_full_beijing(decision.decision_time)}
**å†³ç­–ç±»å‹**: `{decision.decision_type}`
**çŠ¶æ€**: `{decision.status}`
**ä½¿ç”¨æ¨¡å‹**: v{decision.training_record_id} | **LLM**: {decision.llm_model or 'N/A'}

{trading_limits_info}

---

### ğŸ’­ å†³ç­–ç†ç”±
{decision.reasoning or 'æ— '}

---

### ğŸ› ï¸ å·¥å…·è°ƒç”¨
{self._format_tool_calls(decision.tool_calls, decision.tool_results)}

---

### ğŸ“¦ å…³è”è®¢å•
{self._format_order_ids(decision.order_ids)}
"""
                return detail

        except Exception as e:
            logger.error(f"è·å–å†³ç­–è¯¦æƒ…å¤±è´¥: {e}")
            return f"è·å–å¤±è´¥: {str(e)}"

    def _format_tool_calls(self, tool_calls, tool_results) -> str:
        """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨ - æ˜¾ç¤ºå®Œæ•´çš„ReActè¿‡ç¨‹"""
        if not tool_calls:
            return "æ— å·¥å…·è°ƒç”¨"

        lines = []
        lines.append("### ğŸ”„ ReAct å·¥å…·è°ƒç”¨è¿‡ç¨‹")
        lines.append("")

        for i, call in enumerate(tool_calls, 1):
            tool_name = call.get('name', 'unknown')
            tool_args = call.get('arguments', {})
            result = tool_results[i-1] if tool_results and len(tool_results) >= i else {}

            # å·¥å…·è°ƒç”¨æ­¥éª¤
            lines.append(f"#### **æ­¥éª¤ {i}: è°ƒç”¨å·¥å…· `{tool_name}`**")

            # æ˜¾ç¤ºå·¥å…·å‚æ•°
            lines.append("**ğŸ“ è°ƒç”¨å‚æ•°:**")
            if tool_args:
                for key, value in tool_args.items():
                    lines.append(f"   - `{key}`: `{value}`")
            else:
                lines.append("   - æ— å‚æ•°")

            # æ˜¾ç¤ºæ‰§è¡Œç»“æœ
            lines.append("**âš¡ æ‰§è¡Œç»“æœ:**")
            if result:
                success = result.get('success', False)
                status_emoji = 'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'
                lines.append(f"   - **çŠ¶æ€**: {status_emoji}")

                if success:
                    # æˆåŠŸæ—¶æ˜¾ç¤ºæ•°æ®æ‘˜è¦
                    data = result.get('data')
                    if data:
                        if isinstance(data, dict):
                            if 'message' in data:
                                lines.append(f"   - **æ¶ˆæ¯**: {data['message']}")
                            if 'count' in data:
                                lines.append(f"   - **æ•°æ®é‡**: {data['count']} æ¡")
                            if 'total' in data:
                                lines.append(f"   - **æ€»æ•°**: {data['total']}")
                            if 'records' in data and isinstance(data['records'], list):
                                lines.append(f"   - **è®°å½•æ•°**: {len(data['records'])} æ¡")
                        elif isinstance(data, list):
                            lines.append(f"   - **æ•°ç»„é•¿åº¦**: {len(data)} é¡¹")

                    if result.get('message'):
                        lines.append(f"   - **è¯´æ˜**: {result['message']}")
                else:
                    # å¤±è´¥æ—¶æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                    error_msg = result.get('error', result.get('message', 'æœªçŸ¥é”™è¯¯'))
                    lines.append(f"   - **é”™è¯¯**: {error_msg}")
            else:
                lines.append("   - **çŠ¶æ€**: âš ï¸ æ— ç»“æœæ•°æ®")

            lines.append("")
            lines.append("---")
            lines.append("")

        return "\n".join(lines)

    def _format_order_ids(self, order_ids) -> str:
        """æ ¼å¼åŒ–è®¢å•ID"""
        if not order_ids:
            return "æ— å…³è”è®¢å•"

        return ", ".join([f"`{oid}`" for oid in order_ids])

    def get_latest_agent_decision_output(self, plan_id: int):
        """
        è·å–æœ€æ–°çš„Agentå†³ç­–è¾“å‡ºï¼ˆChatbotæ ¼å¼ï¼‰ï¼ŒåŒ…å«æœ€æ–°çš„é¢„æµ‹æ•°æ®é¢„è§ˆ

        Returns:
            List[Dict]: Chatbot messages æ ¼å¼ [{"role": "assistant", "content": ...}]
        """
        try:
            with get_db() as db:
                # è·å–æœ€æ–°çš„Agentå†³ç­–
                decision = db.query(AgentDecision).filter(
                    AgentDecision.plan_id == plan_id
                ).order_by(desc(AgentDecision.decision_time)).first()

                # è·å–æœ€æ–°çš„å·²å®Œæˆè®­ç»ƒè®°å½•
                latest_training = db.query(TrainingRecord).filter(
                    and_(
                        TrainingRecord.plan_id == plan_id,
                        TrainingRecord.status == 'completed',
                        TrainingRecord.is_active == True
                    )
                ).order_by(desc(TrainingRecord.created_at)).first()

                # æ„å»ºè¾“å‡ºå†…å®¹
                output_parts = []

                # æ·»åŠ é¢„æµ‹æ•°æ®é¢„è§ˆéƒ¨åˆ†
                if latest_training:
                    # è·å–æœ€åä¸€æ¬¡æ¨ç†çš„é¢„æµ‹æ•°æ®
                    from sqlalchemy import func
                    latest_batch = db.query(
                        PredictionData.inference_batch_id,
                        func.max(PredictionData.created_at).label('max_time')
                    ).filter(
                        PredictionData.training_record_id == latest_training.id
                    ).group_by(
                        PredictionData.inference_batch_id
                    ).order_by(
                        func.max(PredictionData.created_at).desc()
                    ).first()

                    if latest_batch:
                        # è·å–è¯¥æ‰¹æ¬¡çš„æ‰€æœ‰é¢„æµ‹æ•°æ®
                        predictions_query = db.query(PredictionData).filter(
                            and_(
                                PredictionData.training_record_id == latest_training.id,
                                PredictionData.inference_batch_id == latest_batch.inference_batch_id
                            )
                        ).order_by(PredictionData.timestamp).all()

                        if predictions_query:
                            predictions = []
                            for pred in predictions_query:
                                predictions.append({
                                    'timestamp': pred.timestamp,
                                    'open': pred.open,
                                    'high': pred.high,
                                    'low': pred.low,
                                    'close': pred.close,
                                    'volume': pred.volume
                                })

                            # æ ¼å¼åŒ–é¢„æµ‹æ•°æ®é¢„è§ˆ
                            pred_output = self._format_prediction_preview(predictions, latest_batch.inference_batch_id, latest_training.version)
                            output_parts.append(pred_output)

                # æ·»åŠ AI Agentå†³ç­–ç»“æœéƒ¨åˆ†
                if decision:
                    # åˆ›å»ºå®Œæ•´çš„ReActå¯¹è¯æ ¼å¼
                    messages = []

                    # 1. ç³»ç»Ÿå’Œåˆå§‹æç¤º
                    messages.append({
                        "role": "system",
                        "content": f"AI Agent äº¤æ˜“åˆ†æç³»ç»Ÿ\nä½¿ç”¨æ¨¡å‹: v{decision.training_record_id} | LLM: {decision.llm_model or 'N/A'}\nå†³ç­–æ—¶é—´: {format_datetime_full_beijing(decision.decision_time)}"
                    })

                    # 2. ç”¨æˆ·è¾“å…¥ï¼ˆé¢„æµ‹æ•°æ®ï¼‰
                    if output_parts:
                        messages.append({
                            "role": "user",
                            "content": output_parts[-1] if output_parts else "è¯·åˆ†ææœ€æ–°çš„å¸‚åœºé¢„æµ‹æ•°æ®å¹¶æä¾›äº¤æ˜“å»ºè®®ã€‚"
                        })

                    # 3. AIæ€è€ƒè¿‡ç¨‹ï¼ˆæ¨ç†ï¼‰
                    reasoning_content = f"""## ğŸ§  AIæ€è€ƒè¿‡ç¨‹

**ğŸ“Š åˆ†æé˜¶æ®µ**:
- æ­£åœ¨åˆ†ææœ€æ–°çš„Kronosé¢„æµ‹æ•°æ®
- è¯„ä¼°å¸‚åœºè¶‹åŠ¿å’Œä»·æ ¼æ³¢åŠ¨
- ç»“åˆäº¤æ˜“é™åˆ¶å’Œé£é™©æ§åˆ¶è¦æ±‚

**ğŸ’¡ å†³ç­–é€»è¾‘**:
{decision.reasoning or 'æ— è¯¦ç»†æ¨ç†è¿‡ç¨‹'}

**ğŸ¯ å†³ç­–ç»“è®º**:
- **å†³ç­–ç±»å‹**: {decision.decision_type or 'N/A'}
- **æ‰§è¡ŒçŠ¶æ€**: {decision.status}
- **å»ºè®®æ“ä½œ**: åŸºäºåˆ†æç»“æœå†³å®šæ˜¯å¦æ‰§è¡Œäº¤æ˜“

"""
                    messages.append({
                        "role": "assistant",
                        "content": reasoning_content
                    })

                    # 4. å·¥å…·è°ƒç”¨è¿‡ç¨‹
                    if decision.tool_calls:
                        tool_process_content = "## ğŸ”§ å·¥å…·æ‰§è¡Œé˜¶æ®µ\n\n"

                        for i, call in enumerate(decision.tool_calls, 1):
                            tool_name = call.get('name', 'unknown')
                            tool_args = call.get('arguments', {})
                            result = decision.tool_results[i-1] if decision.tool_results and len(decision.tool_results) >= i else {}

                            # å·¥å…·è°ƒç”¨æ­¥éª¤
                            tool_process_content += f"### **æ­¥éª¤ {i}: æ‰§è¡Œ `{tool_name}`**\n\n"

                            # è°ƒç”¨å‚æ•°
                            tool_process_content += "**ğŸ“‹ è°ƒç”¨å‚æ•°:**\n"
                            if tool_args:
                                for key, value in tool_args.items():
                                    tool_process_content += f"- `{key}`: `{value}`\n"
                            else:
                                tool_process_content += "- æ— å‚æ•°\n"

                            tool_process_content += "\n**âš¡ æ‰§è¡Œç»“æœ:**\n"

                            if result:
                                success = result.get('success', False)
                                status_emoji = 'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'
                                tool_process_content += f"- **çŠ¶æ€**: {status_emoji}\n"

                                if success:
                                    data = result.get('data')
                                    if data:
                                        if isinstance(data, dict):
                                            if 'message' in data:
                                                tool_process_content += f"- **è¿”å›ä¿¡æ¯**: {data['message']}\n"
                                            if 'count' in data:
                                                tool_process_content += f"- **æ•°æ®é‡**: {data['count']} æ¡\n"
                                            if 'total' in data:
                                                tool_process_content += f"- **æ€»æ•°**: {data['total']}\n"
                                            if 'records' in data and isinstance(data['records'], list):
                                                tool_process_content += f"- **è®°å½•æ•°**: {len(data['records'])} æ¡\n"
                                                # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®ç¤ºä¾‹
                                                if len(data['records']) > 0:
                                                    sample_record = data['records'][0]
                                                    tool_process_content += f"- **æ•°æ®ç¤ºä¾‹**: {str(sample_record)[:100]}...\n"
                                        elif isinstance(data, list):
                                            tool_process_content += f"- **æ•°ç»„é•¿åº¦**: {len(data)} é¡¹\n"
                                            if len(data) > 0:
                                                tool_process_content += f"- **ç¤ºä¾‹æ•°æ®**: {str(data[0])[:100]}...\n"

                                if result.get('message'):
                                    tool_process_content += f"- **è¯´æ˜**: {result['message']}\n"
                            else:
                                tool_process_content += "- **çŠ¶æ€**: âš ï¸ æ— ç»“æœæ•°æ®\n"

                            tool_process_content += "\n---\n\n"

                        messages.append({
                            "role": "assistant",
                            "content": tool_process_content
                        })

                    # 5. æœ€ç»ˆæ€»ç»“
                    summary_content = f"""## ğŸ“‹ æ‰§è¡Œæ€»ç»“

**ğŸ¯ å†³ç­–ç»“æœ**: {decision.status}
**ğŸ”§ å·¥å…·è°ƒç”¨æ¬¡æ•°**: {len(decision.tool_calls) if decision.tool_calls else 0}
**ğŸ“¦ å…³è”è®¢å•**: {len(decision.order_ids) if decision.order_ids else 0}
**â° å®Œæˆæ—¶é—´**: {format_datetime_full_beijing(decision.decision_time)}

**ğŸ’° äº¤æ˜“æ‰§è¡Œæƒ…å†µ**:
{self._format_order_ids(decision.order_ids) if decision.order_ids else "æ— è®¢å•ç”Ÿæˆ"}

---
*æ­¤å†³ç­–ç”±AI AgentåŸºäºKronosé¢„æµ‹æ•°æ®è‡ªåŠ¨åˆ†æç”Ÿæˆ*
"""
                    messages.append({
                        "role": "assistant",
                        "content": summary_content
                    })

                    return messages
                else:
                    # æ²¡æœ‰å†³ç­–è®°å½•æ—¶çš„é»˜è®¤å“åº”
                    if output_parts:
                        return [{"role": "assistant", "content": output_parts[-1]}]
                    else:
                        return [{"role": "assistant", "content": "æš‚æ— AI Agentå†³ç­–è®°å½•ã€‚è¯·å…ˆè¿è¡Œæ¨ç†å¹¶è®©AI Agentè¿›è¡Œåˆ†æã€‚"}]

        except Exception as e:
            logger.error(f"è·å–æœ€æ–°Agentå†³ç­–è¾“å‡ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [{"role": "assistant", "content": f"ç­‰å¾…æ¨ç†...\n\nâŒ è·å–å¤±è´¥: {str(e)}"}]

    def get_latest_conversation_messages(self, plan_id: int) -> List[Dict]:
        """
        è·å–æœ€æ–°çš„å¯¹è¯æ¶ˆæ¯ï¼Œæ”¯æŒæ–°çš„æ¶ˆæ¯æ ¼å¼ï¼ˆthinkæ¨¡å¼ã€toolè°ƒç”¨ã€playç»“æœï¼‰

        Returns:
            List[Dict]: Chatbot messages æ ¼å¼ [{"role": "assistant", "content": ...}]
        """
        try:
            from database.models import AgentConversation, AgentMessage, TradingPlan
            from database.db import get_db

            with get_db() as db:
                # è·å–æœ€æ–°çš„å¯¹è¯ï¼ˆä»»ä½•ç±»å‹ï¼‰
                latest_conversation = db.query(AgentConversation).filter(
                    AgentConversation.plan_id == plan_id
                ).order_by(desc(AgentConversation.started_at)).first()

                if latest_conversation:
                    # è·å–è¯¥å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯
                    messages = db.query(AgentMessage).filter(
                        AgentMessage.conversation_id == latest_conversation.id
                    ).order_by(AgentMessage.timestamp.asc()).all()

                    if messages:
                        # ä½¿ç”¨æ–°çš„æ ¼å¼åŒ–æ–¹æ³•
                        return format_conversation_history(messages)

                # å¦‚æœæ²¡æœ‰ä»»ä½•å¯¹è¯ï¼Œè¿”å›æ¬¢è¿æ¶ˆæ¯å¹¶æ£€æŸ¥é…ç½®
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if plan and plan.llm_config_id:
                    welcome_msg = """ğŸ‘‹ æ¬¢è¿ä½¿ç”¨AI Agentå¯¹è¯ç³»ç»Ÿ

âœ… **é…ç½®å·²å®Œæˆ**
- LLMé…ç½®å·²å°±ç»ª
- Agentå·¥å…·å·²é…ç½®
- å‡†å¤‡å°±ç»ª

ğŸ’¡ **ä½¿ç”¨æ–¹å¼**
- ç‚¹å‡»ã€Œæ‰§è¡Œæ¨ç†ã€å¼€å§‹å¸‚åœºåˆ†æ
- åœ¨å¯¹è¯æ¡†ä¸­è¾“å…¥æ¶ˆæ¯è¿›è¡Œå¯¹è¯
- æ”¯æŒæ€è€ƒæ¨¡å¼ã€å·¥å…·è°ƒç”¨ã€æŠ•èµ„å†³ç­–å±•ç¤º

ğŸš€ å¼€å§‹æ‚¨çš„æ™ºèƒ½äº¤æ˜“ä¹‹æ—…å§ï¼"""
                else:
                    welcome_msg = """ğŸ‘‹ æ¬¢è¿ä½¿ç”¨AI Agentå¯¹è¯ç³»ç»Ÿ

âš ï¸ **éœ€è¦é…ç½®**
- è¯·å…ˆåœ¨ã€Œâš™ï¸ Agenté…ç½®ã€ä¸­è®¾ç½®LLMæä¾›å•†
- é…ç½®å¯ç”¨å·¥å…·
- è®¾ç½®äº¤æ˜“é™åˆ¶

ğŸ’¡ **é…ç½®å®Œæˆåå³å¯å¼€å§‹**:
- æ™ºèƒ½å¸‚åœºåˆ†æ
- è‡ªåŠ¨äº¤æ˜“å†³ç­–
- å¯¹è¯å¼äº¤äº’"""

                return [{"role": "assistant", "content": welcome_msg}]

        except Exception as e:
            logger.error(f"è·å–æœ€æ–°å¯¹è¯æ¶ˆæ¯å¤±è´¥: {e}")
            return [{"role": "assistant", "content": f"ç­‰å¾…å¯¹è¯...\n\nâŒ è·å–å¤±è´¥: {str(e)}"}]

    def get_conversation_tool_calls_summary(self, plan_id: int) -> List[Dict]:
        """
        è·å–å¯¹è¯çš„å·¥å…·è°ƒç”¨æ‘˜è¦

        Returns:
            List[Dict]: å·¥å…·è°ƒç”¨åˆ—è¡¨
        """
        try:
            latest_conversation = ConversationService.get_latest_conversation(
                plan_id=plan_id,
                conversation_type="auto_inference"
            )

            if latest_conversation:
                return ConversationService.get_tool_calls_summary(latest_conversation.id)

            return []

        except Exception as e:
            logger.error(f"è·å–å·¥å…·è°ƒç”¨æ‘˜è¦å¤±è´¥: {e}")
            return []

    def get_plan_conversations_list(self, plan_id: int, limit: int = 10) -> List[Dict]:
        """
        è·å–è®¡åˆ’çš„å¯¹è¯ä¼šè¯åˆ—è¡¨

        Returns:
            List[Dict]: å¯¹è¯ä¼šè¯åˆ—è¡¨
        """
        try:
            conversations = ConversationService.get_plan_conversations(plan_id, limit)

            conversation_list = []
            for conv in conversations:
                conversation_list.append({
                    'id': conv.id,
                    'session_name': conv.session_name,
                    'conversation_type': conv.conversation_type,
                    'status': conv.status,
                    'total_messages': conv.total_messages,
                    'total_tool_calls': conv.total_tool_calls,
                    'started_at': format_datetime_full_beijing(conv.started_at),
                    'last_message_at': format_datetime_full_beijing(conv.last_message_at) if conv.last_message_at else 'N/A',
                    'completed_at': format_datetime_full_beijing(conv.completed_at) if conv.completed_at else 'è¿›è¡Œä¸­'
                })

            return conversation_list

        except Exception as e:
            logger.error(f"è·å–å¯¹è¯ä¼šè¯åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def enhanced_inference_stream(self, plan_id: int, training_record_id: int = None, progress=None):
        """
        å¢å¼ºç‰ˆæ¨ç†æµå¼è¾“å‡ºï¼Œæ”¯æŒReactå¾ªç¯å±•ç¤ºå’Œå·¥å…·è°ƒç”¨è®°å½•

        Args:
            plan_id: è®¡åˆ’ID
            training_record_id: è®­ç»ƒè®°å½•ID
            progress: Gradioè¿›åº¦æ¡

        Yields:
            Dict: åŒ…å«å¯¹è¯çŠ¶æ€ã€æ¶ˆæ¯ç­‰çš„æµå¼æ•°æ®
        """
        try:
            from services.agent_decision_service import AgentDecisionService

            async for chunk in AgentDecisionService.enhanced_react_tool_use_stream(
                plan_id=plan_id,
                training_id=training_record_id,
                progress=progress,
                session_name=f"æ¨ç†ä¼šè¯_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            ):
                yield chunk

        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆæ¨ç†æµå¤±è´¥: {e}")
            yield {
                'type': 'error',
                'content': f'âŒ æ¨ç†å¤±è´¥: {str(e)}',
                'messages': [{"role": "assistant", "content": f"âŒ æ¨ç†å¤±è´¥: {str(e)}"}]
            }

    def _format_prediction_preview(self, predictions: list, batch_id: str, version: str) -> str:
        """
        æ ¼å¼åŒ–é¢„æµ‹æ•°æ®é¢„è§ˆ

        Args:
            predictions: é¢„æµ‹æ•°æ®åˆ—è¡¨
            batch_id: æ¨ç†æ‰¹æ¬¡ID
            version: è®­ç»ƒç‰ˆæœ¬

        Returns:
            æ ¼å¼åŒ–åçš„é¢„æµ‹æ•°æ®é¢„è§ˆå­—ç¬¦ä¸²
        """
        try:
            if not predictions or len(predictions) == 0:
                return "## ğŸ“Š æœ€æ–°é¢„æµ‹æ•°æ®\n\næš‚æ— é¢„æµ‹æ•°æ®"

            # è®¡ç®—è¶‹åŠ¿
            first_pred = predictions[0]
            last_pred = predictions[-1]
            first_close = first_pred['close']
            last_close = last_pred['close']
            change_pct = ((last_close - first_close) / first_close) * 100

            # è·å–æ¦‚ç‡æŒ‡æ ‡ï¼ˆä»æ•°æ®åº“æŸ¥è¯¢ï¼‰
            upward_prob = None
            volatility_amp_prob = None
            sample_count = 1

            try:
                with get_db() as db:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªé¢„æµ‹æ•°æ®è·å–ç›¸å…³ä¿¡æ¯
                    pred_record = db.query(PredictionData).filter(
                        PredictionData.inference_batch_id == batch_id
                    ).first()

                    if pred_record:
                        upward_prob = pred_record.upward_probability
                        volatility_amp_prob = pred_record.volatility_amplification_probability
                        if pred_record.inference_params:
                            sample_count = pred_record.inference_params.get('sample_count', 1)
            except Exception as e:
                logger.error(f"è·å–æ¦‚ç‡æŒ‡æ ‡å¤±è´¥: {e}")

            # æ—¶é—´èŒƒå›´
            first_time = format_datetime_beijing(first_pred['timestamp'], '%Y-%m-%d %H:%M') if hasattr(first_pred['timestamp'], 'strftime') else str(first_pred['timestamp'])[:16]
            last_time = format_datetime_beijing(last_pred['timestamp'], '%Y-%m-%d %H:%M') if hasattr(last_pred['timestamp'], 'strftime') else str(last_pred['timestamp'])[:16]

            # ä»·æ ¼ç»Ÿè®¡
            close_prices = [p['close'] for p in predictions]
            min_close = min(close_prices)
            max_close = max(close_prices)

            # è¶‹åŠ¿åˆ¤æ–­
            trend = "ğŸ“ˆ ä¸Šæ¶¨è¶‹åŠ¿" if change_pct > 0 else "ğŸ“‰ ä¸‹è·Œè¶‹åŠ¿" if change_pct < 0 else "â¡ï¸ æ¨ªç›˜"
            trend_emoji = "ğŸ“ˆ" if change_pct > 0 else "ğŸ“‰" if change_pct < 0 else "â¡ï¸"

            # æ„å»ºè¾“å‡º
            output = f"""## ğŸ“Š æœ€æ–°é¢„æµ‹æ•°æ®é¢„è§ˆ

**æ‰¹æ¬¡ID**: {batch_id} | **è®­ç»ƒç‰ˆæœ¬**: {version}
**é¢„æµ‹å‘¨æœŸæ•°**: {len(predictions)} | **æ—¶é—´èŒƒå›´**: {first_time} ~ {last_time}

---

### ğŸ“ˆ ä»·æ ¼é¢„æµ‹

**å½“å‰ä»·æ ¼**: ${first_close:.4f}
**é¢„æµ‹ä»·æ ¼**: ${last_close:.4f}
**ä»·æ ¼åŒºé—´**: ${min_close:.4f} ~ ${max_close:.4f}
**é¢„æµ‹æ¶¨è·Œ**: {trend_emoji} {change_pct:+.2f}%
**è¶‹åŠ¿åˆ¤æ–­**: {trend}"""

            # æ·»åŠ æ¦‚ç‡æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if upward_prob is not None and volatility_amp_prob is not None:
                upward_percent = upward_prob * 100
                volatility_percent = volatility_amp_prob * 100

                # æ ¹æ®æ¦‚ç‡å€¼é€‰æ‹©è¡¨æƒ…å’Œé¢œè‰²
                if upward_percent >= 60:
                    upward_emoji = "ğŸ“ˆ"
                    upward_color = "green"
                elif upward_percent >= 40:
                    upward_emoji = "â¡ï¸"
                    upward_color = "orange"
                else:
                    upward_emoji = "ğŸ“‰"
                    upward_color = "red"

                if volatility_percent >= 60:
                    volatility_emoji = "âš¡"
                    volatility_color = "red"
                elif volatility_percent >= 40:
                    volatility_emoji = "ã€°ï¸"
                    volatility_color = "orange"
                else:
                    volatility_emoji = "ğŸ˜´"
                    volatility_color = "green"

                output += f"""

---

### ğŸ¯ æ¦‚ç‡æŒ‡æ ‡

<div style="display: flex; gap: 20px; margin: 10px 0;">
  <div style="flex: 1; padding: 15px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 10px; color: white;">
    <div style="font-size: 14px; opacity: 0.9;">ä¸Šæ¶¨æ¦‚ç‡ï¼ˆæœªæ¥é¢„æµ‹æœŸï¼‰</div>
    <div style="font-size: 36px; font-weight: bold; margin: 10px 0;">{upward_emoji} {upward_percent:.1f}%</div>
    <div style="font-size: 12px; opacity: 0.8;">æ¨¡å‹å¯¹ä»·æ ¼ä¸Šæ¶¨çš„ç½®ä¿¡åº¦</div>
  </div>
  <div style="flex: 1; padding: 15px; background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); border-radius: 10px; color: white;">
    <div style="font-size: 14px; opacity: 0.9;">æ³¢åŠ¨æ€§æ”¾å¤§</div>
    <div style="font-size: 36px; font-weight: bold; margin: 10px 0;">{volatility_emoji} {volatility_percent:.1f}%</div>
    <div style="font-size: 12px; opacity: 0.8;">æœªæ¥æ³¢åŠ¨ç‡è¶…è¿‡å†å²çš„æ¦‚ç‡</div>
  </div>
</div>

**æ•°æ®æ¥æº**: åŸºäº {sample_count} æ¡è’™ç‰¹å¡ç½—è·¯å¾„"""

            # æ·»åŠ è¯¦ç»†é¢„æµ‹æ•°æ®é¢„è§ˆ
            output += f"""

---

### ğŸ“‹ é¢„æµ‹æ•°æ®è¯¦æƒ… (å‰10æ¡)

| åºå· | æ—¶é—´ | å¼€ç›˜ | æœ€é«˜ | æœ€ä½ | æ”¶ç›˜ |
|------|------|------|------|------|------|"""

            # æ˜¾ç¤ºå‰10æ¡è¯¦ç»†æ•°æ®
            for i, pred in enumerate(predictions[:10], 1):
                timestamp_str = format_datetime_short_beijing(pred['timestamp']) if hasattr(pred['timestamp'], 'strftime') else str(pred['timestamp'])[:16]
                output += f"\n| {i} | {timestamp_str} | ${pred['open']:.2f} | ${pred['high']:.2f} | ${pred['low']:.2f} | ${pred['close']:.2f} |"

            if len(predictions) > 10:
                output += f"\n... (å…±{len(predictions)}æ¡ï¼Œä»…æ˜¾ç¤ºå‰10æ¡)"

            output += f"""

---

**ğŸ’¡ æç¤º**: æ­¤é¢„æµ‹æ•°æ®å¯ç”¨äº AI Agent åˆ†æå’Œå†³ç­–ã€‚ç‚¹å‡»ã€Œæ‰‹åŠ¨æ¨ç†ã€æŒ‰é’®å¯ä»¥è®© AI Agent åŸºäºè¿™äº›æ•°æ®è¿›è¡Œäº¤æ˜“åˆ†æã€‚"""

            return output

        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–é¢„æµ‹æ•°æ®é¢„è§ˆå¤±è´¥: {e}")
            return f"## ğŸ“Š æœ€æ–°é¢„æµ‹æ•°æ®\n\nâŒ æ ¼å¼åŒ–å¤±è´¥: {str(e)}"

    def get_finetune_params(self, plan_id: int) -> dict:
        """è·å–å¾®è°ƒå‚æ•°"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return {}
                return plan.finetune_params or {}
        except Exception as e:
            logger.error(f"è·å–å¾®è°ƒå‚æ•°å¤±è´¥: {e}")
            return {}

    def save_finetune_params(self, plan_id: int, params: dict) -> str:
        """ä¿å­˜å¾®è°ƒå‚æ•°"""
        try:
            with get_db() as db:
                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'finetune_params': params
                })
                db.commit()
                logger.info(f"å¾®è°ƒå‚æ•°å·²ä¿å­˜: plan_id={plan_id}")
                return "âœ… å‚æ•°å·²ä¿å­˜"
        except Exception as e:
            logger.error(f"ä¿å­˜å¾®è°ƒå‚æ•°å¤±è´¥: {e}")
            return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"

    def get_llm_configs(self) -> list:
        """è·å–æ‰€æœ‰LLMé…ç½®"""
        try:
            with get_db() as db:
                from database.models import LLMConfig
                configs = db.query(LLMConfig).filter(LLMConfig.is_active == True).all()
                return [(f"{c.provider} - {c.model_name}", c.id) for c in configs]
        except Exception as e:
            logger.error(f"è·å–LLMé…ç½®å¤±è´¥: {e}")
            return []

    def get_prompt_templates(self) -> list:
        """è·å–æ‰€æœ‰æç¤ºè¯æ¨¡ç‰ˆ"""
        try:
            with get_db() as db:
                from database.models import AgentPromptTemplate
                templates = db.query(AgentPromptTemplate).filter(
                    AgentPromptTemplate.is_active == True
                ).all()
                return [(f"{t.name} ({t.category})", t.id) for t in templates]
        except Exception as e:
            logger.error(f"è·å–æç¤ºè¯æ¨¡ç‰ˆå¤±è´¥: {e}")
            return []

    def get_inference_params(self, plan_id: int) -> dict:
        """è·å–æ¨ç†å‚æ•°"""
        try:
            # é¦–å…ˆç¡®ä¿å‚æ•°ç»“æ„æ­£ç¡®
            self.ensure_finetune_params_structure(plan_id)

            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return {}

                finetune_params = plan.finetune_params or {}
                inference_config = finetune_params.get('inference', {})
                data_config = finetune_params.get('data', {})

                # ç¡®ä¿ç±»å‹è½¬æ¢ï¼šä»æ•°æ®åº“JSONBè¯»å–çš„æ•°å€¼å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è½¬æ¢ä¸ºæ•°å­—
                return {
                    'lookback_window': int(data_config.get('lookback_window', 512)),
                    'predict_window': int(data_config.get('predict_window', 48)),
                    'temperature': float(inference_config.get('temperature', 1.0)),
                    'top_p': float(inference_config.get('top_p', 0.9)),
                    'sample_count': int(inference_config.get('sample_count', 30)),
                    'data_offset': int(inference_config.get('data_offset', 0))
                }
        except Exception as e:
            logger.error(f"è·å–æ¨ç†å‚æ•°å¤±è´¥: {e}")
            return {
                'lookback_window': 512,
                'predict_window': 48,
                'temperature': 1.0,
                'top_p': 0.9,
                'sample_count': 30,
                'data_offset': 0
            }

    def ensure_finetune_params_structure(self, plan_id: int) -> bool:
        """
        ç¡®ä¿finetune_paramsç»“æ„æ­£ç¡®ï¼Œå¦‚æœä¸æ­£ç¡®åˆ™ä¿®å¤

        Args:
            plan_id: è®¡åˆ’ID

        Returns:
            æ˜¯å¦è¿›è¡Œäº†ä¿®å¤
        """
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return False

                finetune_params = plan.finetune_params or {}
                needs_fix = False

                # ç¡®ä¿åµŒå¥—ç»“æ„å­˜åœ¨
                if 'data' not in finetune_params:
                    finetune_params['data'] = {}
                    needs_fix = True

                if 'inference' not in finetune_params:
                    finetune_params['inference'] = {}
                    needs_fix = True

                # å¤„ç†æ‰å¹³ç»“æ„å‚æ•°ï¼ˆå…¼å®¹æ€§ï¼‰
                # å¦‚æœå‚æ•°åœ¨é¡¶å±‚ï¼Œç§»åŠ¨åˆ°å¯¹åº”çš„åµŒå¥—ç»“æ„ä¸­
                if 'lookback_window' in finetune_params and 'lookback_window' not in finetune_params['data']:
                    finetune_params['data']['lookback_window'] = finetune_params['lookback_window']
                    del finetune_params['lookback_window']  # ç§»é™¤é¡¶å±‚å‚æ•°
                    needs_fix = True

                if 'predict_window' in finetune_params and 'predict_window' not in finetune_params['data']:
                    finetune_params['data']['predict_window'] = finetune_params['predict_window']
                    del finetune_params['predict_window']  # ç§»é™¤é¡¶å±‚å‚æ•°
                    needs_fix = True

                # è®¾ç½®é»˜è®¤æ¨ç†å‚æ•°ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                inference_defaults = {
                    'temperature': 1.0,
                    'top_p': 0.9,
                    'sample_count': 30,
                    'data_offset': 0
                }

                for key, default_value in inference_defaults.items():
                    if key not in finetune_params['inference']:
                        finetune_params['inference'][key] = default_value
                        needs_fix = True

                # å¦‚æœéœ€è¦ä¿®å¤ï¼Œæ›´æ–°æ•°æ®åº“
                if needs_fix:
                    db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                        'finetune_params': finetune_params
                    })
                    db.commit()
                    logger.info(f"å·²ä¿®å¤è®¡åˆ’{plan_id}çš„å‚æ•°ç»“æ„")

                return needs_fix

        except Exception as e:
            logger.error(f"ç¡®ä¿å‚æ•°ç»“æ„å¤±è´¥: plan_id={plan_id}, error={e}")
            return False

    def save_inference_params(
        self,
        plan_id: int,
        lookback_window: int,
        predict_window: int,
        temperature: float,
        top_p: float,
        sample_count: int,
        data_offset: int = 0
    ) -> str:
        """ä¿å­˜æ¨ç†å‚æ•°"""
        try:
            # éªŒè¯å‚æ•°èŒƒå›´
            if not (64 <= lookback_window <= 2048):
                return "âŒ Lookback Window å¿…é¡»åœ¨ 64 åˆ° 2048 ä¹‹é—´"

            if not (1 <= predict_window <= 512):
                return "âŒ Predict Window å¿…é¡»åœ¨ 1 åˆ° 512 ä¹‹é—´"

            if not (0.0 <= temperature <= 2.0):
                return "âŒ Temperature å¿…é¡»åœ¨ 0.0 åˆ° 2.0 ä¹‹é—´"

            if not (0.0 <= top_p <= 1.0):
                return "âŒ Top-p å¿…é¡»åœ¨ 0.0 åˆ° 1.0 ä¹‹é—´"

            if not (1 <= sample_count <= 100):
                return "âŒ Sample Count å¿…é¡»åœ¨ 1 åˆ° 100 ä¹‹é—´"

            if not (0 <= data_offset <= 1000):
                return "âŒ æ•°æ®åç§»å¿…é¡»åœ¨ 0 åˆ° 1000 ä¹‹é—´"

            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨"

                # è·å–ç°æœ‰çš„ finetune_params
                finetune_params = plan.finetune_params or {}
                if 'inference' not in finetune_params:
                    finetune_params['inference'] = {}
                if 'data' not in finetune_params:
                    finetune_params['data'] = {}

                # æ›´æ–°æ•°æ®çª—å£å‚æ•°ï¼ˆä¿å­˜åˆ° data é…ç½®ä¸­ï¼‰
                finetune_params['data']['lookback_window'] = int(lookback_window)
                finetune_params['data']['predict_window'] = int(predict_window)

                # æ›´æ–°æ¨ç†å‚æ•°
                finetune_params['inference']['temperature'] = float(temperature)
                finetune_params['inference']['top_p'] = float(top_p)
                finetune_params['inference']['sample_count'] = int(sample_count)
                finetune_params['inference']['data_offset'] = int(data_offset)

                # ä¿å­˜åˆ°æ•°æ®åº“
                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'finetune_params': finetune_params
                })
                db.commit()

                logger.info(
                    f"æ¨ç†å‚æ•°å·²ä¿å­˜: plan_id={plan_id}, "
                    f"lookback={lookback_window}, predict={predict_window}, "
                    f"temperature={temperature}, top_p={top_p}, sample_count={sample_count}, data_offset={data_offset}"
                )
                return "âœ… æ¨ç†å‚æ•°å·²ä¿å­˜"
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨ç†å‚æ•°å¤±è´¥: {e}")
            return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"

    def get_agent_config(self, plan_id: int) -> dict:
        """è·å–Agenté…ç½®"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return {}
                return {
                    'llm_config_id': plan.llm_config_id,
                    'agent_prompt': plan.agent_prompt or '',
                    'agent_tools_config': plan.agent_tools_config or {}
                }
        except Exception as e:
            logger.error(f"è·å–Agenté…ç½®å¤±è´¥: {e}")
            return {}

    def save_agent_config(self, plan_id: int, llm_config_id: int,
                         agent_prompt: str, tools_config: dict) -> str:
        """ä¿å­˜Agenté…ç½®"""
        try:
            with get_db() as db:
                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'llm_config_id': llm_config_id,
                    'agent_prompt': agent_prompt,
                    'agent_tools_config': tools_config
                })
                db.commit()
                logger.info(f"Agenté…ç½®å·²ä¿å­˜: plan_id={plan_id}")
                return "âœ… Agenté…ç½®å·²ä¿å­˜"
        except Exception as e:
            logger.error(f"ä¿å­˜Agenté…ç½®å¤±è´¥: {e}")
            return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"

    
    def get_trading_limits_config(self, plan_id: int) -> dict:
        """è·å–äº¤æ˜“é™åˆ¶é…ç½®"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return {
                        'available_usdt_amount': 1000.0,
                        'available_usdt_percentage': 30.0,
                        'avg_order_count': 10,
                        'stop_loss_percentage': 20.0
                    }

                trading_limits = plan.trading_limits or {}
                # ç¡®ä¿ç±»å‹è½¬æ¢ï¼šä»æ•°æ®åº“JSONBè¯»å–çš„æ•°å€¼å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è½¬æ¢ä¸ºæ•°å­—
                return {
                    'available_usdt_amount': float(trading_limits.get('available_usdt_amount', 1000.0)),
                    'available_usdt_percentage': float(trading_limits.get('available_usdt_percentage', 30.0)),
                    'avg_order_count': int(trading_limits.get('avg_order_count', 10)),
                    'stop_loss_percentage': float(trading_limits.get('stop_loss_percentage', 20.0))
                }
        except Exception as e:
            logger.error(f"è·å–äº¤æ˜“é™åˆ¶é…ç½®å¤±è´¥: {e}")
            return {
                'available_usdt_amount': 1000.0,
                'available_usdt_percentage': 30.0,
                'avg_order_count': 10,
                'stop_loss_percentage': 20.0
            }

    def save_trading_limits_config(
        self,
        plan_id: int,
        available_usdt_amount: float,
        available_usdt_percentage: float,
        avg_order_count: int,
        stop_loss_percentage: float
    ) -> str:
        """ä¿å­˜äº¤æ˜“é™åˆ¶é…ç½®"""
        try:
            # éªŒè¯å‚æ•°
            if available_usdt_amount < 0:
                return "âŒ å¯ç”¨USDTé‡‘é¢ä¸èƒ½ä¸ºè´Ÿæ•°"
            if not (0 <= available_usdt_percentage <= 100):
                return "âŒ å¯ç”¨èµ„é‡‘æ¯”ä¾‹å¿…é¡»åœ¨ 0% åˆ° 100% ä¹‹é—´"
            if avg_order_count < 1:
                return "âŒ å¹³æ‘Šå•é‡å¿…é¡»å¤§äº0"
            if not (0.1 <= stop_loss_percentage <= 100):
                return "âŒ æ­¢æŸæ¯”ä¾‹å¿…é¡»åœ¨ 0.1% åˆ° 100% ä¹‹é—´"

            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨"

                # è·å–ç°æœ‰çš„trading_limitsé…ç½®
                trading_limits = plan.trading_limits or {}

                # æ›´æ–°äº¤æ˜“é™åˆ¶é…ç½®
                trading_limits.update({
                    'available_usdt_amount': float(available_usdt_amount),
                    'available_usdt_percentage': float(available_usdt_percentage),
                    'avg_order_count': int(avg_order_count),
                    'stop_loss_percentage': float(stop_loss_percentage),
                    # ä¿ç•™åŸæœ‰çš„å…¼å®¹å­—æ®µ
                    'max_position_size': trading_limits.get('max_position_size', 1.0),
                    'max_order_amount': float(available_usdt_amount)
                })

                # ä¿å­˜åˆ°æ•°æ®åº“
                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'trading_limits': trading_limits
                })
                db.commit()

                logger.info(
                    f"äº¤æ˜“é™åˆ¶é…ç½®å·²ä¿å­˜: plan_id={plan_id}, "
                    f"available_usdt={available_usdt_amount}, percentage={available_usdt_percentage}%, "
                    f"avg_orders={avg_order_count}, stop_loss={stop_loss_percentage}%"
                )
                return "âœ… äº¤æ˜“é™åˆ¶é…ç½®å·²ä¿å­˜"

        except Exception as e:
            logger.error(f"ä¿å­˜äº¤æ˜“é™åˆ¶é…ç½®å¤±è´¥: {e}")
            return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"

    def load_prompt_template(self, template_id: int) -> str:
        """åŠ è½½æç¤ºè¯æ¨¡ç‰ˆå†…å®¹"""
        try:
            with get_db() as db:
                from database.models import AgentPromptTemplate
                template = db.query(AgentPromptTemplate).filter(
                    AgentPromptTemplate.id == template_id
                ).first()
                if template:
                    return template.content
                return ""
        except Exception as e:
            logger.error(f"åŠ è½½æç¤ºè¯æ¨¡ç‰ˆå¤±è´¥: {e}")
            return ""

    def get_automation_config(self, plan_id: int) -> dict:
        """è·å–è‡ªåŠ¨åŒ–é…ç½®ï¼ˆå››ä¸ªå¼€å…³å’Œæ—¶é—´è¡¨ï¼‰"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return {
                        'auto_finetune_enabled': False,
                        'auto_inference_enabled': False,
                        'auto_agent_enabled': False,
                        'auto_tool_execution_enabled': False,
                        'schedule': []
                    }
                return {
                    'auto_finetune_enabled': plan.auto_finetune_enabled or False,
                    'auto_inference_enabled': plan.auto_inference_enabled or False,
                    'auto_agent_enabled': plan.auto_agent_enabled or False,
                    'auto_tool_execution_enabled': plan.auto_tool_execution_enabled or False,
                    'schedule': plan.auto_finetune_schedule or []
                }
        except Exception as e:
            logger.error(f"è·å–è‡ªåŠ¨åŒ–é…ç½®å¤±è´¥: {e}")
            return {
                'auto_finetune_enabled': False,
                'auto_inference_enabled': False,
                'auto_agent_enabled': False,
                'auto_tool_execution_enabled': False,
                'schedule': []
            }

    def save_automation_config(self, plan_id: int, auto_finetune: bool, auto_inference: bool,
                               auto_agent: bool, auto_tool_execution: bool, schedule_times: str) -> str:
        """ä¿å­˜è‡ªåŠ¨åŒ–é…ç½®"""
        try:
            # è§£ææ—¶é—´è¡¨
            schedule_list = []
            if schedule_times and schedule_times.strip():
                time_parts = [t.strip() for t in schedule_times.split(',')]
                for time_str in time_parts:
                    # éªŒè¯æ—¶é—´æ ¼å¼ HH:MM
                    if len(time_str) == 5 and time_str.count(':') == 1:
                        try:
                            hour, minute = time_str.split(':')
                            if 0 <= int(hour) <= 23 and 0 <= int(minute) <= 59:
                                schedule_list.append(time_str)
                        except ValueError:
                            continue

            with get_db() as db:
                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'auto_finetune_enabled': auto_finetune,
                    'auto_inference_enabled': auto_inference,
                    'auto_agent_enabled': auto_agent,
                    'auto_tool_execution_enabled': auto_tool_execution,
                    'auto_finetune_schedule': schedule_list
                })
                db.commit()
                logger.info(
                    f"è‡ªåŠ¨åŒ–é…ç½®å·²ä¿å­˜: plan_id={plan_id}, "
                    f"finetune={auto_finetune}, inference={auto_inference}, "
                    f"agent={auto_agent}, tool_exec={auto_tool_execution}, "
                    f"schedule={schedule_list}"
                )
                return f"âœ… è‡ªåŠ¨åŒ–é…ç½®å·²ä¿å­˜\nğŸ“… è®­ç»ƒæ—¶é—´è¡¨: {', '.join(schedule_list) if schedule_list else 'æœªé…ç½®'}"
        except Exception as e:
            logger.error(f"ä¿å­˜è‡ªåŠ¨åŒ–é…ç½®å¤±è´¥: {e}")
            return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"

    def get_automation_status_display(self, plan_id: int) -> str:
        """è·å–è‡ªåŠ¨åŒ–çŠ¶æ€æ˜¾ç¤º"""
        try:
            from services.automation_service import automation_service
            status = automation_service.get_automation_status(plan_id)

            if not status:
                return "### ğŸ“Š è‡ªåŠ¨åŒ–çŠ¶æ€\n\nâŒ æ— æ³•è·å–çŠ¶æ€ä¿¡æ¯"

            # æ„å»ºçŠ¶æ€æ˜¾ç¤º
            lines = ["### ğŸ“Š è‡ªåŠ¨åŒ–çŠ¶æ€"]
            lines.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

            # è‡ªåŠ¨åŒ–é…ç½®çŠ¶æ€
            lines.append("#### ğŸ¯ è‡ªåŠ¨åŒ–é…ç½®")
            finetune_status = "âœ…" if status.get('auto_finetune_enabled') else "âŒ"
            inference_status = "âœ…" if status.get('auto_inference_enabled') else "âŒ"
            agent_status = "âœ…" if status.get('auto_agent_enabled') else "âŒ"
            tool_status = "ğŸš«"  # å·¥å…·ç¡®è®¤åŠŸèƒ½å·²åºŸå¼ƒ

            lines.append(f"- ğŸ§  è‡ªåŠ¨å¾®è°ƒè®­ç»ƒ: {finetune_status}")
            lines.append(f"- ğŸ”® è‡ªåŠ¨é¢„æµ‹æ¨ç†: {inference_status}")
            lines.append(f"- ğŸ¤– è‡ªåŠ¨Agentå†³ç­–: {agent_status}")
            lines.append(f"- âš¡ è‡ªåŠ¨å·¥å…·æ‰§è¡Œ: {tool_status} (å·²åºŸå¼ƒ - AI Agentç›´æ¥è°ƒç”¨å·¥å…·)")

            # è°ƒåº¦å™¨çŠ¶æ€
            lines.append("")
            lines.append("#### ğŸ”„ è°ƒåº¦å™¨çŠ¶æ€")
            scheduler_emoji = "âœ…" if status.get('scheduler_running') else "âŒ"
            lines.append(f"- è°ƒåº¦å™¨è¿è¡ŒçŠ¶æ€: {scheduler_emoji}")

            if status.get('last_check_time'):
                last_check = format_datetime_full_beijing(status['last_check_time'])
                lines.append(f"- æœ€åæ£€æŸ¥æ—¶é—´: {last_check}")

            # å½“å‰ä»»åŠ¡çŠ¶æ€
            current_task = status.get('current_task', {})
            if current_task:
                lines.append("")
                lines.append("#### âš¡ å½“å‰ä»»åŠ¡")
                task_stage = current_task.get('stage', 'unknown')
                task_start = current_task.get('start_time')
                if task_start:
                    start_str = format_datetime_beijing(task_start, "%H:%M:%S")
                    lines.append(f"- ä»»åŠ¡é˜¶æ®µ: {task_stage}")
                    lines.append(f"- å¼€å§‹æ—¶é—´: {start_str}")
                    lines.append(f"- ä»»åŠ¡ID: {current_task.get('task_id', 'N/A')}")
            else:
                lines.append("")
                lines.append("#### âš¡ å½“å‰ä»»åŠ¡: æ— æ´»è·ƒä»»åŠ¡")

            # æœ€æ–°è®­ç»ƒè®°å½•
            latest_training = status.get('latest_auto_training')
            if latest_training:
                lines.append("")
                lines.append("#### ğŸ“ˆ æœ€æ–°è‡ªåŠ¨è®­ç»ƒ")
                lines.append(f"- è®­ç»ƒID: {latest_training['id']}")
                lines.append(f"- çŠ¶æ€: {latest_training['status']}")
                if latest_training['created_at']:
                    training_time = format_datetime_beijing(latest_training['created_at'], "%Y-%m-%d %H:%M")
                    lines.append(f"- è®­ç»ƒæ—¶é—´: {training_time}")

            # æ—¶é—´è¡¨é…ç½®
            schedule = status.get('auto_finetune_schedule', [])
            if schedule:
                lines.append("")
                lines.append("#### â° è®­ç»ƒæ—¶é—´è¡¨")
                for time_point in schedule:
                    lines.append(f"- {time_point}")

            return "\n".join(lines)

        except Exception as e:
            logger.error(f"è·å–è‡ªåŠ¨åŒ–çŠ¶æ€å¤±è´¥: {e}")
            return f"### ğŸ“Š è‡ªåŠ¨åŒ–çŠ¶æ€\n\nâŒ è·å–å¤±è´¥: {str(e)}"

    def get_scheduler_status_display(self) -> str:
        """è·å–è°ƒåº¦å™¨çŠ¶æ€æ˜¾ç¤º"""
        try:
            from services.automation_service import automation_service
            running = automation_service.scheduler_running
            last_check = automation_service.last_check_time

            if running:
                status_emoji = "âœ… è¿è¡Œä¸­"
                status_text = "è°ƒåº¦å™¨æ­£åœ¨è¿è¡Œï¼Œä¼šæ¯åˆ†é’Ÿæ£€æŸ¥è‡ªåŠ¨åŒ–ä»»åŠ¡"
            else:
                status_emoji = "âŒ å·²åœæ­¢"
                status_text = "è°ƒåº¦å™¨å·²åœæ­¢ï¼Œä¸ä¼šè‡ªåŠ¨æ‰§è¡Œä»»åŠ¡"

            lines = [f"ğŸ”„ è°ƒåº¦å™¨çŠ¶æ€: {status_emoji}"]
            lines.append(status_text)

            if last_check:
                last_check_str = format_datetime_full_beijing(last_check)
                lines.append(f"æœ€åæ£€æŸ¥: {last_check_str}")

            return "\n".join(lines)

        except Exception as e:
            logger.error(f"è·å–è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {e}")
            return "ğŸ”„ è°ƒåº¦å™¨çŠ¶æ€: âŒ è·å–å¤±è´¥"

    def start_automation_scheduler(self) -> str:
        """å¯åŠ¨è‡ªåŠ¨åŒ–è°ƒåº¦å™¨"""
        try:
            from services.automation_service import automation_service
            automation_service.start_scheduler()
            return "âœ… è‡ªåŠ¨åŒ–è°ƒåº¦å™¨å·²å¯åŠ¨"
        except Exception as e:
            logger.error(f"å¯åŠ¨è‡ªåŠ¨åŒ–è°ƒåº¦å™¨å¤±è´¥: {e}")
            return f"âŒ å¯åŠ¨å¤±è´¥: {str(e)}"

    def stop_automation_scheduler(self) -> str:
        """åœæ­¢è‡ªåŠ¨åŒ–è°ƒåº¦å™¨"""
        try:
            from services.automation_service import automation_service
            automation_service.stop_scheduler()
            return "âœ… è‡ªåŠ¨åŒ–è°ƒåº¦å™¨å·²åœæ­¢"
        except Exception as e:
            logger.error(f"åœæ­¢è‡ªåŠ¨åŒ–è°ƒåº¦å™¨å¤±è´¥: {e}")
            return f"âŒ åœæ­¢å¤±è´¥: {str(e)}"

    # å·¥å…·ç¡®è®¤ç›¸å…³æ–¹æ³•å·²ç§»é™¤ - AI Agentç°åœ¨ç›´æ¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·ï¼Œæ— éœ€ç¡®è®¤

    def get_finetune_schedule(self, plan_id: int) -> list:
        """è·å–è‡ªåŠ¨å¾®è°ƒæ—¶é—´è¡¨"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return []
                return plan.auto_finetune_schedule or []
        except Exception as e:
            logger.error(f"è·å–è‡ªåŠ¨å¾®è°ƒæ—¶é—´è¡¨å¤±è´¥: {e}")
            return []

    def add_finetune_schedule_time(self, plan_id: int, time_str: str) -> tuple:
        """
        æ·»åŠ è‡ªåŠ¨å¾®è°ƒæ—¶é—´ç‚¹

        Args:
            plan_id: è®¡åˆ’ID
            time_str: æ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼ HH:MM

        Returns:
            (ç»“æœæ¶ˆæ¯, æ›´æ–°åçš„æ—¶é—´è¡¨åˆ—è¡¨)
        """
        try:
            # éªŒè¯æ—¶é—´æ ¼å¼
            import re
            if not re.match(r'^\d{2}:\d{2}$', time_str):
                return "âŒ æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ HH:MM æ ¼å¼", []

            hour, minute = map(int, time_str.split(':'))
            if not (0 <= hour <= 23 and 0 <= minute <= 59):
                return "âŒ æ—¶é—´èŒƒå›´é”™è¯¯ï¼Œå°æ—¶åº”ä¸º 00-23ï¼Œåˆ†é’Ÿåº”ä¸º 00-59", []

            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨", []

                schedule = plan.auto_finetune_schedule or []

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if time_str in schedule:
                    return f"âš ï¸ æ—¶é—´ç‚¹ {time_str} å·²å­˜åœ¨", schedule

                # æ·»åŠ å¹¶æ’åº
                schedule.append(time_str)
                schedule.sort()

                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'auto_finetune_schedule': schedule
                })
                db.commit()

                logger.info(f"å·²æ·»åŠ è‡ªåŠ¨å¾®è°ƒæ—¶é—´ç‚¹: plan_id={plan_id}, time={time_str}")
                return f"âœ… å·²æ·»åŠ æ—¶é—´ç‚¹ {time_str}", schedule

        except Exception as e:
            logger.error(f"æ·»åŠ è‡ªåŠ¨å¾®è°ƒæ—¶é—´ç‚¹å¤±è´¥: {e}")
            return f"âŒ æ·»åŠ å¤±è´¥: {str(e)}", []

    def remove_finetune_schedule_time(self, plan_id: int, time_str: str) -> tuple:
        """
        åˆ é™¤è‡ªåŠ¨å¾®è°ƒæ—¶é—´ç‚¹

        Args:
            plan_id: è®¡åˆ’ID
            time_str: æ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼ HH:MM

        Returns:
            (ç»“æœæ¶ˆæ¯, æ›´æ–°åçš„æ—¶é—´è¡¨åˆ—è¡¨)
        """
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨", []

                schedule = plan.auto_finetune_schedule or []

                if time_str not in schedule:
                    return f"âš ï¸ æ—¶é—´ç‚¹ {time_str} ä¸å­˜åœ¨", schedule

                schedule.remove(time_str)

                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'auto_finetune_schedule': schedule
                })
                db.commit()

                logger.info(f"å·²åˆ é™¤è‡ªåŠ¨å¾®è°ƒæ—¶é—´ç‚¹: plan_id={plan_id}, time={time_str}")
                return f"âœ… å·²åˆ é™¤æ—¶é—´ç‚¹ {time_str}", schedule

        except Exception as e:
            logger.error(f"åˆ é™¤è‡ªåŠ¨å¾®è°ƒæ—¶é—´ç‚¹å¤±è´¥: {e}")
            return f"âŒ åˆ é™¤å¤±è´¥: {str(e)}", []

    def get_inference_schedule(self, plan_id: int) -> list:
        """è·å–è‡ªåŠ¨é¢„æµ‹é—´éš”æ—¶é—´"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return [4]  # é»˜è®¤4å°æ—¶
                interval_hours = getattr(plan, 'auto_inference_interval_hours', 4)
                return [interval_hours] if interval_hours else [4]  # é»˜è®¤4å°æ—¶
        except Exception as e:
            logger.error(f"è·å–è‡ªåŠ¨é¢„æµ‹é—´éš”æ—¶é—´å¤±è´¥: {e}")
            return [4]

    def add_inference_schedule_time(self, plan_id: int, time_str: str) -> tuple:
        """
        æ·»åŠ è‡ªåŠ¨é¢„æµ‹æ—¶é—´ç‚¹

        Args:
            plan_id: è®¡åˆ’ID
            time_str: æ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼ HH:MM

        Returns:
            (ç»“æœæ¶ˆæ¯, æ›´æ–°åçš„æ—¶é—´è¡¨åˆ—è¡¨)
        """
        try:
            # éªŒè¯æ—¶é—´æ ¼å¼
            import re
            if not re.match(r'^([0-1]?[0-9]|2[0-3]):[0-5][0-9]$', time_str.strip()):
                return "âŒ æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨HH:MMæ ¼å¼ï¼ˆä¾‹å¦‚ï¼š06:00, 18:30ï¼‰", []

            time_str = time_str.strip()

            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨", []

                # ä¸ºäº†å…¼å®¹æ€§ï¼Œå°†æ—¶é—´ç‚¹è½¬æ¢ä¸ºé—´éš”æ—¶é—´
                # è§£æHH:MMï¼Œè½¬æ¢ä¸ºå°æ—¶æ•°ä½œä¸ºé—´éš”
                try:
                    if ':' in time_str:
                        hour, minute = time_str.split(':')
                        interval_hours = int(hour) + 1  # ç®€å•è½¬æ¢ï¼šæ—¶é—´ç‚¹+1å°æ—¶ä½œä¸ºé—´éš”
                    else:
                        interval_hours = 4  # é»˜è®¤4å°æ—¶
                except:
                    interval_hours = 4  # å‡ºé”™æ—¶ä½¿ç”¨é»˜è®¤å€¼

                # éªŒè¯é—´éš”æ—¶é—´
                if interval_hours <= 0:
                    interval_hours = 4
                elif interval_hours > 168:  # 7å¤©
                    interval_hours = 168

                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'auto_inference_interval_hours': interval_hours,
                    'auto_inference_enabled': True  # è®¾ç½®é—´éš”æ—¶é—´æ—¶è‡ªåŠ¨å¯ç”¨
                })
                db.commit()

                logger.info(f"å·²è®¾ç½®è‡ªåŠ¨é¢„æµ‹é—´éš”æ—¶é—´: plan_id={plan_id}, interval={interval_hours}å°æ—¶")
                return f"âœ… å·²è®¾ç½®ä¸º{interval_hours}å°æ—¶é—´éš”", [interval_hours]

        except Exception as e:
            logger.error(f"æ·»åŠ è‡ªåŠ¨é¢„æµ‹æ—¶é—´ç‚¹å¤±è´¥: {e}")
            return f"âŒ æ·»åŠ å¤±è´¥: {str(e)}", []

    def remove_inference_schedule_time(self, plan_id: int, time_str: str) -> tuple:
        """
        åˆ é™¤è‡ªåŠ¨é¢„æµ‹æ—¶é—´ç‚¹

        Args:
            plan_id: è®¡åˆ’ID
            time_str: æ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼ HH:MM

        Returns:
            (ç»“æœæ¶ˆæ¯, æ›´æ–°åçš„æ—¶é—´è¡¨åˆ—è¡¨)
        """
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨", []

                # å…¼å®¹æ€§æ–¹æ³•ï¼šé‡ç½®ä¸ºé»˜è®¤4å°æ—¶é—´éš”
                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'auto_inference_interval_hours': 4,
                    'auto_inference_enabled': False  # é‡ç½®æ—¶ç¦ç”¨
                })
                db.commit()

                logger.info(f"å·²é‡ç½®è‡ªåŠ¨é¢„æµ‹é—´éš”æ—¶é—´: plan_id={plan_id}, interval=4å°æ—¶")
                return f"âœ… å·²é‡ç½®ä¸º4å°æ—¶é—´éš”", [4]

        except Exception as e:
            logger.error(f"åˆ é™¤è‡ªåŠ¨é¢„æµ‹æ—¶é—´ç‚¹å¤±è´¥: {e}")
            return f"âŒ åˆ é™¤å¤±è´¥: {str(e)}", []

    def get_data_date_range(self, inst_id: str, interval: str):
        """
        è·å–Kçº¿æ•°æ®çš„æ—¥æœŸèŒƒå›´

        Returns:
            (min_date, max_date, count)
        """
        try:
            with get_db() as db:
                result = db.query(
                    func.min(KlineData.timestamp).label('min_date'),
                    func.max(KlineData.timestamp).label('max_date'),
                    func.count(KlineData.id).label('count')
                ).filter(
                    and_(
                        KlineData.inst_id == inst_id,
                        KlineData.interval == interval
                    )
                ).first()

                if result and result.min_date and result.max_date:
                    return result.min_date, result.max_date, result.count
                else:
                    return None, None, 0

        except Exception as e:
            logger.error(f"è·å–æ—¥æœŸèŒƒå›´å¤±è´¥: {e}")
            return None, None, 0

    def save_training_data_config(self, plan_id: int, start_date_str: str, end_date_str: str) -> tuple:
        """
        ä¿å­˜è®­ç»ƒæ•°æ®èŒƒå›´é…ç½®

        Args:
            plan_id: è®¡åˆ’ID
            start_date_str: å¼€å§‹æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
            end_date_str: ç»“æŸæ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)

        Returns:
            (ç»“æœæ¶ˆæ¯, æ•°æ®ç»Ÿè®¡ä¿¡æ¯)
        """
        try:
            from datetime import datetime

            # è§£ææ—¥æœŸ
            start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
            end_date = datetime.strptime(end_date_str, '%Y-%m-%d')

            if start_date >= end_date:
                return "âŒ å¼€å§‹æ—¥æœŸå¿…é¡»æ—©äºç»“æŸæ—¥æœŸ", ""

            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨", ""

                # ç»Ÿè®¡æ•°æ®é‡
                data_count = db.query(func.count(KlineData.id)).filter(
                    and_(
                        KlineData.inst_id == plan.inst_id,
                        KlineData.interval == plan.interval,
                        KlineData.timestamp >= start_date,
                        KlineData.timestamp <= end_date
                    )
                ).scalar()

                if data_count == 0:
                    return f"âš ï¸ è¯¥æ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ•°æ®", ""

                # è·å–ç°æœ‰çš„ finetune_params
                finetune_params = plan.finetune_params or {}
                if 'data' not in finetune_params:
                    finetune_params['data'] = {}

                # æ›´æ–°æ•°æ®èŒƒå›´é…ç½®
                finetune_params['data']['train_start_date'] = start_date_str
                finetune_params['data']['train_end_date'] = end_date_str

                # ä¿å­˜åˆ°æ•°æ®åº“
                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'finetune_params': finetune_params,
                    'data_start_time': start_date,
                    'data_end_time': end_date
                })
                db.commit()

                logger.info(f"è®­ç»ƒæ•°æ®é…ç½®å·²ä¿å­˜: plan_id={plan_id}, range={start_date_str} to {end_date_str}, count={data_count}")

                # æ„å»ºç»Ÿè®¡ä¿¡æ¯
                stats_info = f"""**å·²é…ç½®è®­ç»ƒæ•°æ®èŒƒå›´**

ğŸ“… **æ—¥æœŸ**: {start_date_str} è‡³ {end_date_str}
ğŸ“Š **æ•°æ®ç‚¹**: {data_count} æ¡
âœ… **çŠ¶æ€**: é…ç½®å·²ä¿å­˜
"""
                return "âœ… è®­ç»ƒæ•°æ®é…ç½®å·²ä¿å­˜", stats_info

        except ValueError as e:
            return f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {str(e)}", ""
        except Exception as e:
            logger.error(f"ä¿å­˜è®­ç»ƒæ•°æ®é…ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}", ""

    def get_training_data_stats(self, plan_id: int) -> str:
        """
        è·å–è®­ç»ƒæ•°æ®ç»Ÿè®¡ä¿¡æ¯

        Args:
            plan_id: è®¡åˆ’ID

        Returns:
            str - Markdown æ ¼å¼çš„ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "**æ•°æ®ç»Ÿè®¡**: è®¡åˆ’ä¸å­˜åœ¨"

                # è·å–æ•°æ®åº“ä¸­çš„å®é™…èŒƒå›´
                min_date, max_date, total_count = self.get_data_date_range(plan.inst_id, plan.interval)

                if min_date is None or max_date is None:
                    return "**æ•°æ®ç»Ÿè®¡**: æš‚æ— æ•°æ®"

                # ä» finetune_params ä¸­è·å–é…ç½®çš„èŒƒå›´
                finetune_params = plan.finetune_params or {}
                data_config = finetune_params.get('data', {})
                train_start_date_str = data_config.get('train_start_date')
                train_end_date_str = data_config.get('train_end_date')

                # è‡ªåŠ¨æ›´æ–°è®­ç»ƒæ•°æ®èŒƒå›´åˆ°æœ€æ–°æ•°æ®
                from datetime import datetime, timedelta
                if train_start_date_str and train_end_date_str:
                    try:
                        configured_start = datetime.strptime(train_start_date_str, '%Y-%m-%d')
                        configured_end = datetime.strptime(train_end_date_str, '%Y-%m-%d')

                        # å¦‚æœé…ç½®çš„ç»“æŸæ—¥æœŸæ—©äºæœ€æ–°æ•°æ®æ—¥æœŸï¼Œè‡ªåŠ¨æ›´æ–°
                        if configured_end.date() < max_date.date():
                            train_end_date_str = max_date.strftime('%Y-%m-%d')
                            # ç¡®ä¿å¼€å§‹æ—¥æœŸä¸ä¼šè¿‡æ™š
                            if configured_start.date() >= max_date.date():
                                train_start_date_str = (max_date - timedelta(days=30)).strftime('%Y-%m-%d')
                    except ValueError:
                        # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çš„æœ€è¿‘30å¤©
                        train_start_date_str = (max_date - timedelta(days=30)).strftime('%Y-%m-%d')
                        train_end_date_str = max_date.strftime('%Y-%m-%d')
                else:
                    # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨æœ€è¿‘30å¤©
                    train_start_date_str = (max_date - timedelta(days=30)).strftime('%Y-%m-%d')
                    train_end_date_str = max_date.strftime('%Y-%m-%d')

                # ç»Ÿè®¡æ›´æ–°åçš„è®­ç»ƒæ•°æ®é‡
                train_start = datetime.strptime(train_start_date_str, '%Y-%m-%d')
                train_end = datetime.strptime(train_end_date_str, '%Y-%m-%d')

                train_data_count = db.query(func.count(KlineData.id)).filter(
                    and_(
                        KlineData.inst_id == plan.inst_id,
                        KlineData.interval == plan.interval,
                        KlineData.timestamp >= train_start,
                        KlineData.timestamp <= train_end
                    )
                ).scalar()

                return f"""**è®­ç»ƒæ•°æ®ç»Ÿè®¡**

ğŸ“… **å½“å‰è®­ç»ƒèŒƒå›´**: {train_start_date_str} ~ {train_end_date_str}
ğŸ“Š **è®­ç»ƒæ•°æ®ç‚¹**: {train_data_count} æ¡

---

ğŸ“… **å…¨éƒ¨å¯ç”¨æ•°æ®**: {format_datetime_beijing(min_date, '%Y-%m-%d')} ~ {format_datetime_beijing(max_date, '%Y-%m-%d')}
ğŸ“Š **æ€»æ•°æ®ç‚¹**: {total_count} æ¡

âœ… **è‡ªåŠ¨æ›´æ–°è‡³æœ€æ–°æ•°æ®èŒƒå›´**
"""

        except Exception as e:
            logger.error(f"è·å–è®­ç»ƒæ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"**æ•°æ®ç»Ÿè®¡**: è·å–å¤±è´¥ - {str(e)}"

    def get_probability_indicators(self, plan_id: int) -> str:
        """
        è·å–æœ€æ–°é¢„æµ‹çš„æ¦‚ç‡æŒ‡æ ‡ï¼ˆä¸Šæ¶¨æ¦‚ç‡ã€æ³¢åŠ¨æ€§æ”¾å¤§æ¦‚ç‡ï¼‰

        Args:
            plan_id: è®¡åˆ’ID

        Returns:
            str - Markdown æ ¼å¼çš„æ¦‚ç‡æŒ‡æ ‡å±•ç¤º
        """
        try:
            with get_db() as db:
                # è·å–æœ€æ–°çš„å·²å®Œæˆè®­ç»ƒè®°å½•
                latest_training = db.query(TrainingRecord).filter(
                    and_(
                        TrainingRecord.plan_id == plan_id,
                        TrainingRecord.status == 'completed',
                        TrainingRecord.is_active == True
                    )
                ).order_by(desc(TrainingRecord.created_at)).first()

                if not latest_training:
                    return """
### ğŸ“Š æ¦‚ç‡æŒ‡æ ‡

æš‚æ— æ•°æ®ï¼ˆå°šæœªå®Œæˆæ¨ç†ï¼‰
"""

                # è·å–è¯¥è®­ç»ƒè®°å½•çš„æœ€æ–°ä¸€æ¡é¢„æµ‹æ•°æ®ï¼ˆæ¦‚ç‡æŒ‡æ ‡å¯¹æ‰€æœ‰æ—¶é—´ç‚¹ç›¸åŒï¼‰
                prediction = db.query(PredictionData).filter(
                    PredictionData.training_record_id == latest_training.id
                ).order_by(PredictionData.timestamp.desc()).first()

                if not prediction:
                    return """
### ğŸ“Š æ¦‚ç‡æŒ‡æ ‡

æš‚æ— æ•°æ®ï¼ˆå°šæœªå®Œæˆæ¨ç†ï¼‰
"""

                # è·å–æ¦‚ç‡æŒ‡æ ‡
                upward_prob = prediction.upward_probability
                volatility_amp_prob = prediction.volatility_amplification_probability

                # å¦‚æœæ²¡æœ‰æ¦‚ç‡æ•°æ®ï¼ˆæ—§ç‰ˆæœ¬æ¨ç†ç»“æœï¼‰
                if upward_prob is None or volatility_amp_prob is None:
                    return """
### ğŸ“Š æ¦‚ç‡æŒ‡æ ‡

âš ï¸ **å½“å‰é¢„æµ‹æ•°æ®ä¸åŒ…å«æ¦‚ç‡æŒ‡æ ‡**

è¯·é‡æ–°æ‰§è¡Œæ¨ç†ä»¥è·å–æœ€æ–°çš„æ¦‚ç‡æŒ‡æ ‡ï¼ˆéœ€è¦å¤šè·¯å¾„è’™ç‰¹å¡ç½—é‡‡æ ·ï¼‰
"""

                # æ ¼å¼åŒ–æ˜¾ç¤º
                upward_percent = upward_prob * 100
                volatility_percent = volatility_amp_prob * 100

                # æ ¹æ®æ¦‚ç‡å€¼é€‰æ‹©è¡¨æƒ…å’Œé¢œè‰²
                if upward_percent >= 60:
                    upward_emoji = "ğŸ“ˆ"
                    upward_color = "green"
                elif upward_percent >= 40:
                    upward_emoji = "â¡ï¸"
                    upward_color = "orange"
                else:
                    upward_emoji = "ğŸ“‰"
                    upward_color = "red"

                if volatility_percent >= 60:
                    volatility_emoji = "âš¡"
                    volatility_color = "red"
                elif volatility_percent >= 40:
                    volatility_emoji = "ã€°ï¸"
                    volatility_color = "orange"
                else:
                    volatility_emoji = "ğŸ˜´"
                    volatility_color = "green"

                return f"""
### ğŸ“Š æ¦‚ç‡æŒ‡æ ‡

<div style="display: flex; gap: 20px; margin: 10px 0;">
  <div style="flex: 1; padding: 15px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 10px; color: white;">
    <div style="font-size: 14px; opacity: 0.9;">ä¸Šæ¶¨æ¦‚ç‡ï¼ˆæœªæ¥é¢„æµ‹æœŸï¼‰</div>
    <div style="font-size: 36px; font-weight: bold; margin: 10px 0;">{upward_emoji} {upward_percent:.1f}%</div>
    <div style="font-size: 12px; opacity: 0.8;">æ¨¡å‹å¯¹ä»·æ ¼ä¸Šæ¶¨çš„ç½®ä¿¡åº¦</div>
  </div>
  <div style="flex: 1; padding: 15px; background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); border-radius: 10px; color: white;">
    <div style="font-size: 14px; opacity: 0.9;">æ³¢åŠ¨æ€§æ”¾å¤§</div>
    <div style="font-size: 36px; font-weight: bold; margin: 10px 0;">{volatility_emoji} {volatility_percent:.1f}%</div>
    <div style="font-size: 12px; opacity: 0.8;">æœªæ¥æ³¢åŠ¨ç‡è¶…è¿‡å†å²çš„æ¦‚ç‡</div>
  </div>
</div>

**æ•°æ®æ¥æº**: è®­ç»ƒç‰ˆæœ¬ v{latest_training.id} | åŸºäº {prediction.inference_params.get('sample_count', 1)} æ¡è’™ç‰¹å¡ç½—è·¯å¾„
"""

        except Exception as e:
            logger.error(f"è·å–æ¦‚ç‡æŒ‡æ ‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"""
### ğŸ“Š æ¦‚ç‡æŒ‡æ ‡

âŒ è·å–å¤±è´¥: {str(e)}
"""

    def set_training_date_range(self, inst_id: str, interval: str, days: int):
        """
        è®¾ç½®è®­ç»ƒæ•°æ®èŒƒå›´ï¼ˆå¿«æ·æŒ‰é’®ï¼‰

        Args:
            inst_id: äº¤æ˜“å¯¹
            interval: æ—¶é—´é¢—ç²’åº¦
            days: æœ€è¿‘å¤šå°‘å¤©

        Returns:
            (æ•°æ®èŒƒå›´ä¿¡æ¯, å¼€å§‹æ—¥æœŸ, ç»“æŸæ—¥æœŸ)
        """
        try:
            from datetime import timedelta
            min_date, max_date, count = self.get_data_date_range(inst_id, interval)

            if min_date is None or max_date is None:
                return (
                    "âš ï¸ **æ•°æ®èŒƒå›´**: æœªæ‰¾åˆ°æ•°æ®",
                    "",
                    ""
                )

            # è®¡ç®—å¼€å§‹æ—¥æœŸï¼ˆä»æœ€æ–°æ—¥æœŸå¾€å‰æ¨Nå¤©ï¼‰
            start_date = max_date - timedelta(days=days)

            # ç¡®ä¿å¼€å§‹æ—¥æœŸä¸æ—©äºæ•°æ®æœ€æ—©æ—¥æœŸ
            if start_date < min_date:
                start_date = min_date

            info = f"""
**æ•°æ®èŒƒå›´**: {format_datetime_beijing(min_date, '%Y-%m-%d')} è‡³ {format_datetime_beijing(max_date, '%Y-%m-%d')} (å…± {count} æ¡)

**å·²é€‰æ‹©**: æœ€è¿‘ {days} å¤© ({format_datetime_beijing(start_date, '%Y-%m-%d')} è‡³ {format_datetime_beijing(max_date, '%Y-%m-%d')})
"""

            return (
                info,
                format_datetime_beijing(start_date, '%Y-%m-%d'),
                format_datetime_beijing(max_date, '%Y-%m-%d')
            )

        except Exception as e:
            logger.error(f"è®¾ç½®è®­ç»ƒæ—¥æœŸèŒƒå›´å¤±è´¥: {e}")
            return (
                f"âŒ è®¾ç½®å¤±è´¥: {str(e)}",
                "",
                ""
            )

    async def manual_inference_async(self, plan_id: int) -> str:
        """æ‰‹åŠ¨æ‰§è¡ŒAI Agentæ¨ç†"""
        try:
            from services.agent_decision_service import AgentDecisionService
            from database.models import LLMConfig, AgentDecision
            import json

            # è·å–è®¡åˆ’é…ç½®
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨"

                # æ£€æŸ¥LLMé…ç½®
                if not plan.llm_config_id:
                    return "âŒ æœªé…ç½®LLMï¼Œè¯·å…ˆåœ¨Agenté…ç½®ä¸­é€‰æ‹©LLM"

                llm_config = db.query(LLMConfig).filter(LLMConfig.id == plan.llm_config_id).first()
                if not llm_config:
                    return "âŒ LLMé…ç½®ä¸å­˜åœ¨"

            # è·å–æœ€æ–°çš„è®­ç»ƒè®°å½•
            with get_db() as db:
                latest_training = db.query(TrainingRecord).filter(
                    and_(
                        TrainingRecord.plan_id == plan_id,
                        TrainingRecord.status == 'completed',
                        TrainingRecord.is_active == True
                    )
                ).order_by(desc(TrainingRecord.created_at)).first()

                if not latest_training:
                    return "âŒ æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒè®°å½•ï¼Œè¯·å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒ"

            # è§¦å‘AI Agentå†³ç­–
            decision_id = await AgentDecisionService.trigger_decision(plan_id, latest_training.id)

            if not decision_id:
                return "âŒ AI Agentå†³ç­–è§¦å‘å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—"

            # è·å–å†³ç­–ç»“æœ
            with get_db() as db:
                decision = db.query(AgentDecision).filter(
                    AgentDecision.id == decision_id
                ).first()

                if not decision:
                    return "âŒ æ— æ³•è·å–å†³ç­–ç»“æœ"

            # æ„å»ºè¿”å›ç»“æœ
            result_md = f"""## âœ… AI Agent æ¨ç†å®Œæˆ

**å†³ç­–æ—¶é—´**: {format_datetime_full_beijing(decision.decision_time)}

**LLM**: {llm_config.provider} / {llm_config.model_name}

**è®­ç»ƒç‰ˆæœ¬**: v{latest_training.version}

---

### ğŸ’­ AIåˆ†æä¸æ¨ç†

{decision.reasoning or 'æ— '}

---

### ğŸ› ï¸ å·¥å…·è°ƒç”¨

"""
            tool_calls = decision.tool_calls or []
            if tool_calls:
                for i, call in enumerate(tool_calls, 1):
                    result_md += f"**{i}. {call.get('name', 'unknown')}**\n"
                    result_md += f"   - å‚æ•°: `{call.get('arguments', {})}`\n"

                    # æ˜¾ç¤ºæ‰§è¡Œç»“æœ
                    if decision.tool_results and len(decision.tool_results) >= i:
                        result = decision.tool_results[i-1]
                        success = result.get('success', False)
                        status_emoji = 'âœ…' if success else 'âŒ'
                        result_md += f"   - ç»“æœ: {status_emoji} {result.get('message', result.get('error', 'N/A'))}\n"
                    result_md += "\n"
            else:
                result_md += "æ— å·¥å…·è°ƒç”¨\n"

            return result_md

        except Exception as e:
            logger.error(f"æ‰‹åŠ¨æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"âŒ æ¨ç†å¤±è´¥: {str(e)}"

    async def manual_inference_stream(self, plan_id: int):
        """æ‰‹åŠ¨æ‰§è¡ŒAI Agentæ¨ç†ï¼ˆæµå¼è¾“å‡ºï¼‰ï¼Œä½¿ç”¨æ–°çš„LangChain Agent v2æœåŠ¡"""
        try:
            from services.langchain_agent import agent_service

            # ä½¿ç”¨AgentæœåŠ¡è¿›è¡Œæµå¼æ¨ç†
            async for message_batch in agent_service.stream_conversation(
                plan_id=plan_id,
                user_message="è¯·æ ¹æ®æœ€æ–°æ•°æ®è¿›è¡Œåˆ†æå’Œå†³ç­–",
                conversation_type="auto_inference"
            ):
                yield message_batch

        except Exception as e:
            logger.error(f"æ‰‹åŠ¨æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            yield [{"role": "assistant", "content": f"âŒ æ¨ç†å¤±è´¥: {str(e)}"}]

    async def chat_with_agent_stream(self, plan_id: int, user_message: str):
        """ä¸AI Agentè¿›è¡Œæµå¼å¯¹è¯"""
        try:
            from services.langchain_agent import agent_service

            # ä½¿ç”¨AgentæœåŠ¡è¿›è¡Œæµå¼å¯¹è¯
            async for message_batch in agent_service.stream_conversation(
                plan_id=plan_id,
                user_message=user_message
            ):
                yield message_batch

        except Exception as e:
            logger.error(f"Agentå¯¹è¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            yield [{"role": "assistant", "content": f"âŒ å¯¹è¯å¤±è´¥: {str(e)}"}]

    def _build_inference_system_prompt(self, plan, latest_training) -> str:
        """æ„å»ºæ¨ç†ç³»ç»Ÿæç¤ºè¯"""
        try:
            # å¯¼å…¥å·¥å…·è·å–å‡½æ•°
            from services.agent_tools import get_all_tools

            # è·å–é¢„æµ‹æ•°æ®
            prediction_data = []
            with get_db() as db:
                if latest_training:
                    prediction_data = db.query(PredictionData).filter(
                        PredictionData.training_record_id == latest_training.id
                    ).order_by(PredictionData.timestamp.desc()).limit(10).all()

            # åŸºç¡€ç³»ç»Ÿæç¤º
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“AIåŠ©æ‰‹ï¼Œè´Ÿè´£åˆ†æå¸‚åœºæ•°æ®å¹¶åšå‡ºäº¤æ˜“å†³ç­–ã€‚

**äº¤æ˜“è®¡åˆ’ä¿¡æ¯**:
- äº¤æ˜“å¯¹: {plan.inst_id}
- æ—¶é—´å‘¨æœŸ: {plan.interval}
- ç¯å¢ƒ: {'ğŸ§ª æ¨¡æ‹Ÿç›˜' if plan.is_demo else 'ğŸ’° å®ç›˜'}
- è®¡åˆ’çŠ¶æ€: {plan.status}

**æ¨ç†ä»»åŠ¡**:
åŸºäºKronosæ¨¡å‹çš„é¢„æµ‹æ•°æ®ï¼Œä½¿ç”¨ReActæ¨¡å¼è¿›è¡Œæ€è€ƒå’Œå†³ç­–ï¼š
1. **æ€è€ƒ** (Thought): åˆ†æå¸‚åœºçŠ¶å†µå’Œé¢„æµ‹æ•°æ®
2. **è¡ŒåŠ¨** (Action): è°ƒç”¨å·¥å…·è·å–æ›´å¤šä¿¡æ¯æˆ–æ‰§è¡Œäº¤æ˜“
3. **è§‚å¯Ÿ** (Observation): åˆ†æå·¥å…·è¿”å›çš„ç»“æœ
4. **é‡å¤** ç›´åˆ°å¾—å‡ºæœ€ç»ˆç»“è®º

**å¯ç”¨å·¥å…·**:
ä½ å¯ä»¥è°ƒç”¨ä»¥ä¸‹å·¥å…·æ¥è·å–ä¿¡æ¯å’Œæ‰§è¡Œæ“ä½œï¼š"""

            # æ·»åŠ å·¥å…·è¯´æ˜
            tools_config = plan.agent_tools_config or {}
            for tool_name, tool_obj in get_all_tools().items():
                if tools_config.get(tool_name, False):
                    description = tool_obj.description
                    system_prompt += f"- {tool_name}: {description}\n"

            # æ·»åŠ é¢„æµ‹æ•°æ®ä¿¡æ¯
            if prediction_data:
                latest_prediction = prediction_data[0]
                system_prompt += f"""

**æœ€æ–°é¢„æµ‹æ•°æ®æ¦‚è§ˆ**:
- å½“å‰ä»·æ ¼: ${latest_prediction.close or 0:.4f}
- é¢„æµ‹åŒºé—´: ${latest_prediction.close_min or 0:.4f} ~ ${latest_prediction.close_max or 0:.4f}
- ä¸Šæ¶¨æ¦‚ç‡: {latest_prediction.upward_probability or 0:.2%}
- æ³¢åŠ¨æ”¾å¤§æ¦‚ç‡: {latest_prediction.volatility_amplification_probability or 0:.2%}
- æ¨¡å‹ç‰ˆæœ¬: {latest_training.version if latest_training else 'N/A'}"""

            # æ·»åŠ è‡ªå®šä¹‰æç¤ºè¯
            if plan.agent_prompt:
                system_prompt += f"""

**é¢å¤–æŒ‡ç¤º**:
{plan.agent_prompt}"""

            system_prompt += """

ç°åœ¨è¯·åŸºäºç”¨æˆ·æä¾›çš„æœ€æ–°é¢„æµ‹æ•°æ®è¿›è¡Œè¯¦ç»†åˆ†æã€‚"""

            return system_prompt

        except Exception as e:
            logger.error(f"æ„å»ºç³»ç»Ÿæç¤ºè¯å¤±è´¥: {e}")
            return "ç³»ç»Ÿæç¤ºè¯æ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚"

    def _get_latest_prediction_data_text(self, training_record_id: int) -> str:
        """è·å–æœ€æ–°é¢„æµ‹æ•°æ®çš„æ–‡æœ¬æ ¼å¼"""
        try:
            with get_db() as db:
                # è·å–æœ€æ–°çš„é¢„æµ‹æ•°æ®
                latest_prediction = db.query(PredictionData).filter(
                    PredictionData.training_record_id == training_record_id
                ).order_by(PredictionData.timestamp.desc()).first()

                if not latest_prediction:
                    return None

                # å®‰å…¨å¤„ç†æ•°æ®
                current_price = latest_prediction.close or 0
                upward_prob = latest_prediction.upward_probability or 0
                volatility_prob = latest_prediction.volatility_amplification_probability or 0
                close_min = latest_prediction.close_min or 0
                close_max = latest_prediction.close_max or 0

                # é¢„æµ‹è¶‹åŠ¿åˆ¤æ–­
                trend = 'æœªçŸ¥'
                if close_max > current_price:
                    trend = 'ğŸ“ˆ ä¸Šæ¶¨'
                elif close_max < current_price:
                    trend = 'ğŸ“‰ ä¸‹è·Œ'
                else:
                    trend = 'â¡ï¸ æ¨ªç›˜'

                return f"""**å½“å‰ä»·æ ¼**: ${current_price:.4f}
**é¢„æµ‹è¶‹åŠ¿**: {trend}
**ä»·æ ¼åŒºé—´**: ${close_min:.4f} ~ ${close_max:.4f}
**ä¸Šæ¶¨æ¦‚ç‡**: {upward_prob:.2%}
**æ³¢åŠ¨æ”¾å¤§æ¦‚ç‡**: {volatility_prob:.2%}
**é¢„æµ‹æ—¶é—´**: {latest_prediction.timestamp.strftime('%Y-%m-%d %H:%M:%S')}"""

        except Exception as e:
            logger.error(f"è·å–é¢„æµ‹æ•°æ®æ–‡æœ¬å¤±è´¥: {e}")
            return None

    # continue_inference_streamæ–¹æ³•å·²ç§»é™¤ - å·¥å…·ç¡®è®¤åŠŸèƒ½å·²åºŸå¼ƒï¼ŒAI Agentç°åœ¨ç›´æ¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·

    async def _get_prediction_preview(self, plan_id: int, training_id: int) -> str:
        """è·å–é¢„æµ‹æ•°æ®é¢„è§ˆ"""
        try:
            from database.models import PredictionData
            from database.db import get_db
            from sqlalchemy import desc

            with get_db() as db:
                # è·å–æœ€æ–°çš„é¢„æµ‹æ•°æ®
                latest_prediction = db.query(PredictionData).filter(
                    PredictionData.training_record_id == training_id
                ).order_by(desc(PredictionData.timestamp)).first()

                if not latest_prediction:
                    return "âš ï¸ æš‚æ— é¢„æµ‹æ•°æ®"

                # å®‰å…¨å¤„ç†æ¦‚ç‡å€¼çš„æ ¼å¼åŒ–
                upward_prob = latest_prediction.upward_probability
                volatility_prob = latest_prediction.volatility_amplification_probability

                upward_prob_str = f"{upward_prob:.2%}" if upward_prob is not None else "N/A"
                volatility_prob_str = f"{volatility_prob:.2%}" if volatility_prob is not None else "N/A"

                # å®‰å…¨å¤„ç†ä»·æ ¼åŒºé—´çš„æ ¼å¼åŒ–
                close_min = latest_prediction.close_min
                close_max = latest_prediction.close_max
                current_price = latest_prediction.close

                close_min_str = f"${close_min:.4f}" if close_min is not None else "N/A"
                close_max_str = f"${close_max:.4f}" if close_max is not None else "N/A"
                current_price_str = f"${current_price:.4f}" if current_price is not None else "N/A"

                # é¢„æµ‹è¶‹åŠ¿åˆ¤æ–­
                trend = 'æœªçŸ¥'
                if close_max is not None and current_price is not None:
                    trend = 'ğŸ“ˆ ä¸Šæ¶¨' if close_max > current_price else 'ğŸ“‰ ä¸‹è·Œ' if close_max < current_price else 'â¡ï¸ æ¨ªç›˜'

                return f"""ğŸ“Š **æœ€æ–°é¢„æµ‹æ•°æ®**:
- **å½“å‰ä»·æ ¼**: {current_price_str}
- **é¢„æµ‹è¶‹åŠ¿**: {trend}
- **ä»·æ ¼åŒºé—´**: {close_min_str} ~ {close_max_str}
- **ä¸Šæ¶¨æ¦‚ç‡**: {upward_prob_str}
- **æ³¢åŠ¨æ”¾å¤§æ¦‚ç‡**: {volatility_prob_str}
- **é¢„æµ‹æ—¶é—´**: {latest_prediction.timestamp.strftime('%Y-%m-%d %H:%M:%S')}"""

        except Exception as e:
            logger.error(f"è·å–é¢„æµ‹é¢„è§ˆå¤±è´¥: {e}")
            return "âš ï¸ æ— æ³•è·å–é¢„æµ‹æ•°æ®"

    def _format_tool_arguments(self, tool_args: dict) -> str:
        """æ ¼å¼åŒ–å·¥å…·å‚æ•°"""
        if not tool_args:
            return "- æ— å‚æ•°"

        formatted_lines = []
        for key, value in tool_args.items():
            if isinstance(value, dict):
                value_str = json.dumps(value, ensure_ascii=False, indent=2)
                formatted_lines.append(f"- `{key}`: {value_str}")
            else:
                formatted_lines.append(f"- `{key}`: `{value}`")

        return "\n".join(formatted_lines)

    def _format_tool_result(self, tool_result: dict) -> str:
        """æ ¼å¼åŒ–å·¥å…·æ‰§è¡Œç»“æœ"""
        if not tool_result:
            return "- æ— è¿”å›æ•°æ®"

        # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
        success = tool_result.get('success', False)
        status_emoji = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"

        result_lines = [f"**çŠ¶æ€**: {status_emoji}"]

        # æ·»åŠ æ¶ˆæ¯
        if tool_result.get('message'):
            result_lines.append(f"**è¯´æ˜**: {tool_result['message']}")

        # æ·»åŠ æ•°æ®ç»Ÿè®¡
        data = tool_result.get('data')
        if data:
            if isinstance(data, dict):
                if 'count' in data:
                    result_lines.append(f"**æ•°æ®é‡**: {data['count']} æ¡")
                if 'total' in data:
                    result_lines.append(f"**æ€»æ•°**: {data['total']}")
                if 'records' in data and isinstance(data['records'], list):
                    result_lines.append(f"**è®°å½•æ•°**: {len(data['records'])} æ¡")
                    # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®ç¤ºä¾‹
                    if len(data['records']) > 0:
                        sample_record = data['records'][0]
                        result_lines.append(f"**æ•°æ®ç¤ºä¾‹**: {str(sample_record)[:150]}...")
            elif isinstance(data, list):
                result_lines.append(f"**æ•°ç»„é•¿åº¦**: {len(data)} é¡¹")
                if len(data) > 0:
                    result_lines.append(f"**ç¤ºä¾‹æ•°æ®**: {str(data[0])[:150]}...")

        # æ·»åŠ é”™è¯¯ä¿¡æ¯
        if tool_result.get('error'):
            result_lines.append(f"**é”™è¯¯ä¿¡æ¯**: {tool_result['error']}")

        return "\n".join(result_lines) if len(result_lines) > 1 else result_lines[0]

    def _build_system_message(self, plan):
        """æ„å»ºç³»ç»Ÿæ¶ˆæ¯"""
        if plan.agent_prompt:
            return plan.agent_prompt

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“AIåŠ©æ‰‹ï¼Œè´Ÿè´£åˆ†æå¸‚åœºæ•°æ®å¹¶åšå‡ºäº¤æ˜“å†³ç­–ã€‚

**äº¤æ˜“å¯¹**: {plan.inst_id}
**æ—¶é—´å‘¨æœŸ**: {plan.interval}
**ç¯å¢ƒ**: {'æ¨¡æ‹Ÿç›˜' if plan.is_demo else 'å®ç›˜'}

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ†æKronosæ¨¡å‹çš„ä»·æ ¼é¢„æµ‹æ•°æ®
2. ç»“åˆå½“å‰è´¦æˆ·çŠ¶æ€å’ŒæŒä»“æƒ…å†µ
3. åšå‡ºåˆç†çš„äº¤æ˜“å†³ç­–ï¼ˆä¹°å…¥/å–å‡º/æŒæœ‰ï¼‰
4. å¿…è¦æ—¶è°ƒç”¨å·¥å…·æ‰§è¡Œäº¤æ˜“æ“ä½œ

è¯·å§‹ç»ˆè°¨æ…å†³ç­–ï¼Œæ§åˆ¶é£é™©ã€‚
"""

    def _build_user_message_with_prediction(self, plan, predictions):
        """æ„å»ºåŒ…å«é¢„æµ‹æ•°æ®çš„ç”¨æˆ·æ¶ˆæ¯"""
        if not predictions or len(predictions) == 0:
            return "âš ï¸ æ— é¢„æµ‹æ•°æ®å¯ç”¨ï¼Œè¯·å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒå’Œæ¨ç†ã€‚"

        # è®¡ç®—è¶‹åŠ¿
        first_close = predictions[0]['close']
        last_close = predictions[-1]['close']
        change_pct = ((last_close - first_close) / first_close) * 100

        # æå–å…³é”®æ•°æ®ç‚¹
        first_5 = predictions[:5] if len(predictions) >= 5 else predictions
        last_5 = predictions[-5:] if len(predictions) >= 5 else []

        # æ—¶é—´èŒƒå›´
        first_time = format_datetime_beijing(predictions[0]['timestamp'], '%Y-%m-%d %H:%M') if hasattr(predictions[0]['timestamp'], 'strftime') else str(predictions[0]['timestamp'])[:16]
        last_time = format_datetime_beijing(predictions[-1]['timestamp'], '%Y-%m-%d %H:%M') if hasattr(predictions[-1]['timestamp'], 'strftime') else str(predictions[-1]['timestamp'])[:16]

        # ä»·æ ¼ç»Ÿè®¡
        close_prices = [p['close'] for p in predictions]
        min_close = min(close_prices)
        max_close = max(close_prices)

        pred_summary = f"""**é¢„æµ‹æ—¶é•¿**: æœªæ¥ {len(predictions)} ä¸ªå‘¨æœŸ

**æ—¶é—´èŒƒå›´**: {first_time} ~ {last_time}

**å½“å‰ä»·æ ¼**: ${first_close:.4f}

**é¢„æµ‹ä»·æ ¼**: ${last_close:.4f}

**ä»·æ ¼åŒºé—´**: ${min_close:.4f} ~ ${max_close:.4f}

**é¢„æµ‹æ¶¨è·Œ**: {change_pct:+.2f}%

**è¶‹åŠ¿åˆ¤æ–­**: {'ğŸ“ˆ ä¸Šæ¶¨è¶‹åŠ¿' if change_pct > 0 else 'ğŸ“‰ ä¸‹è·Œè¶‹åŠ¿' if change_pct < 0 else 'â¡ï¸ æ¨ªç›˜'}
"""

        message = f"""## ğŸ“Š Kronosæ¨¡å‹é¢„æµ‹åˆ†æ

{pred_summary}

### é¢„æµ‹æ•°æ®ï¼ˆå‰5ä¸ªå‘¨æœŸï¼‰

"""
        for i, p in enumerate(first_5, 1):
            timestamp_str = format_datetime_beijing(p['timestamp'], '%Y-%m-%d %H:%M') if hasattr(p['timestamp'], 'strftime') else str(p['timestamp'])[:19]
            message += f"{i}. **{timestamp_str}** - å¼€: ${p['open']:.2f}, é«˜: ${p['high']:.2f}, ä½: ${p['low']:.2f}, æ”¶: ${p['close']:.2f}\n"

        if last_5:
            message += "\n...\n\n### é¢„æµ‹æ•°æ®ï¼ˆå5ä¸ªå‘¨æœŸï¼‰\n\n"

            for i, p in enumerate(last_5, 1):
                timestamp_str = format_datetime_beijing(p['timestamp'], '%Y-%m-%d %H:%M') if hasattr(p['timestamp'], 'strftime') else str(p['timestamp'])[:19]
                message += f"{i}. **{timestamp_str}** - å¼€: ${p['open']:.2f}, é«˜: ${p['high']:.2f}, ä½: ${p['low']:.2f}, æ”¶: ${p['close']:.2f}\n"

        message += f"""

## ğŸ¯ è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡

1. åˆ†æé¢„æµ‹è¶‹åŠ¿å’Œå…³é”®ä»·æ ¼æ°´å¹³
2. æŸ¥è¯¢å½“å‰è´¦æˆ·ä½™é¢å’ŒæŒä»“æƒ…å†µ
3. åŸºäºé¢„æµ‹å’Œè´¦æˆ·çŠ¶æ€ï¼Œåˆ¶å®šäº¤æ˜“ç­–ç•¥
4. å¦‚æœåˆé€‚ï¼Œæ‰§è¡Œäº¤æ˜“æ“ä½œ

è¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œå†³ç­–ç†ç”±ã€‚
"""

        return message

    def _get_tool_definitions(self, plan):
        """è·å–å·¥å…·å®šä¹‰"""
        tools_config = plan.agent_tools_config or {}
        tools = []

        # 1. ğŸ”® query_prediction_data - æŒ‰æ—¶é—´èŒƒå›´å’Œæ‰¹æ¬¡IDæŸ¥è¯¢é¢„æµ‹æ•°æ®
        if tools_config.get('query_prediction_data', True):
            tools.append({
                "type": "function",
                "function": {
                    "name": "query_prediction_data",
                    "description": "æŒ‰æ—¶é—´èŒƒå›´å’Œæ‰¹æ¬¡IDæŸ¥è¯¢é¢„æµ‹æ•°æ®",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "start_time": {"type": "string", "description": "å¼€å§‹æ—¶é—´ (YYYY-MM-DD HH:MM:SS)"},
                            "end_time": {"type": "string", "description": "ç»“æŸæ—¶é—´ (YYYY-MM-DD HH:MM:SS)"},
                            "batch_id": {"type": "string", "description": "æ¨ç†æ‰¹æ¬¡ID"}
                        },
                        "required": []
                    }
                }
            })

        # 2. ğŸ“ˆ get_prediction_history - æŸ¥è¯¢å†å²é¢„æµ‹æ‰¹æ¬¡åˆ—è¡¨ï¼ˆæœ€å¤š30æ‰¹æ¬¡ï¼‰
        if tools_config.get('get_prediction_history', True):
            tools.append({
                "type": "function",
                "function": {
                    "name": "get_prediction_history",
                    "description": "æŸ¥è¯¢å†å²é¢„æµ‹æ‰¹æ¬¡åˆ—è¡¨ï¼ˆæœ€å¤š30æ‰¹æ¬¡ï¼‰",
                    "parameters": {"type": "object", "properties": {}}
                }
            })

        # 3. ğŸ“ˆ query_historical_kline_data - æŸ¥è¯¢å†å²Kçº¿æ•°æ®ï¼ˆUTC+8æ—¶é—´æˆ³ï¼‰
        if tools_config.get('query_historical_kline_data', True):
            tools.append({
                "type": "function",
                "function": {
                    "name": "query_historical_kline_data",
                    "description": "æŸ¥è¯¢å†å²Kçº¿æ•°æ®ï¼ˆUTC+8æ—¶é—´æˆ³ï¼‰",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "start_time": {"type": "string", "description": "å¼€å§‹æ—¶é—´ (YYYY-MM-DD HH:MM:SS)"},
                            "end_time": {"type": "string", "description": "ç»“æŸæ—¶é—´ (YYYY-MM-DD HH:MM:SS)"},
                            "limit": {"type": "integer", "description": "æ•°æ®æ¡æ•°é™åˆ¶"}
                        },
                        "required": []
                    }
                }
            })

        # 4. ğŸ•’ get_current_utc_time - è·å–å½“å‰UTC+8æ—¶é—´
        if tools_config.get('get_current_utc_time', True):
            tools.append({
                "type": "function",
                "function": {
                    "name": "get_current_utc_time",
                    "description": "è·å–å½“å‰UTC+8æ—¶é—´",
                    "parameters": {"type": "object", "properties": {}}
                }
            })

        # 5. ğŸ¤– run_latest_model_inference - è§¦å‘æœ€æ–°æ¨¡å‹æ¨ç†
        if tools_config.get('run_latest_model_inference', False):
            tools.append({
                "type": "function",
                "function": {
                    "name": "run_latest_model_inference",
                    "description": "è§¦å‘æœ€æ–°æ¨¡å‹æ¨ç†",
                    "parameters": {"type": "object", "properties": {}}
                }
            })

        # 6. ğŸ” get_account_balance - æŸ¥è¯¢è´¦æˆ·ä½™é¢
        if tools_config.get('get_account_balance', True):
            tools.append({
                "type": "function",
                "function": {
                    "name": "get_account_balance",
                    "description": "æŸ¥è¯¢è´¦æˆ·ä½™é¢",
                    "parameters": {"type": "object", "properties": {}}
                }
            })

        # 7. ğŸ“‹ get_pending_orders - æŸ¥è¯¢æŒ‚å•
        if tools_config.get('get_pending_orders', True):
            tools.append({
                "type": "function",
                "function": {
                    "name": "get_pending_orders",
                    "description": "æŸ¥è¯¢æŒ‚å•",
                    "parameters": {"type": "object", "properties": {}}
                }
            })

        # 8. ğŸ’° place_order - ä¸‹é™ä»·å•
        if tools_config.get('place_order', True):
            tools.append({
                "type": "function",
                "function": {
                    "name": "place_order",
                    "description": "ä¸‹é™ä»·å•",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "side": {"type": "string", "enum": ["buy", "sell"]},
                            "size": {"type": "number"},
                            "price": {"type": "number"}
                        },
                        "required": ["side", "size"]
                    }
                }
            })

        # 9. âŒ cancel_order - æ’¤å•
        if tools_config.get('cancel_order', True):
            tools.append({
                "type": "function",
                "function": {
                    "name": "cancel_order",
                    "description": "æ’¤å•",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "order_id": {"type": "string", "description": "è®¢å•ID"}
                        },
                        "required": ["order_id"]
                    }
                }
            })

        # 10. âœï¸ amend_order - æ”¹å•
        if tools_config.get('amend_order', True):
            tools.append({
                "type": "function",
                "function": {
                    "name": "amend_order",
                    "description": "æ”¹å•",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "order_id": {"type": "string", "description": "è®¢å•ID"},
                            "new_size": {"type": "number", "description": "æ–°æ•°é‡"},
                            "new_price": {"type": "number", "description": "æ–°ä»·æ ¼"}
                        },
                        "required": ["order_id"]
                    }
                }
            })

        return tools if tools else None

    def get_prediction_text(self, training_id: int) -> str:
        """
        è·å–é¢„æµ‹æ•°æ®çš„æ–‡æœ¬æ ¼å¼ï¼ˆä¾›AI Agentä½¿ç”¨ï¼‰

        Args:
            training_id: è®­ç»ƒè®°å½•ID

        Returns:
            é¢„æµ‹æ•°æ®çš„æ–‡æœ¬æè¿°
        """
        try:
            predictions = InferenceService.get_prediction_data(training_id)

            if not predictions or len(predictions) == 0:
                return "âš ï¸ æš‚æ— é¢„æµ‹æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œ\"é¢„æµ‹äº¤æ˜“æ•°æ®\"æˆ–\"Mocké¢„æµ‹\""

            # è·å–æ¦‚ç‡æŒ‡æ ‡ï¼ˆä»æ•°æ®åº“ï¼‰
            upward_prob = None
            volatility_amp_prob = None
            sample_count = 1

            with get_db() as db:
                pred = db.query(PredictionData).filter(
                    PredictionData.training_record_id == training_id
                ).order_by(PredictionData.timestamp.desc()).first()

                if pred:
                    upward_prob = pred.upward_probability
                    volatility_amp_prob = pred.volatility_amplification_probability
                    if pred.inference_params:
                        sample_count = pred.inference_params.get('sample_count', 1)

            # æ„å»ºæ–‡æœ¬æ ¼å¼
            text_lines = []
            text_lines.append(f"ğŸ“Š é¢„æµ‹æ•°æ®ç»Ÿè®¡")
            text_lines.append(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            text_lines.append(f"é¢„æµ‹æ•°æ®æ¡æ•°: {len(predictions)}æ¡")

            # æ˜¾ç¤ºæ¦‚ç‡æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if upward_prob is not None and volatility_amp_prob is not None:
                text_lines.append(f"è’™ç‰¹å¡ç½—è·¯å¾„: {sample_count}æ¡")
                text_lines.append(f"")
                text_lines.append(f"ğŸ“Š æ¦‚ç‡æŒ‡æ ‡")
                text_lines.append(f"  â€¢ ä¸Šæ¶¨æ¦‚ç‡: {upward_prob*100:.1f}%")
                text_lines.append(f"  â€¢ æ³¢åŠ¨æ€§æ”¾å¤§æ¦‚ç‡: {volatility_amp_prob*100:.1f}%")

            first_pred = predictions[0]
            last_pred = predictions[-1]

            # æ—¶é—´èŒƒå›´
            first_time = format_datetime_beijing(first_pred['timestamp'], '%Y-%m-%d %H:%M') if hasattr(first_pred['timestamp'], 'strftime') else str(first_pred['timestamp'])[:16]
            last_time = format_datetime_beijing(last_pred['timestamp'], '%Y-%m-%d %H:%M') if hasattr(last_pred['timestamp'], 'strftime') else str(last_pred['timestamp'])[:16]
            text_lines.append(f"")
            text_lines.append(f"æ—¶é—´èŒƒå›´: {first_time} ~ {last_time}")

            # ä»·æ ¼ç»Ÿè®¡
            close_prices = [p['close'] for p in predictions]
            min_close = min(close_prices)
            max_close = max(close_prices)
            first_close = close_prices[0]
            last_close = close_prices[-1]

            text_lines.append(f"")
            text_lines.append(f"ä»·æ ¼åŒºé—´: ${min_close:.2f} ~ ${max_close:.2f}")
            text_lines.append(f"èµ·å§‹ä»·æ ¼: ${first_close:.2f}")
            text_lines.append(f"ç»“æŸä»·æ ¼: ${last_close:.2f}")

            # è¶‹åŠ¿åˆ¤æ–­
            change_pct = ((last_close - first_close) / first_close) * 100
            trend = "ğŸ“ˆ ä¸Šæ¶¨è¶‹åŠ¿" if change_pct > 0 else "ğŸ“‰ ä¸‹è·Œè¶‹åŠ¿" if change_pct < 0 else "â¡ï¸ æ¨ªç›˜"
            text_lines.append(f"é¢„æµ‹æ¶¨è·Œ: {change_pct:+.2f}%")
            text_lines.append(f"è¶‹åŠ¿åˆ¤æ–­: {trend}")

            text_lines.append(f"")
            text_lines.append(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
            text_lines.append(f"ğŸ“‹ è¯¦ç»†é¢„æµ‹æ•°æ® (å‰10æ¡)")
            text_lines.append(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

            # æ˜¾ç¤ºå‰10æ¡è¯¦ç»†æ•°æ®
            for i, pred in enumerate(predictions[:10], 1):
                timestamp_str = format_datetime_short_beijing(pred['timestamp']) if hasattr(pred['timestamp'], 'strftime') else str(pred['timestamp'])[:16]
                text_lines.append(
                    f"{i:2d}. {timestamp_str} | "
                    f"å¼€: ${pred['open']:7.2f} | é«˜: ${pred['high']:7.2f} | "
                    f"ä½: ${pred['low']:7.2f} | æ”¶: ${pred['close']:7.2f}"
                )

            if len(predictions) > 10:
                text_lines.append(f"... (å…±{len(predictions)}æ¡ï¼Œä»…æ˜¾ç¤ºå‰10æ¡)")

            return "\n".join(text_lines)

        except Exception as e:
            logger.error(f"è·å–é¢„æµ‹æ•°æ®æ–‡æœ¬å¤±è´¥: {e}")
            return f"âŒ è·å–é¢„æµ‹æ•°æ®å¤±è´¥: {str(e)}"

    async def execute_inference_async(self, training_id: int) -> str:
        """æ‰§è¡ŒKronosæ¨¡å‹æ¨ç†"""
        try:
            success = await InferenceService.start_inference(training_id)
            if success:
                return f"âœ… æ¨ç†å·²å®Œæˆï¼Œè®­ç»ƒè®°å½•ID: {training_id}"
            else:
                return f"âŒ æ¨ç†å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—"
        except Exception as e:
            logger.error(f"æ‰§è¡Œæ¨ç†å¤±è´¥: {e}")
            return f"âŒ æ¨ç†å¤±è´¥: {str(e)}"

    async def mock_predictions_async(self, training_id: int) -> str:
        """ç”ŸæˆMocké¢„æµ‹æ•°æ®"""
        try:
            result = InferenceService.mock_prediction_data(training_id, predict_window=48)
            if result['success']:
                return f"âœ… Mockæ•°æ®å·²ç”Ÿæˆï¼Œå…± {result['predictions_count']} æ¡"
            else:
                return f"âŒ ç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
        except Exception as e:
            logger.error(f"ç”ŸæˆMockæ•°æ®å¤±è´¥: {e}")
            return f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"

    async def start_training_async(self, plan_id: int, train_start_date: str = None, train_end_date: str = None):
        """
        å¼€å§‹è®­ç»ƒï¼ˆå¼‚æ­¥ï¼‰- ä½¿ç”¨ç”Ÿæˆå™¨å®æ—¶è¿”å›è¿›åº¦

        Args:
            plan_id: è®¡åˆ’ID
            train_start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ (YYYY-MM-DD)ï¼Œä¸ºç©ºåˆ™ä½¿ç”¨è®¡åˆ’é…ç½®
            train_end_date: è®­ç»ƒç»“æŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œä¸ºç©ºåˆ™ä½¿ç”¨è®¡åˆ’é…ç½®

        Yields:
            è®­ç»ƒè¿›åº¦æ¶ˆæ¯
        """
        try:
            from datetime import datetime
            import time

            # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œä¸´æ—¶æ›´æ–°è®¡åˆ’çš„æ•°æ®æ—¶é—´èŒƒå›´
            if train_start_date and train_end_date:
                try:
                    start_dt = datetime.strptime(train_start_date, '%Y-%m-%d')
                    end_dt = datetime.strptime(train_end_date, '%Y-%m-%d')

                    # ä¸´æ—¶æ›´æ–°è®¡åˆ’çš„æ•°æ®æ—¶é—´èŒƒå›´
                    with get_db() as db:
                        db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                            'data_start_time': start_dt,
                            'data_end_time': end_dt
                        })
                        db.commit()

                    logger.info(f"å·²æ›´æ–°è®­ç»ƒæ•°æ®èŒƒå›´: {train_start_date} è‡³ {train_end_date}")

                except ValueError as e:
                    yield f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {str(e)}"
                    return

            # å¯åŠ¨è®­ç»ƒ
            training_id = await TrainingService.start_training(plan_id, manual=True)
            if not training_id:
                yield "âŒ è®­ç»ƒå¯åŠ¨å¤±è´¥"
                return

            yield f"âœ… è®­ç»ƒå·²å¯åŠ¨ï¼Œè®°å½•ID: {training_id}\n\nå¼€å§‹è®­ç»ƒ..."

            # è½®è¯¢è®­ç»ƒè¿›åº¦
            last_progress = -1
            max_wait_time = 3600  # æœ€å¤šç­‰å¾…1å°æ—¶
            start_time = time.time()

            while True:
                # æ£€æŸ¥è¶…æ—¶
                if time.time() - start_time > max_wait_time:
                    yield "\n\nâš ï¸ è®­ç»ƒè¶…æ—¶ï¼ˆè¶…è¿‡1å°æ—¶ï¼‰"
                    break

                # è·å–è¿›åº¦
                progress_info = TrainingService.get_training_progress(training_id)

                if progress_info:
                    current_progress = progress_info['progress']
                    stage = progress_info['stage']
                    message = progress_info['message']

                    # åªåœ¨è¿›åº¦å˜åŒ–æ—¶æ›´æ–°
                    if abs(current_progress - last_progress) > 0.01:
                        progress_percent = int(current_progress * 100)
                        progress_bar = 'â–ˆ' * (progress_percent // 2) + 'â–‘' * (50 - progress_percent // 2)
                        yield f"\n\n**è®­ç»ƒè¿›åº¦**: {progress_percent}%\n\n`{progress_bar}`\n\n**é˜¶æ®µ**: {stage}\n\n**çŠ¶æ€**: {message}"
                        last_progress = current_progress

                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if stage == 'completed':
                        yield f"\n\nâœ… è®­ç»ƒå®Œæˆï¼è®°å½•ID: {training_id}"
                        break
                    elif stage == 'failed':
                        yield f"\n\nâŒ è®­ç»ƒå¤±è´¥: {message}"
                        break

                # æ£€æŸ¥è®­ç»ƒè®°å½•çŠ¶æ€
                with get_db() as db:
                    record = db.query(TrainingRecord).filter(
                        TrainingRecord.id == training_id
                    ).first()

                    if record:
                        if record.status == 'completed':
                            yield f"\n\nâœ… è®­ç»ƒå®Œæˆï¼\n\n- TokenizeræŸå¤±: {record.train_metrics.get('tokenizer_loss', 'N/A')}\n- PredictoræŸå¤±: {record.train_metrics.get('predictor_loss', 'N/A')}"
                            break
                        elif record.status == 'failed':
                            yield f"\n\nâŒ è®­ç»ƒå¤±è´¥: {record.error_message or 'æœªçŸ¥é”™è¯¯'}"
                            break

                # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æŸ¥è¯¢
                await asyncio.sleep(2)

        except Exception as e:
            logger.error(f"è®­ç»ƒç›‘æ§å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            yield f"\n\nâŒ é”™è¯¯: {str(e)}"

    async def start_websocket_async(self, plan_id: int) -> str:
        """å¯åŠ¨WebSocketï¼ˆå¼‚æ­¥ï¼‰"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨"

                # ä½¿ç”¨å…¨å±€è¿æ¥ç®¡ç†å™¨
                from services.ws_connection_manager import ws_connection_manager

                # è·å–æˆ–åˆ›å»ºWebSocketè¿æ¥
                ws_service = ws_connection_manager.get_or_create_connection(
                    inst_id=plan.inst_id,
                    interval=plan.interval,
                    is_demo=plan.is_demo,
                    ui_callback=None
                )

                if ws_service:
                    # è®¢é˜…Kçº¿äº‹ä»¶
                    from services.kline_event_service import get_kline_event_service
                    get_kline_event_service().subscribe_plan(plan_id)

                    # æ›´æ–°è®¡åˆ’çš„ws_connectedçŠ¶æ€
                    db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                        'ws_connected': True
                    })
                    db.commit()
                    return "âœ… WebSocketå·²å¯åŠ¨ï¼Œå·²è®¢é˜…Kçº¿äº‹ä»¶"
                else:
                    return "âŒ WebSocketå¯åŠ¨å¤±è´¥"

        except Exception as e:
            logger.error(f"å¯åŠ¨WebSocketå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"âŒ å¯åŠ¨å¤±è´¥: {str(e)}"

    async def stop_websocket_async(self, plan_id: int) -> str:
        """åœæ­¢WebSocketï¼ˆå¼‚æ­¥ï¼‰"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨"

                # ä½¿ç”¨å…¨å±€è¿æ¥ç®¡ç†å™¨
                from services.ws_connection_manager import ws_connection_manager

                # åœæ­¢WebSocket (stop_connectionæ˜¯åŒæ­¥æ–¹æ³•)
                ws_connection_manager.stop_connection(
                    inst_id=plan.inst_id,
                    interval=plan.interval,
                    is_demo=plan.is_demo
                )

                # å–æ¶ˆè®¢é˜…Kçº¿äº‹ä»¶
                from services.kline_event_service import get_kline_event_service
                get_kline_event_service().unsubscribe_plan(plan_id)

                # æ›´æ–°è®¡åˆ’çš„ws_connectedçŠ¶æ€
                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'ws_connected': False
                })
                db.commit()
                return "âœ… WebSocketå·²åœæ­¢ï¼Œå·²å–æ¶ˆè®¢é˜…Kçº¿äº‹ä»¶"

        except Exception as e:
            logger.error(f"åœæ­¢WebSocketå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"âŒ åœæ­¢å¤±è´¥: {str(e)}"

    async def start_plan_async(self, plan_id: int) -> str:
        """å¯åŠ¨è®¡åˆ’ï¼ˆå¯åŠ¨å®šæ—¶ä»»åŠ¡ï¼‰"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨"

                # æ›´æ–°è®¡åˆ’çŠ¶æ€
                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'status': 'running'
                })
                db.commit()

                # å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
                from services.schedule_service import ScheduleService
                success = await ScheduleService.start_schedule(plan_id)

                # è®¢é˜…Kçº¿äº‹ä»¶
                from services.kline_event_service import get_kline_event_service
                get_kline_event_service().subscribe_plan(plan_id)

                # å¯åŠ¨è‡ªåŠ¨åŒ–è°ƒåº¦å™¨ï¼ˆå¦‚æœé…ç½®äº†è‡ªåŠ¨åŒ–ï¼‰
                automation_status = ""
                if plan.auto_finetune_enabled or plan.auto_inference_enabled or plan.auto_agent_enabled:  # auto_tool_execution_enabledå·²åºŸå¼ƒ
                    try:
                        from services.automation_service import automation_service
                        automation_service.start_scheduler()
                        automation_status = "\nğŸ¤– è‡ªåŠ¨åŒ–è°ƒåº¦å™¨å·²å¯åŠ¨"
                        logger.info(f"è‡ªåŠ¨åŒ–è°ƒåº¦å™¨å·²å¯åŠ¨: plan_id={plan_id}")
                    except Exception as e:
                        logger.error(f"å¯åŠ¨è‡ªåŠ¨åŒ–è°ƒåº¦å™¨å¤±è´¥: {e}")
                        automation_status = f"\nâš ï¸ è‡ªåŠ¨åŒ–è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {str(e)}"

                # å¯åŠ¨è´¦æˆ·WebSocketè¿æ¥
                if plan.okx_api_key and plan.okx_secret_key and plan.okx_passphrase:
                    from services.account_ws_manager import account_ws_manager
                    account_ws_manager.get_or_create_connection(
                        api_key=plan.okx_api_key,
                        secret_key=plan.okx_secret_key,
                        passphrase=plan.okx_passphrase,
                        is_demo=plan.is_demo,
                        plan_id=plan_id
                    )
                    logger.info(f"è´¦æˆ·WebSocketå·²å¯åŠ¨: plan_id={plan_id}")

                logger.info(f"è®¡åˆ’å·²å¯åŠ¨: plan_id={plan_id}, schedule_success={success}")

                result_msg = "âœ… è®¡åˆ’å·²å¯åŠ¨"
                if success:
                    # è·å–å·²åˆ›å»ºçš„ä»»åŠ¡ä¿¡æ¯
                    jobs = ScheduleService.get_plan_jobs(plan_id)
                    job_info = f"å·²åˆ›å»º {len(jobs)} ä¸ªå®šæ—¶ä»»åŠ¡"
                    result_msg += f"\nâœ… {job_info}"
                else:
                    result_msg += "\nâš ï¸ å®šæ—¶ä»»åŠ¡åˆ›å»ºå¤±è´¥ï¼ˆå¯èƒ½æœªé…ç½®æ—¶é—´è¡¨ï¼‰"

                result_msg += automation_status

                return result_msg

        except Exception as e:
            logger.error(f"å¯åŠ¨è®¡åˆ’å¤±è´¥: {e}")
            return f"âŒ å¯åŠ¨å¤±è´¥: {str(e)}"

    async def stop_plan_async(self, plan_id: int) -> str:
        """åœæ­¢è®¡åˆ’ï¼ˆåœæ­¢å®šæ—¶ä»»åŠ¡ï¼‰"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return "âŒ è®¡åˆ’ä¸å­˜åœ¨"

                # æ›´æ–°è®¡åˆ’çŠ¶æ€
                db.query(TradingPlan).filter(TradingPlan.id == plan_id).update({
                    'status': 'stopped'
                })
                db.commit()

                # åœæ­¢è´¦æˆ·WebSocketè¿æ¥
                if plan.okx_api_key:
                    from services.account_ws_manager import account_ws_manager
                    account_ws_manager.stop_connection(
                        api_key=plan.okx_api_key,
                        is_demo=plan.is_demo,
                        plan_id=plan_id
                    )
                    logger.info(f"è´¦æˆ·WebSocketå·²åœæ­¢: plan_id={plan_id}")

                # å–æ¶ˆè®¢é˜…Kçº¿äº‹ä»¶
                from services.kline_event_service import get_kline_event_service
                get_kline_event_service().unsubscribe_plan(plan_id)

                # åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
                from services.schedule_service import ScheduleService
                success = await ScheduleService.stop_schedule(plan_id)

                logger.info(f"è®¡åˆ’å·²åœæ­¢: plan_id={plan_id}, schedule_success={success}")
                return "âœ… è®¡åˆ’å·²åœæ­¢\nâœ… æ‰€æœ‰å®šæ—¶ä»»åŠ¡å·²ç§»é™¤"

        except Exception as e:
            logger.error(f"åœæ­¢è®¡åˆ’å¤±è´¥: {e}")
            return f"âŒ åœæ­¢å¤±è´¥: {str(e)}"

    def get_account_info(self, plan_id: int) -> str:
        """è·å–è´¦æˆ·ä¿¡æ¯ï¼ˆMarkdownæ ¼å¼ï¼‰"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan or not plan.okx_api_key:
                    return "### ğŸ’° è´¦æˆ·ä¿¡æ¯\n\næœªé…ç½®OKX API Key"

                from services.account_ws_manager import account_ws_manager

                # è·å–è¿æ¥çŠ¶æ€
                status = account_ws_manager.get_connection_status(
                    api_key=plan.okx_api_key,
                    is_demo=plan.is_demo
                )

                if not status['connected']:
                    return f"### ğŸ’° è´¦æˆ·ä¿¡æ¯\n\nâšª æœªè¿æ¥\n\n{'æ¨¡æ‹Ÿç›˜' if plan.is_demo else 'çœŸå®ç›˜'}"

                # è·å–è´¦æˆ·æ•°æ®
                account_info = account_ws_manager.get_account_info(
                    api_key=plan.okx_api_key,
                    is_demo=plan.is_demo
                )

                if not account_info:
                    return "### ğŸ’° è´¦æˆ·ä¿¡æ¯\n\nâšª æš‚æ— æ•°æ®"

                balances = account_info.get('balances', {})
                positions = account_info.get('positions', [])
                last_update = account_info.get('last_update')

                # æ„å»ºMarkdown
                lines = ["### ğŸ’° è´¦æˆ·ä¿¡æ¯\n"]
                lines.append(f"**ç¯å¢ƒ**: {'ğŸ§ª æ¨¡æ‹Ÿç›˜' if plan.is_demo else 'ğŸ’° çœŸå®ç›˜'}")
                lines.append(f"**çŠ¶æ€**: ğŸŸ¢ å·²è¿æ¥")

                if last_update:
                    lines.append(f"**æ›´æ–°æ—¶é—´**: {format_datetime_beijing(last_update, '%H:%M:%S')}")

                lines.append("\n---\n")

                # ä½™é¢ä¿¡æ¯
                if balances:
                    lines.append("**è´¦æˆ·ä½™é¢**:\n")
                    for ccy, data in balances.items():
                        available = data.get('available', 0)
                        balance = data.get('balance', 0)
                        equity = data.get('equity', 0)
                        lines.append(f"- **{ccy}**: å¯ç”¨ {available:.4f} | ä½™é¢ {balance:.4f} | æƒç›Š {equity:.4f}")
                else:
                    lines.append("**è´¦æˆ·ä½™é¢**: æš‚æ— æ•°æ®")

                lines.append("\n---\n")

                # æŒä»“ä¿¡æ¯
                if positions:
                    lines.append(f"**æŒä»“** ({len(positions)}ä¸ª):\n")
                    for pos in positions[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        inst_id = pos.get('inst_id', 'N/A')
                        pos_qty = pos.get('pos', 0)
                        avg_price = pos.get('avg_price', 0)
                        upl = pos.get('upl', 0)
                        upl_ratio = pos.get('upl_ratio', 0)

                        upl_emoji = 'ğŸ“ˆ' if upl >= 0 else 'ğŸ“‰'
                        lines.append(
                            f"- {upl_emoji} **{inst_id}**: {pos_qty} @ {avg_price:.4f} | "
                            f"ç›ˆäº {upl:+.2f} ({upl_ratio:+.2%})"
                        )
                else:
                    lines.append("**æŒä»“**: æ— ")

                return "\n".join(lines)

        except Exception as e:
            logger.error(f"è·å–è´¦æˆ·ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"### ğŸ’° è´¦æˆ·ä¿¡æ¯\n\nâŒ è·å–å¤±è´¥: {str(e)}"

    def get_orders_info(self, plan_id: int) -> pd.DataFrame:
        """è·å–è®¢å•è®°å½•ï¼ˆä»…æ˜¾ç¤ºAgentæ“ä½œçš„è®¢å•ï¼‰"""
        try:
            with get_db() as db:
                # ä»æ•°æ®åº“è·å–ä»…Agentæ“ä½œçš„è®¢å•
                from database.models import TradeOrder

                orders = db.query(TradeOrder).filter(
                    TradeOrder.plan_id == plan_id,
                    TradeOrder.is_from_agent == True
                ).order_by(TradeOrder.created_at.desc()).limit(50).all()

                if not orders:
                    return pd.DataFrame()

                # æ„å»ºDataFrame
                df_data = []
                for order in orders:
                    side_emoji = 'ğŸŸ¢' if order.side == 'buy' else 'ğŸ”´'
                    state_map = {
                        'live': 'â³ æœªæˆäº¤',
                        'partially_filled': 'â¸ï¸ éƒ¨åˆ†æˆäº¤',
                        'filled': 'âœ… å®Œå…¨æˆäº¤',
                        'canceled': 'âŒ å·²å–æ¶ˆ',
                        'mmp_canceled': 'âŒ MMPå–æ¶ˆ',
                        'failed': 'âŒ å¤±è´¥'
                    }
                    state_emoji = state_map.get(order.status, f"â“ {order.status}")

                    # è½¬æ¢æ—¶é—´æˆ³
                    create_time = format_datetime_beijing(order.created_at, '%m-%d %H:%M:%S')
                    update_time = format_datetime_beijing(order.updated_at, '%m-%d %H:%M:%S')

                    df_data.append({
                        'è®¢å•ID': order.order_id[:10] + '...' if order.order_id else 'æœ¬åœ°è®¢å•',
                        'äº¤æ˜“å¯¹': order.inst_id,
                        'æ–¹å‘': f"{side_emoji} {order.side}",
                        'ç±»å‹': order.order_type,
                        'ä»·æ ¼': f"{float(order.price):.4f}" if order.price else 'å¸‚ä»·',
                        'æ•°é‡': f"{float(order.size):.4f}",
                        'å·²æˆäº¤': f"{float(order.filled_size):.4f}",
                        'çŠ¶æ€': state_emoji,
                        'åˆ›å»ºæ—¶é—´': create_time,
                        'æ›´æ–°æ—¶é—´': update_time
                    })

                return safe_dataframe_from_data(df_data)

        except Exception as e:
            logger.error(f"è·å–è®¢å•ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()

    def clear_agent_records(self, plan_id: int) -> str:
        """æ¸…é™¤AI Agentå¯¹è¯è®°å½•"""
        try:
            if not plan_id:
                return "âŒ è¯·å…ˆé€‰æ‹©è®¡åˆ’"

            with get_db() as db:
                # å…ˆè·å–è¦åˆ é™¤çš„å¯¹è¯ä¼šè¯æ•°é‡
                conversations = db.query(AgentConversation).filter(
                    AgentConversation.plan_id == plan_id
                ).all()

                if not conversations:
                    return "âœ… æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…é™¤çš„å¯¹è¯è®°å½•"

                # è·å–ä¼šè¯IDåˆ—è¡¨
                conversation_ids = [conv.id for conv in conversations]

                # åˆ é™¤æ‰€æœ‰ç›¸å…³çš„æ¶ˆæ¯è®°å½•
                deleted_messages = db.query(AgentMessage).filter(
                    AgentMessage.conversation_id.in_(conversation_ids)
                ).delete(synchronize_session=False)

                # åˆ é™¤å¯¹è¯ä¼šè¯è®°å½•
                deleted_conversations = db.query(AgentConversation).filter(
                    AgentConversation.plan_id == plan_id
                ).delete()

                db.commit()

                logger.info(f"æ¸…é™¤è®¡åˆ’ {plan_id} çš„ {deleted_conversations} ä¸ªå¯¹è¯ä¼šè¯å’Œ {deleted_messages} æ¡æ¶ˆæ¯è®°å½•")

                return f"âœ… å·²æ¸…é™¤ {deleted_conversations} ä¸ªå¯¹è¯ä¼šè¯å’Œ {deleted_messages} æ¡æ¶ˆæ¯è®°å½•"

        except Exception as e:
            logger.error(f"æ¸…é™¤å¯¹è¯è®°å½•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"âŒ æ¸…é™¤å¤±è´¥: {str(e)}"

    def load_task_executions(self, plan_id: int) -> pd.DataFrame:
        """åŠ è½½ä»»åŠ¡æ‰§è¡Œè®°å½•"""
        try:
            from services.scheduler_service import scheduler_service

            # è·å–ä»»åŠ¡å†å²
            task_history = scheduler_service.get_task_history(plan_id, limit=100)

            if not task_history:
                return pd.DataFrame(columns=[
                    'ID', 'ä»»åŠ¡ç±»å‹', 'ä»»åŠ¡åç§°', 'çŠ¶æ€', 'è®¡åˆ’æ—¶é—´', 'å¼€å§‹æ—¶é—´',
                    'å®Œæˆæ—¶é—´', 'æ‰§è¡Œæ—¶é•¿(ç§’)', 'è§¦å‘æ–¹å¼', 'è¿›åº¦(%)'
                ]).astype({
                    'ID': 'int',
                    'æ‰§è¡Œæ—¶é•¿(ç§’)': 'int',
                    'è¿›åº¦(%)': 'int'
                })

            # æ„å»ºDataFrame
            df_data = []
            for task in task_history:
                df_data.append({
                    'ID': task['id'],
                    'ä»»åŠ¡ç±»å‹': task['type_display'],
                    'ä»»åŠ¡åç§°': task['task_name'],
                    'çŠ¶æ€': task['status_display'],
                    'è®¡åˆ’æ—¶é—´': task['scheduled_time'] or '',
                    'å¼€å§‹æ—¶é—´': task['started_at'] or '',
                    'å®Œæˆæ—¶é—´': task['completed_at'] or '',
                    'æ‰§è¡Œæ—¶é•¿(ç§’)': task['duration_seconds'] or 0,  # ç¡®ä¿æ•°å­—ç±»å‹
                    'è§¦å‘æ–¹å¼': task['trigger_type'],
                    'è¿›åº¦(%)': task['progress_percentage'] or 0  # ç¡®ä¿æ•°å­—ç±»å‹
                })

            # ä½¿ç”¨å®‰å…¨çš„DataFrameåˆ›å»ºå‡½æ•°
            return safe_dataframe_from_data(df_data)

        except Exception as e:
            logger.error(f"åŠ è½½ä»»åŠ¡æ‰§è¡Œè®°å½•å¤±è´¥: {e}")
            return pd.DataFrame(columns=[
                    'ID', 'ä»»åŠ¡ç±»å‹', 'ä»»åŠ¡åç§°', 'çŠ¶æ€', 'è®¡åˆ’æ—¶é—´', 'å¼€å§‹æ—¶é—´',
                    'å®Œæˆæ—¶é—´', 'æ‰§è¡Œæ—¶é•¿(ç§’)', 'è§¦å‘æ–¹å¼', 'è¿›åº¦(%)'
                ]).astype({
                    'ID': 'int',
                    'æ‰§è¡Œæ—¶é•¿(ç§’)': 'int',
                    'è¿›åº¦(%)': 'int'
                })
