"""
AI Agentæµå¼å¯¹è¯æœåŠ¡
ä¸“é—¨ç”¨äºGradio chatbotçš„æµå¼æ¨ç†ï¼Œæ”¯æŒthinkingå’Œå·¥å…·è°ƒç”¨
"""
import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, AsyncGenerator
from database.db import get_db
from database.models import TradingPlan, TrainingRecord, PredictionData, LLMConfig
from utils.logger import setup_logger
from services.agent_tools import get_all_tools
import gradio as gr

logger = setup_logger(__name__, "agent_stream_service.log")


class AgentStreamService:
    """AI Agentæµå¼å¯¹è¯æœåŠ¡"""

    @classmethod
    async def chat_with_tools_stream(
        cls,
        message: str,
        history: List[Dict],
        plan_id: int,
        training_record_id: Optional[int] = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼å¯¹è¯ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œthinking

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            history: å¯¹è¯å†å²
            plan_id: äº¤æ˜“è®¡åˆ’ID
            training_record_id: è®­ç»ƒè®°å½•ID

        Yields:
            æµå¼å“åº”å­—ç¬¦ä¸²ï¼ˆJSONæ ¼å¼ï¼‰
        """
        try:
            # è·å–è®¡åˆ’ä¿¡æ¯
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    yield json.dumps({"type": "error", "content": "âŒ è®¡åˆ’ä¸å­˜åœ¨"})
                    return

                # è·å–LLMé…ç½®
                llm_config = None
                if plan.llm_config_id:
                    llm_config = db.query(LLMConfig).filter(
                        LLMConfig.id == plan.llm_config_id
                    ).first()

                if not llm_config:
                    yield json.dumps({"type": "error", "content": "âŒ æœªé…ç½®LLM"})
                    return

                # è·å–è®­ç»ƒè®°å½•å’Œé¢„æµ‹æ•°æ®
                training_record = None
                prediction_data = []

                if training_record_id:
                    training_record = db.query(TrainingRecord).filter(
                        TrainingRecord.id == training_record_id
                    ).first()
                else:
                    # è·å–æœ€æ–°çš„è®­ç»ƒè®°å½•
                    training_record = db.query(TrainingRecord).filter(
                        TrainingRecord.plan_id == plan_id,
                        TrainingRecord.status == 'completed',
                        TrainingRecord.is_active == True
                    ).order_by(TrainingRecord.created_at.desc()).first()

                if training_record:
                    prediction_data = db.query(PredictionData).filter(
                        PredictionData.training_record_id == training_record.id
                    ).order_by(PredictionData.timestamp.desc()).limit(10).all()

            # æ„å»ºç³»ç»Ÿæ¶ˆæ¯
            system_prompt = cls._build_system_prompt(plan, training_record, prediction_data)

            # æ„å»ºå¯¹è¯å†å²
            messages = []

            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
            messages.append({"role": "system", "content": system_prompt})

            # æ·»åŠ å†å²å¯¹è¯
            for msg in history:
                if isinstance(msg, dict):
                    messages.append({
                        "role": msg.get("role", "user"),
                        "content": msg.get("content", "")
                    })

            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": message})

            # è·å–å¯ç”¨å·¥å…·
            tools_config = plan.agent_tools_config or {}
            available_tools = {}
            enabled_tools = []

            for tool_name, tool_obj in get_all_tools().items():
                if tools_config.get(tool_name, False):
                    available_tools[tool_name] = tool_obj
                    # è½¬æ¢ä¸ºOpenAIå·¥å…·æ ¼å¼
                    enabled_tools.append({
                        "type": "function",
                        "function": {
                            "name": tool_name,
                            "description": tool_obj.description,
                            "parameters": tool_obj.parameters
                        }
                    })

            # å‘é€å¼€å§‹æ€è€ƒæ¶ˆæ¯
            yield json.dumps({
                "type": "thinking_start",
                "content": "ğŸ§  æ­£åœ¨æ€è€ƒ..."
            })

            # è°ƒç”¨LLMè¿›è¡Œæµå¼æ¨ç†
            if llm_config.provider == 'qwen':
                async for chunk in cls._stream_qwen_response(
                    llm_config, messages, enabled_tools, available_tools, plan_id
                ):
                    yield chunk
            elif llm_config.provider == 'openai':
                async for chunk in cls._stream_openai_response(
                    llm_config, messages, enabled_tools, available_tools, plan_id
                ):
                    yield chunk
            elif llm_config.provider == 'anthropic':
                async for chunk in cls._stream_claude_response(
                    llm_config, messages, enabled_tools, available_tools, plan_id
                ):
                    yield chunk
            else:
                yield json.dumps({
                    "type": "error",
                    "content": f"âŒ ä¸æ”¯æŒçš„LLMæä¾›å•†: {llm_config.provider}"
                })

        except Exception as e:
            logger.error(f"æµå¼å¯¹è¯å¤±è´¥: {e}")
            yield json.dumps({"type": "error", "content": f"âŒ å¯¹è¯å¤±è´¥: {str(e)}"})

    @classmethod
    def _build_system_prompt(
        cls,
        plan: TradingPlan,
        training_record: Optional[TrainingRecord],
        prediction_data: List[PredictionData]
    ) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""

        # åŸºç¡€ç³»ç»Ÿæç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“AIåŠ©æ‰‹ï¼Œè´Ÿè´£åˆ†æå¸‚åœºæ•°æ®å¹¶åšå‡ºäº¤æ˜“å†³ç­–ã€‚

**äº¤æ˜“è®¡åˆ’ä¿¡æ¯**:
- äº¤æ˜“å¯¹: {plan.inst_id}
- æ—¶é—´å‘¨æœŸ: {plan.interval}
- ç¯å¢ƒ: {'ğŸ§ª æ¨¡æ‹Ÿç›˜' if plan.is_demo else 'ğŸ’° å®ç›˜'}
- è®¡åˆ’çŠ¶æ€: {plan.status}

**æ¨ç†ä»»åŠ¡**:
åŸºäºKronosæ¨¡å‹çš„é¢„æµ‹æ•°æ®ï¼Œä½¿ç”¨ReActæ¨¡å¼è¿›è¡Œæ€è€ƒå’Œå†³ç­–ï¼š
1. **æ€è€ƒ** (Thought): åˆ†æå¸‚åœºçŠ¶å†µå’Œé¢„æµ‹æ•°æ®
2. **è¡ŒåŠ¨** (Action): è°ƒç”¨å·¥å…·è·å–æ›´å¤šä¿¡æ¯æˆ–æ‰§è¡Œäº¤æ˜“
3. **è§‚å¯Ÿ** (Observation): åˆ†æå·¥å…·è¿”å›çš„ç»“æœ
4. **é‡å¤** ç›´åˆ°å¾—å‡ºæœ€ç»ˆç»“è®º

**å¯ç”¨å·¥å…·**:
ä½ å¯ä»¥è°ƒç”¨ä»¥ä¸‹å·¥å…·æ¥è·å–ä¿¡æ¯å’Œæ‰§è¡Œæ“ä½œï¼š
"""

        # æ·»åŠ å·¥å…·è¯´æ˜
        tools_config = plan.agent_tools_config or {}
        for tool_name, tool_obj in get_all_tools().items():
            if tools_config.get(tool_name, False):
                description = tool_obj.description
                system_prompt += f"- {tool_name}: {description}\n"

        # æ·»åŠ é¢„æµ‹æ•°æ®
        if prediction_data:
            latest_prediction = prediction_data[0]  # æœ€æ–°çš„é¢„æµ‹æ•°æ®

            # å®‰å…¨å¤„ç†æ•°æ®
            current_price = latest_prediction.close or 0
            upward_prob = latest_prediction.upward_probability or 0
            volatility_prob = latest_prediction.volatility_amplification_probability or 0
            close_min = latest_prediction.close_min or 0
            close_max = latest_prediction.close_max or 0

            prediction_info = f"""

**æœ€æ–°é¢„æµ‹æ•°æ®**:
- å½“å‰ä»·æ ¼: ${current_price:.4f}
- é¢„æµ‹åŒºé—´: ${close_min:.4f} ~ ${close_max:.4f}
- ä¸Šæ¶¨æ¦‚ç‡: {upward_prob:.2%}
- æ³¢åŠ¨æ”¾å¤§æ¦‚ç‡: {volatility_prob:.2%}
- é¢„æµ‹æ—¶é—´: {latest_prediction.timestamp.strftime('%Y-%m-%d %H:%M:%S')}
- æ¨¡å‹ç‰ˆæœ¬: {training_record.version if training_record else 'N/A'}
"""

            system_prompt += prediction_info

        # æ·»åŠ è‡ªå®šä¹‰æç¤ºè¯
        if plan.agent_prompt:
            system_prompt += f"""

**é¢å¤–æŒ‡ç¤º**:
{plan.agent_prompt}
"""

        system_prompt += """

**é‡è¦æé†’**:
- å§‹ç»ˆè°¨æ…å†³ç­–ï¼Œæ§åˆ¶é£é™©
- åœ¨æ¨¡æ‹Ÿç›˜ç¯å¢ƒä¸­å¯ä»¥å¤§èƒ†å°è¯•ç­–ç•¥
- æ‰€æœ‰äº¤æ˜“æ“ä½œéƒ½ä¼šè¢«è®°å½•ç”¨äºåˆ†æ
- ä½¿ç”¨é™ä»·å•è€Œéå¸‚ä»·å•ä»¥é¿å…ä»·æ ¼æ»‘ç‚¹

ç°åœ¨è¯·åŸºäºä»¥ä¸Šä¿¡æ¯è¿›è¡Œåˆ†æå’Œæ¨ç†ã€‚"""

        return system_prompt

    @classmethod
    async def _stream_qwen_response(
        cls,
        llm_config: LLMConfig,
        messages: List[Dict],
        tools: List[Dict],
        available_tools: Dict,
        plan_id: int
    ) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨é€šä¹‰åƒé—®"""
        try:
            import openai

            client = openai.AsyncOpenAI(
                api_key=llm_config.api_key,
                base_url=llm_config.api_base_url
            )

            response = await client.chat.completions.create(
                model=llm_config.model_name,
                messages=messages,
                tools=tools if tools else None,
                tool_choice="auto" if tools else None,
                temperature=llm_config.temperature,
                max_tokens=llm_config.max_tokens,
                stream=True
            )

            thinking_content = ""
            content = ""
            current_tool_call = None

            async for chunk in response:
                delta = chunk.choices[0].delta

                # å¤„ç†thinkingï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                    thinking_content += delta.reasoning_content
                    yield json.dumps({
                        "type": "thinking",
                        "content": thinking_content
                    })

                # å¤„ç†æ­£å¸¸å†…å®¹
                if delta.content:
                    content += delta.content
                    yield json.dumps({
                        "type": "content",
                        "content": content
                    })

                # å¤„ç†å·¥å…·è°ƒç”¨
                if delta.tool_calls:
                    for tool_call in delta.tool_calls:
                        if not current_tool_call or current_tool_call.get("id") != tool_call.id:
                            if current_tool_call:
                                # æ‰§è¡Œå‰ä¸€ä¸ªå·¥å…·è°ƒç”¨
                                async for chunk in cls._execute_and_stream_tool(
                                    current_tool_call, available_tools, plan_id
                                ):
                                    yield chunk

                            current_tool_call = {
                                "id": tool_call.id,
                                "name": tool_call.function.name,
                                "arguments": tool_call.function.arguments or ""
                            }

                            yield json.dumps({
                                "type": "tool_call_start",
                                "tool_name": tool_call.function.name,
                                "tool_id": tool_call.id
                            })
                        else:
                            current_tool_call["arguments"] += tool_call.function.arguments or ""

            # æ‰§è¡Œæœ€åçš„å·¥å…·è°ƒç”¨
            if current_tool_call:
                async for chunk in cls._execute_and_stream_tool(current_tool_call, available_tools, plan_id):
                    yield chunk

        except Exception as e:
            logger.error(f"Qwenæµå¼è°ƒç”¨å¤±è´¥: {e}")
            yield json.dumps({"type": "error", "content": f"âŒ è°ƒç”¨å¤±è´¥: {str(e)}"})

    @classmethod
    async def _stream_openai_response(
        cls,
        llm_config: LLMConfig,
        messages: List[Dict],
        tools: List[Dict],
        available_tools: Dict,
        plan_id: int
    ) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨OpenAI"""
        try:
            import openai

            client = openai.AsyncOpenAI(api_key=llm_config.api_key)

            response = await client.chat.completions.create(
                model=llm_config.model_name,
                messages=messages,
                tools=tools if tools else None,
                tool_choice="auto" if tools else None,
                temperature=llm_config.temperature,
                max_tokens=llm_config.max_tokens,
                stream=True
            )

            content = ""
            current_tool_call = None

            async for chunk in response:
                delta = chunk.choices[0].delta

                # å¤„ç†æ­£å¸¸å†…å®¹
                if delta.content:
                    content += delta.content
                    yield json.dumps({
                        "type": "content",
                        "content": content
                    })

                # å¤„ç†å·¥å…·è°ƒç”¨
                if delta.tool_calls:
                    for tool_call in delta.tool_calls:
                        if not current_tool_call or current_tool_call.get("index") != tool_call.index:
                            if current_tool_call:
                                async for chunk in cls._execute_and_stream_tool(
                                    current_tool_call, available_tools, plan_id
                                ):
                                    yield chunk

                            current_tool_call = {
                                "id": tool_call.id,
                                "name": tool_call.function.name,
                                "arguments": tool_call.function.arguments or "",
                                "index": tool_call.index
                            }

                            yield json.dumps({
                                "type": "tool_call_start",
                                "tool_name": tool_call.function.name,
                                "tool_id": tool_call.id
                            })
                        else:
                            current_tool_call["arguments"] += tool_call.function.arguments or ""

            # æ‰§è¡Œæœ€åçš„å·¥å…·è°ƒç”¨
            if current_tool_call:
                async for chunk in cls._execute_and_stream_tool(current_tool_call, available_tools, plan_id):
                    yield chunk

        except Exception as e:
            logger.error(f"OpenAIæµå¼è°ƒç”¨å¤±è´¥: {e}")
            yield json.dumps({"type": "error", "content": f"âŒ è°ƒç”¨å¤±è´¥: {str(e)}"})

    @classmethod
    async def _stream_claude_response(
        cls,
        llm_config: LLMConfig,
        messages: List[Dict],
        tools: List[Dict],
        available_tools: Dict,
        plan_id: int
    ) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨Claude"""
        try:
            import anthropic

            client = anthropic.AsyncAnthropic(api_key=llm_config.api_key)

            # è¿‡æ»¤å‡ºéç³»ç»Ÿæ¶ˆæ¯
            non_system_messages = [msg for msg in messages if msg["role"] != "system"]
            system_content = " ".join([msg["content"] for msg in messages if msg["role"] == "system"])

            response = await client.messages.create(
                model=llm_config.model_name,
                max_tokens=llm_config.max_tokens,
                temperature=llm_config.temperature,
                system=system_content,
                messages=non_system_messages,
                tools=tools if tools else None,
                stream=True
            )

            content = ""
            current_tool_call = None

            async for chunk in response:
                if chunk.type == "content_block_delta":
                    if chunk.delta.type == "text_delta":
                        content += chunk.delta.text
                        yield json.dumps({
                            "type": "content",
                            "content": content
                        })

                elif chunk.type == "content_block_start":
                    if hasattr(chunk, 'content_block') and chunk.content_block.type == "tool_use":
                        tool_block = chunk.content_block
                        current_tool_call = {
                            "id": tool_block.id,
                            "name": tool_block.name,
                            "arguments": ""
                        }

                        yield json.dumps({
                            "type": "tool_call_start",
                            "tool_name": tool_block.name,
                            "tool_id": tool_block.id
                        })

                elif chunk.type == "content_block_delta" and current_tool_call:
                    if hasattr(chunk.delta, 'partial_json'):
                        current_tool_call["arguments"] += chunk.delta.partial_json

                elif chunk.type == "content_block_stop" and current_tool_call:
                    async for chunk in cls._execute_and_stream_tool(current_tool_call, available_tools, plan_id):
                        yield chunk
                    current_tool_call = None

        except Exception as e:
            logger.error(f"Claudeæµå¼è°ƒç”¨å¤±è´¥: {e}")
            yield json.dumps({"type": "error", "content": f"âŒ è°ƒç”¨å¤±è´¥: {str(e)}"})

    @classmethod
    async def _execute_and_stream_tool(
        cls,
        tool_call: Dict,
        available_tools: Dict,
        plan_id: int
    ) -> AsyncGenerator[str, None]:
        """æ‰§è¡Œå·¥å…·å¹¶æµå¼è¿”å›ç»“æœ"""
        try:
            # ç¡®ä¿tool_nameæ˜¯å­—ç¬¦ä¸²
            tool_name = str(tool_call.get("name", ""))
            if not tool_name:
                yield json.dumps({
                    "type": "tool_error",
                    "tool_name": "unknown",
                    "tool_id": tool_call.get("id", "unknown"),
                    "error": "å·¥å…·åç§°ä¸ºç©º"
                })
                return

            tool_id = tool_call.get("id", "")
            arguments_str = tool_call.get("arguments", "")

            # è§£æå‚æ•°
            try:
                arguments = json.loads(arguments_str) if arguments_str else {}
            except json.JSONDecodeError:
                arguments = {}
                yield json.dumps({
                    "type": "tool_error",
                    "tool_name": tool_name,
                    "tool_id": tool_id,
                    "error": f"å·¥å…·å‚æ•°è§£æå¤±è´¥: {arguments_str}"
                })

            yield json.dumps({
                "type": "tool_call_arguments",
                "tool_name": tool_name,
                "tool_id": tool_id,
                "arguments": arguments
            })

            # æ‰§è¡Œå·¥å…·
            from services.trading_tools import OKXTradingTools
            from database.models import TradingPlan
            from database.db import get_db

            # è·å–è®¡åˆ’ä¿¡æ¯ä»¥å¾—åˆ°APIå‡­æ®
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    yield json.dumps({
                        "type": "tool_error",
                        "tool_name": tool_name,
                        "tool_id": tool_id,
                        "error": "è®¡åˆ’ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡Œå·¥å…·"
                    })
                    return

                trading_tools = OKXTradingTools(
                    api_key=plan.okx_api_key,
                    secret_key=plan.okx_secret_key,
                    passphrase=plan.okx_passphrase,
                    is_demo=plan.is_demo,
                    trading_limits=plan.trading_limits
                )
            tool_func = getattr(trading_tools, tool_name, None)

            if tool_func and callable(tool_func):
                # è¿‡æ»¤å‚æ•°ï¼Œåªä¼ é€’å·¥å…·å‡½æ•°æœŸæœ›çš„å‚æ•°
                import inspect
                try:
                    sig = inspect.signature(tool_func)
                    valid_params = {}
                    for param_name, param in sig.parameters.items():
                        if param_name in arguments:
                            valid_params[param_name] = arguments[param_name]
                        elif param.default == inspect.Parameter.empty and param.kind != inspect.Parameter.VAR_KEYWORD:
                            # å¿…éœ€å‚æ•°ç¼ºå¤±
                            valid_params[param_name] = None

                    # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥å·¥å…·
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(None, lambda: tool_func(**valid_params))
                except TypeError as te:
                    logger.error(f"å·¥å…·å‚æ•°é”™è¯¯: {te}")
                    result = {"success": False, "error": f"å·¥å…·å‚æ•°é”™è¯¯: {str(te)}"}
                except Exception as exec_error:
                    logger.error(f"å·¥å…·æ‰§è¡Œé”™è¯¯: {exec_error}")
                    result = {"success": False, "error": f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(exec_error)}"}
            else:
                result = {"success": False, "error": f"å·¥å…· '{tool_name}' ä¸å­˜åœ¨æˆ–ä¸å¯è°ƒç”¨"}

            yield json.dumps({
                "type": "tool_result",
                "tool_name": tool_name,
                "tool_id": tool_id,
                "result": result
            })

        except Exception as e:
            logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
            yield json.dumps({
                "type": "tool_error",
                "tool_name": tool_call.get("name", "unknown"),
                "tool_id": tool_call.get("id", "unknown"),
                "error": str(e)
            })

  