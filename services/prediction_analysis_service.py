"""
é¢„æµ‹åˆ†ææœåŠ¡

æä¾›å¤šæ‰¹æ¬¡é¢„æµ‹æ•°æ®çš„åˆ†æå’Œç»Ÿè®¡åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- è·å–æœ€æ–°æ‰¹æ¬¡é¢„æµ‹å‡å€¼æ•°æ®
- è®¡ç®—æå€¼é¢„æµ‹å’Œæ—¶é—´èŒƒå›´
- åˆ†æå¤šæ‰¹æ¬¡é¢„æµ‹çš„é‡å åº¦å’Œå…±è¯†åº¦
"""

from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Optional, Tuple
import numpy as np

from database.models import PredictionData, TrainingRecord, KlineData, TradingPlan
from database.db import SessionLocal, get_db
from sqlalchemy import and_, func
from utils.logger import setup_logger

logger = setup_logger(__name__, "prediction_analysis.log")


class PredictionAnalysisService:
    """é¢„æµ‹åˆ†ææœåŠ¡ç±»"""

    @staticmethod
    def get_latest_training_record(plan_id: int) -> Optional[TrainingRecord]:
        """è·å–æŒ‡å®šè®¡åˆ’çš„æœ€æ–°å·²å®Œæˆè®­ç»ƒè®°å½•"""
        try:
            with get_db() as db:
                training_record = db.query(TrainingRecord).filter(
                    TrainingRecord.plan_id == plan_id,
                    TrainingRecord.status == 'completed'
                ).order_by(TrainingRecord.created_at.desc()).first()
                return training_record
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°è®­ç»ƒè®°å½•å¤±è´¥: {e}")
            return None

    @staticmethod
    def get_latest_kline_time(plan_id: int) -> Optional[datetime]:
        """è·å–æŒ‡å®šè®¡åˆ’çš„å½“å‰æœ€æ–°Kçº¿æ—¶é—´"""
        try:
            with get_db() as db:
                plan = db.query(TradingPlan).filter(TradingPlan.id == plan_id).first()
                if not plan:
                    return None

                latest_kline = db.query(KlineData).filter(
                    KlineData.inst_id == plan.inst_id,
                    KlineData.interval == plan.interval
                ).order_by(KlineData.timestamp.desc()).first()

                return latest_kline.timestamp if latest_kline else None
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°Kçº¿æ—¶é—´å¤±è´¥: {e}")
            return None

    @staticmethod
    def get_future_prediction_data(training_id: int, plan_id: int) -> List[PredictionData]:
        """è·å–è®­ç»ƒè®°å½•ä¸­åªåŒ…å«æœªæ¥æ—¶é—´çš„é¢„æµ‹æ•°æ®"""
        try:
            with get_db() as db:
                # è·å–æœ€æ–°Kçº¿æ—¶é—´ä½œä¸º"ç°åœ¨"çš„åŸºå‡†
                latest_kline_time = PredictionAnalysisService.get_latest_kline_time(plan_id)

                query = db.query(PredictionData).filter(
                    PredictionData.training_record_id == training_id
                )

                # åªè·å–æœªæ¥é¢„æµ‹æ•°æ®ï¼ˆæ™šäºæœ€æ–°Kçº¿æ—¶é—´çš„æ•°æ®ï¼‰
                if latest_kline_time:
                    query = query.filter(PredictionData.timestamp > latest_kline_time)

                predictions = query.order_by(PredictionData.timestamp).all()
                logger.info(f"è·å–åˆ° {len(predictions)} æ¡æœªæ¥é¢„æµ‹æ•°æ®ï¼ŒåŸºå‡†æ—¶é—´: {latest_kline_time}")
                return predictions

        except Exception as e:
            logger.error(f"è·å–æœªæ¥é¢„æµ‹æ•°æ®å¤±è´¥: {e}")
            return []

    @staticmethod
    def analyze_batch_predictions(predictions: List[PredictionData]) -> Dict:
        """åˆ†æå¤šæ‰¹æ¬¡é¢„æµ‹æ•°æ®ï¼Œè®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
        if not predictions:
            return {}

        # æŒ‰æ—¶é—´ç‚¹åˆ†ç»„æ•°æ®
        time_stats = defaultdict(lambda: {
            'highs': [], 'lows': [], 'closes': [], 'volumes': [],
            'upward_probs': [], 'volatility_probs': [], 'close_stds': [],
            'batch_ids': []
        })

        for pred in predictions:
            t = pred.timestamp
            time_stats[t]['highs'].append(pred.high)
            time_stats[t]['lows'].append(pred.low)
            time_stats[t]['closes'].append(pred.close)
            time_stats[t]['volumes'].append(pred.volume or 0)
            time_stats[t]['upward_probs'].append(pred.upward_probability or 0)
            time_stats[t]['volatility_probs'].append(pred.volatility_amplification_probability or 0)
            time_stats[t]['close_stds'].append(pred.close_std or 0)
            time_stats[t]['batch_ids'].append(pred.inference_batch_id)

        # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„ç»Ÿè®¡æŒ‡æ ‡
        results = []
        for timestamp, stats in sorted(time_stats.items()):
            result = {
                'timestamp': timestamp,
                'sample_count': len(stats['highs']),
                'high_mean': np.mean(stats['highs']),
                'high_max': np.max(stats['highs']),
                'high_min': np.min(stats['highs']),
                'high_std': np.std(stats['highs']),
                'low_mean': np.mean(stats['lows']),
                'low_max': np.max(stats['lows']),
                'low_min': np.min(stats['lows']),
                'low_std': np.std(stats['lows']),
                'close_mean': np.mean(stats['closes']),
                'close_max': np.max(stats['closes']),
                'close_min': np.min(stats['closes']),
                'close_std_mean': np.mean(stats['close_stds']),
                'volume_mean': np.mean(stats['volumes']),
                'upward_prob_mean': np.mean(stats['upward_probs']),
                'volatility_prob_mean': np.mean(stats['volatility_probs']),
                'consensus_score': 1 - (np.std(stats['closes']) / np.mean(stats['closes'])) if np.mean(stats['closes']) > 0 else 0,
                'batch_count': len(set(stats['batch_ids']))  # ä¸åŒæ‰¹æ¬¡æ•°
            }
            results.append(result)

        return results

    @staticmethod
    def find_extreme_predictions(stats_results: List[Dict]) -> Optional[Dict]:
        """ä»ç»Ÿè®¡ç»“æœä¸­æ‰¾åˆ°æå€¼é¢„æµ‹"""
        if not stats_results:
            return None

        # æ‰¾åˆ°æœ€é«˜ç‚¹å’Œæœ€ä½ç‚¹
        max_high_point = max(stats_results, key=lambda x: x['high_max'])
        min_low_point = min(stats_results, key=lambda x: x['low_min'])

        # æ‰¾åˆ°æœ€ä¸€è‡´çš„æ—¶é—´ç‚¹ï¼ˆå…±è¯†åº¦æœ€é«˜ï¼‰
        highest_consensus = max(stats_results, key=lambda x: x['consensus_score'])

        # æ‰¾åˆ°é¢„æµ‹èŒƒå›´æœ€å¤§çš„æ—¶é—´ç‚¹
        max_range_point = max(stats_results, key=lambda x: x['high_max'] - x['low_min'])

        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        all_highs = [r['high_mean'] for r in stats_results]
        all_lows = [r['low_mean'] for r in stats_results]
        all_closes = [r['close_mean'] for r in stats_results]

        # è®¡ç®—æ—¶é—´èŒƒå›´
        time_range = {
            'start': stats_results[0]['timestamp'],
            'end': stats_results[-1]['timestamp'],
            'duration_hours': (stats_results[-1]['timestamp'] - stats_results[0]['timestamp']).total_seconds() / 3600
        }

        return {
            'highest_price': {
                'value': max_high_point['high_max'],
                'time': max_high_point['timestamp'],
                'mean_price': max_high_point['high_mean'],
                'sample_count': max_high_point['sample_count'],
                'batch_count': max_high_point['batch_count']
            },
            'lowest_price': {
                'value': min_low_point['low_min'],
                'time': min_low_point['timestamp'],
                'mean_price': min_low_point['low_mean'],
                'sample_count': min_low_point['sample_count'],
                'batch_count': min_low_point['batch_count']
            },
            'highest_consensus': {
                'time': highest_consensus['timestamp'],
                'consensus_score': highest_consensus['consensus_score'],
                'price_mean': highest_consensus['close_mean'],
                'sample_count': highest_consensus['sample_count'],
                'batch_count': highest_consensus['batch_count']
            },
            'widest_range': {
                'time': max_range_point['timestamp'],
                'range_size': max_range_point['high_max'] - max_range_point['low_min'],
                'high_max': max_range_point['high_max'],
                'low_min': max_range_point['low_min'],
                'sample_count': max_range_point['sample_count'],
                'batch_count': max_range_point['batch_count']
            },
            'time_range': time_range,
            'overall_stats': {
                'high_mean': np.mean(all_highs),
                'low_mean': np.mean(all_lows),
                'close_mean': np.mean(all_closes),
                'high_volatility': np.std(all_highs) / np.mean(all_highs) if np.mean(all_highs) > 0 else 0,
                'low_volatility': np.std(all_lows) / np.mean(all_lows) if np.mean(all_lows) > 0 else 0,
                'close_volatility': np.std(all_closes) / np.mean(all_closes) if np.mean(all_closes) > 0 else 0,
                'prediction_range': max(all_highs) - min(all_lows),
                'total_time_points': len(stats_results),
                'avg_sample_count': np.mean([r['sample_count'] for r in stats_results]),
                'avg_batch_count': np.mean([r['batch_count'] for r in stats_results])
            }
        }

    @classmethod
    def get_latest_prediction_analysis(cls, plan_id: int = 3) -> Dict:
        """
        è·å–æœ€æ–°æ‰¹æ¬¡é¢„æµ‹å‡å€¼æ•°æ®çš„ä¸»è¦æ¥å£æ–¹æ³•

        Args:
            plan_id: äº¤æ˜“è®¡åˆ’IDï¼Œé»˜è®¤ä¸º3

        Returns:
            Dict: åŒ…å«æå€¼é¢„æµ‹å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹åˆ†æè®¡åˆ’ {plan_id} çš„æœ€æ–°é¢„æµ‹æ•°æ®")

            # è·å–æœ€æ–°è®­ç»ƒè®°å½•
            latest_training = cls.get_latest_training_record(plan_id)
            if not latest_training:
                logger.warning(f"è®¡åˆ’ {plan_id} æ²¡æœ‰æ‰¾åˆ°å·²å®Œæˆçš„è®­ç»ƒè®°å½•")
                return {'error': 'æ²¡æœ‰æ‰¾åˆ°å·²å®Œæˆçš„è®­ç»ƒè®°å½•'}

            # è·å–æœªæ¥é¢„æµ‹æ•°æ®
            predictions = cls.get_future_prediction_data(latest_training.id, plan_id)
            if not predictions:
                logger.warning(f"è®­ç»ƒè®°å½• {latest_training.id} æ²¡æœ‰æ‰¾åˆ°æœªæ¥é¢„æµ‹æ•°æ®")
                return {'error': 'æ²¡æœ‰æ‰¾åˆ°æœªæ¥é¢„æµ‹æ•°æ®'}

            # åˆ†æé¢„æµ‹æ•°æ®
            stats_results = cls.analyze_batch_predictions(predictions)
            if not stats_results:
                logger.warning("é¢„æµ‹æ•°æ®åˆ†æç»“æœä¸ºç©º")
                return {'error': 'é¢„æµ‹æ•°æ®åˆ†æç»“æœä¸ºç©º'}

            # æ‰¾åˆ°æå€¼
            extremes = cls.find_extreme_predictions(stats_results)

            result = {
                'training_id': latest_training.id,
                'training_version': latest_training.version,
                'plan_id': plan_id,
                'analysis_time': datetime.now(),
                'data_points_count': len(predictions),
                'time_points_count': len(stats_results),
                'extremes': extremes,
                'raw_stats': stats_results[:5] if len(stats_results) > 5 else stats_results  # ä¿ç•™å‰5ä¸ªæ—¶é—´ç‚¹çš„è¯¦ç»†æ•°æ®
            }

            logger.info(f"åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {len(stats_results)} ä¸ªæ—¶é—´ç‚¹çš„é¢„æµ‹æ•°æ®")
            return result

        except Exception as e:
            error_msg = f"è·å–æœ€æ–°é¢„æµ‹åˆ†æå¤±è´¥: {e}"
            logger.error(error_msg)
            return {'error': error_msg}

    @classmethod
    def format_analysis_result(cls, analysis_result: Dict) -> str:
        """æ ¼å¼åŒ–åˆ†æç»“æœä¸ºå¯è¯»æ–‡æœ¬"""
        if 'error' in analysis_result:
            return f"âŒ åˆ†æå¤±è´¥: {analysis_result['error']}"

        extremes = analysis_result['extremes']

        result = [
            f"ğŸ“ˆ æœ€æ–°é¢„æµ‹åˆ†æç»“æœ",
            f"",
            f"ğŸ”¹ è®­ç»ƒè®°å½•: {analysis_result['training_version']} (ID: {analysis_result['training_id']})",
            f"ğŸ”¹ æ•°æ®ç‚¹æ•°: {analysis_result['data_points_count']}",
            f"ğŸ”¹ æ—¶é—´ç‚¹æ•°: {analysis_result['time_points_count']}",
            f"",
            f"ğŸ¯ æå€¼é¢„æµ‹:",
            f"  â¬†ï¸  æœ€é«˜ä»·: {extremes['highest_price']['value']:.2f}",
            f"     æ—¶é—´: {extremes['highest_price']['time']}",
            f"     å‡å€¼: {extremes['highest_price']['mean_price']:.2f}",
            f"     æ ·æœ¬/æ‰¹æ¬¡: {extremes['highest_price']['sample_count']}/{extremes['highest_price']['batch_count']}",
            f"",
            f"  â¬‡ï¸  æœ€ä½ä»·: {extremes['lowest_price']['value']:.2f}",
            f"     æ—¶é—´: {extremes['lowest_price']['time']}",
            f"     å‡å€¼: {extremes['lowest_price']['mean_price']:.2f}",
            f"     æ ·æœ¬/æ‰¹æ¬¡: {extremes['lowest_price']['sample_count']}/{extremes['lowest_price']['batch_count']}",
            f"",
            f"ğŸ¯ é¢„æµ‹èŒƒå›´: {extremes['overall_stats']['prediction_range']:.2f}",
            f"",
            f"ğŸ”® å…±è¯†åº¦æœ€é«˜çš„æ—¶é—´ç‚¹:",
            f"  æ—¶é—´: {extremes['highest_consensus']['time']}",
            f"  å…±è¯†åº¦: {extremes['highest_consensus']['consensus_score']:.3f}",
            f"  ä»·æ ¼: {extremes['highest_consensus']['price_mean']:.2f}",
            f"",
            f"ğŸ“Š æ•´ä½“ç»Ÿè®¡:",
            f"  å¹³å‡æœ€é«˜ä»·: {extremes['overall_stats']['high_mean']:.2f}",
            f"  å¹³å‡æœ€ä½ä»·: {extremes['overall_stats']['low_mean']:.2f}",
            f"  å¹³å‡æ”¶ç›˜ä»·: {extremes['overall_stats']['close_mean']:.2f}",
            f"  ä»·æ ¼æ³¢åŠ¨ç‡: {extremes['overall_stats']['close_volatility']:.3f}",
            f"",
            f"â±ï¸  é¢„æµ‹æ—¶é—´èŒƒå›´:",
            f"  å¼€å§‹: {extremes['time_range']['start']}",
            f"  ç»“æŸ: {extremes['time_range']['end']}",
            f"  æŒç»­: {extremes['time_range']['duration_hours']:.1f} å°æ—¶"
        ]

        return "\n".join(result)


# å…¨å±€æœåŠ¡å®ä¾‹
prediction_analysis_service = PredictionAnalysisService()